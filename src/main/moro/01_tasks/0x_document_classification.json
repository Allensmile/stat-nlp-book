{
  "name" : "Document Classification",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Document classification is an assignment problem - given a set of documents, assign each document a previously established class.\n\n## Formal definition\n\nUsing our [structured prediction recipe](link), we will take a look at the document classification task as simple instance of the structure prediction problem.\n\nWe:\n\n* define a parametrised model S_THETA(X, Y) which measures the match of X and Y. In this case, x is a document, y is its label. As opposed to the problem of machine translation, for example, here the set y is the set of all document classes, which makes the search problem much easier (trivial) than usual.\n* Learn the parameters THETA fro the training data.\n* Given an input X, find the highest-scoring output structure, in this case a single label.\n\n\\y∗=\\argmax\\y∈\\Yss(\\x,\\y)\n\nThis is the prediction of the model.\n\nSEBASTIAN'S NOTE:\nAnd I think it would make sense to do something similar for documentation classification, even though document classification is trivial in terms of the structured search space. The recipe is still valid (model s, learn parameters \\theta, know how to do discrete argmax).\n\n\n\nThere are numerous examples of different document classification problems.\n\n##### Spam classification\n\nOne of the earliest, and best known examples is the e-mail spam classification problem. Given a set of e-mails, some of which are labeled as spam, others as ham (non-spam), decide whether a new e-mail is spam or not. The e-mail client, or a service you are using has a module which is either pre-trained to deal with this problem, or a module which constantly improves its performance with all the new spams and hams you keep getting. Another similar application to this one is the classification of mail into important and unimportant.\n\n##### Sentiment analysis\n\nSentiment analysis is the problem of assigning sentiment to a document (usually review). If sentiment categories are nominal, this problem is a typical document classification problem. If they are numeric (strength of positiveness and negativeness), that is a text regression problem, which goes out of the scope of this chapter.\n\n##### Language classification\n\nHaving a set of documents written in different language, the goal is to identify the language these documents are written in. This is a first step in automated language translation, when there is no prior information in what language the text is written in.\n\n##### Others\n\nNumerous other tasks are set under the term of document classification. For example, classification of Tweets into relevant and irrelevant, identification of genre (grouping documents in established genre classes), gender identification (identifying whether the document was written by a woman or a man), author identification (identification of whether the piece of text was written by an author A, B, C, ...) and others.",
      "extraFields" : { },
      "outputFormat" : "<p>Document classification is an assignment problem - given a set of documents, assign each document a previously established class.</p><h2>Formal definition</h2><p>Using our <a href=\"link\">structured prediction recipe</a>, we will take a look at the document classification task as simple instance of the structure prediction problem.</p><p>We:</p>\n<ul>\n  <li>define a parametrised model S_THETA(X, Y) which measures the match of X and Y. In this case, x is a document, y is its label. As opposed to the problem of machine translation, for example, here the set y is the set of all document classes, which makes the search problem much easier (trivial) than usual.</li>\n  <li>Learn the parameters THETA fro the training data.</li>\n  <li>Given an input X, find the highest-scoring output structure, in this case a single label.</li>\n</ul><p>\\y∗=\\argmax\\y∈\\Yss(\\x,\\y)</p><p>This is the prediction of the model.</p><p>SEBASTIAN'S NOTE: And I think it would make sense to do something similar for documentation classification, even though document classification is trivial in terms of the structured search space. The recipe is still valid (model s, learn parameters \\theta, know how to do discrete argmax).</p><p>There are numerous examples of different document classification problems.</p><h5>Spam classification</h5><p>One of the earliest, and best known examples is the e-mail spam classification problem. Given a set of e-mails, some of which are labeled as spam, others as ham (non-spam), decide whether a new e-mail is spam or not. The e-mail client, or a service you are using has a module which is either pre-trained to deal with this problem, or a module which constantly improves its performance with all the new spams and hams you keep getting. Another similar application to this one is the classification of mail into important and unimportant.</p><h5>Sentiment analysis</h5><p>Sentiment analysis is the problem of assigning sentiment to a document (usually review). If sentiment categories are nominal, this problem is a typical document classification problem. If they are numeric (strength of positiveness and negativeness), that is a text regression problem, which goes out of the scope of this chapter.</p><h5>Language classification</h5><p>Having a set of documents written in different language, the goal is to identify the language these documents are written in. This is a first step in automated language translation, when there is no prior information in what language the text is written in.</p><h5>Others</h5><p>Numerous other tasks are set under the term of document classification. For example, classification of Tweets into relevant and irrelevant, identification of genre (grouping documents in established genre classes), gender identification (identifying whether the document was written by a woman or a man), author identification (identification of whether the piece of text was written by an author A, B, C, ...) and others.</p>"
    }
  }, {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "## General approach\n\nLet's choose a problem, find (or construct!) a proper dataset. The format of the dataset will dictate possible ways to preprocess the text before going on to the next important steps: engineering a set of features which we deem important for successfully solving the problem, and using an appropriate classifier. In order to assess the performance of our system, we will need to properly evaluate the system.",
      "extraFields" : { },
      "outputFormat" : "<h2>General approach</h2><p>Let's choose a problem, find (or construct!) a proper dataset. The format of the dataset will dictate possible ways to preprocess the text before going on to the next important steps: engineering a set of features which we deem important for successfully solving the problem, and using an appropriate classifier. In order to assess the performance of our system, we will need to properly evaluate the system.</p>"
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "## Problem\n\nWhat do we want to solve, and why? How will we approach it:\n\nThe next step would be obtaining a dataset over which we will classify some documents.",
      "extraFields" : { },
      "outputFormat" : "<h2>Problem</h2><p>What do we want to solve, and why? How will we approach it:</p><p>The next step would be obtaining a dataset over which we will classify some documents.</p>"
    }
  }, {
    "id" : 3,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "## Dataset\n\nLet's get the dataset\n\nWebKB (http://www.cs.cmu.edu/~webkb/)\n\nDOWNLOAD\n\nDirectory structure\n\nLoading in the data...",
      "extraFields" : { },
      "outputFormat" : "<h2>Dataset</h2><p>Let's get the dataset</p><p>WebKB (http://www.cs.cmu.edu/~webkb/)</p><p>DOWNLOAD</p><p>Directory structure</p><p>Loading in the data...</p>"
    }
  }, {
    "id" : 4,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "## Preprocessing\n\nExtracting text only (getting rid of markup)",
      "extraFields" : { },
      "outputFormat" : "<h2>Preprocessing</h2><p>Extracting text only (getting rid of markup)</p>"
    }
  }, {
    "id" : 5,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "## Feature engineering\n\nFirst BOW\n\nThen maybe something more elaborate",
      "extraFields" : { },
      "outputFormat" : "<h2>Feature engineering</h2><p>First BOW</p><p>Then maybe something more elaborate</p>"
    }
  }, {
    "id" : 6,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "## Classification\n\nLinear classifier\n\nMLE Naive Bayes",
      "extraFields" : { },
      "outputFormat" : "<h2>Classification</h2><p>Linear classifier</p><p>MLE Naive Bayes</p>"
    }
  }, {
    "id" : 7,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "## Evaluation\n\nSplit into train/dev/test\n\nPrecision/recall",
      "extraFields" : { },
      "outputFormat" : "<h2>Evaluation</h2><p>Split into train/dev/test</p><p>Precision/recall</p>"
    }
  }, {
    "id" : 8,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "## Room for improvement\n\nUsing more advanced features",
      "extraFields" : { },
      "outputFormat" : "<h2>Room for improvement</h2><p>Using more advanced features</p>"
    }
  }, {
    "id" : 9,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "## Exercises\n\n1. Create a SMS spam classifier. Use the SMS Spam Collection v.1 dataset available here (http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/).\n\n2. Build a language identificator. Download free texts in language of your choice from the Leipzig Corpora Collection (http://corpora.uni-leipzig.de/download.html).\n\n3. Build a language indentificator between European Portuguese and Brazilian Portuguese. What kind of difficulties do you encounter there?\n\n4. Have fun with the Reuters dataset...\n\n5. Sentiment analysis?\n\n6. Gender identification?\n\n7. Music lyrics classification\n\n\nTODO: CHECK http://trec.nist.gov/data.html\n",
      "extraFields" : { },
      "outputFormat" : "<h2>Exercises</h2>\n<ol>\n  <li><p>Create a SMS spam classifier. Use the SMS Spam Collection v.1 dataset available here (http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/).</p></li>\n  <li><p>Build a language identificator. Download free texts in language of your choice from the Leipzig Corpora Collection (http://corpora.uni-leipzig.de/download.html).</p></li>\n  <li><p>Build a language indentificator between European Portuguese and Brazilian Portuguese. What kind of difficulties do you encounter there?</p></li>\n  <li><p>Have fun with the Reuters dataset...</p></li>\n  <li><p>Sentiment analysis?</p></li>\n  <li><p>Gender identification?</p></li>\n  <li><p>Music lyrics classification</p></li>\n</ol><p>TODO: CHECK http://trec.nist.gov/data.html</p>"
    }
  } ],
  "config" : { }
}
