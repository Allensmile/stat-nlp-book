{
  "name" : "Tokenization",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : " Before a program can process natural language, we need identify the _words_ that constitute a string of characters. This is important because the meaning of text generally depends on the relations of words in that text. For example, to understand the command \"find me a Chinese restaurant\" TODO. \n\nBy default text on a computer is represented through `String` values. These values store a sequence of characters (nowadays mostly in [UTF-8](http://en.wikipedia.org/wiki/UTF-8) format). The first step of an NLP pipeline is therefore to split the text into smaller units corresponding to the words of the language we are considering. In the context of NLP we often refer to these units as _tokens_, and the process of extracting these units is called _tokenization_. \n\nIn Scala (and Java) a simple way to tokenize a text is via the `split` method that divides a text according to a [regular expression](http://en.wikipedia.org/wiki/Regular_expression) that defines where a token should be split. ",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 1,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "val text = \"Thinkin' of a master plan.\"\ntext.split(\" \").toSeq",
      "extraFields" : {
        "aggregatedCells" : "[]"
      },
      "outputFormat" : null
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "There are clearly a lot of shortcomings with this simple approach to tokenization. However, before we address these we will switch to using the Wolfe [document data structures](linkToImplementationSection). These simplify downstream processing, and also enable tailor-made rendering that will come in handy.\n\nNotice that in wolfe any document is always in a tokenized state, but it may be based on a very coarse grained tokenization where the whole document is one single token. Tokenization in this setting amounts to refining a given tokenization.\n\nBelow `Document.fromString` creates a document where a single token spans the complete source string. \n",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 3,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "val doc = Document.fromString(\"Thinkin' of a master plan.\")\nrenderTokens(doc)",
      "extraFields" : {
        "aggregatedCells" : "[\"val text = \\\"Thinkin' of a master plan.\\\"\\ntext.split(\\\" \\\").toSeq\"]"
      },
      "outputFormat" : null
    }
  }, {
    "id" : 4,
    "compiler" : "heading3",
    "input" : {
      "sessionId" : null,
      "code" : "RegEx Tokenization",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 5,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Blah",
      "extraFields" : { },
      "outputFormat" : null
    }
  } ],
  "config" : { }
}
