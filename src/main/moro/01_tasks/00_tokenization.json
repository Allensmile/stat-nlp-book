{
  "name" : "Tokenization",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : " Before a program can process natural language, we need identify the _words_ that constitute a string of characters. This is important because the meaning of text generally depends on the relations of words in that text. For example, to understand the command \"find me a Chinese restaurant\" TODO. \n\nBy default text on a computer is represented through `String` values. These values store a sequence of characters (nowadays mostly in [UTF-8](http://en.wikipedia.org/wiki/UTF-8) format). The first step of an NLP pipeline is therefore to split the text into smaller units corresponding to the words of the language we are considering. In the context of NLP we often refer to these units as _tokens_, and the process of extracting these units is called _tokenization_. \n\nIn Scala (and Jave) a simple way to tokenize a text is via the `split` method that divides a text according to a [regular expression](http://en.wikipedia.org/wiki/Regular_expression) that defines where a token should be split. ",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 1,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "val text = \"Thinkin' of a master plan.\"\ntext.split(\"((?<=\\\\.)|(?=\\\\.))| \").toSeq",
      "extraFields" : {
        "aggregatedCells" : "[]"
      },
      "outputFormat" : null
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Work with [document data structures](linkToImplementationSection)...",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 3,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "val doc = Document.fromString(\"Thinkin' of a master plan.\")\nrenderTokens(doc)",
      "extraFields" : {
        "aggregatedCells" : "[\"val text = \\\"Thinkin' of a master plan.\\\"\\ntext.split(\\\"((?<=\\\\\\\\.)|(?=\\\\\\\\.))| \\\").toSeq\"]"
      },
      "outputFormat" : null
    }
  } ],
  "config" : { }
}
