{
  "name" : "Tokenization",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nBefore a program can process natural language, we need identify the _words_ that constitute a string of characters. This is important because the meaning of text generally depends on the relations of words in that text. <span class=\"summary\">We need to identify which units constitutes the **words** of a sentence.</span>\n\nBy default text on a computer is represented through `String` values. These values store a sequence of characters (nowadays mostly in [UTF-8](http://en.wikipedia.org/wiki/UTF-8) format). The first step of an NLP pipeline is therefore to split the text into smaller units corresponding to the words of the language we are considering. In the context of NLP we often refer to these units as _tokens_, and the process of extracting these units is called _tokenization_. Tokenization is considered boring by most, but it's hard to overemphasize its importance, seeing as it's the first step in a long pipeline of NLP processors, and if you get this step wrong, all further steps will suffer. <span class=\"summary\"></span>\n\n<div class=\"newslide\"></div>\nIn Scala (and Java) a simple way to tokenize a text is via the `split` method that divides a text wherever a particular pattern matches. In the code below this pattern is simply the whitespace character, and this seems like a reasonable starting point for an English tokenization approach.<span class=\"summary\">In Scala we can use `string.split`:</span> ",
      "extraFields" : { }
    }
  }, {
    "id" : 1,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val text = \"Mr. Bob Dobolina is thinkin' of a master plan. Why doesn't he quit?\"\ntext.split(\" \").toSeq",
      "extraFields" : {
        "aggregatedCells" : "[]"
      }
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nThere are clearly shortcomings in this tokenization. However, before we address these we will switch to using the Wolfe [document data structures](http://localhost:9000/assets/wolfe/wolfe-nlp/target/scala-2.11/api/index.html#ml.wolfe.nlp.Document). These simplify downstream processing, and also enable tailor-made rendering that will come in handy.<span class=\"summary\">This isn't quite optimal. Before we fix it we move to Wolfe [document data structures](http://localhost:9000/assets/wolfe/wolfe-nlp/target/scala-2.11/api/index.html#ml.wolfe.nlp.Document).</span>\n\nNotice that in wolfe any document is always in a tokenized state, but it may be based on a very coarse grained tokenization where the whole document is one single token. Tokenization in this setting amounts to refining a given tokenization.<span class=\"summary\"></span>\n\n<div class=\"newslide\"></div>\nBelow `Document.fromString` creates a document where a single token spans the complete source string.\n",
      "extraFields" : { }
    }
  }, {
    "id" : 3,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val doc = Document.fromString(text)\nrenderTokens(doc)",
      "extraFields" : {
        "aggregatedCells" : "[\"val text = \\\"Mr. Bob Dobolina is thinkin' of a master plan. Why doesn't he quit?\\\"\\ntext.split(\\\" \\\").toSeq\"]"
      }
    }
  }, {
    "id" : 4,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n### Tokenization with Regular Expressions\nWolfe allows users to construct tokenizers using [regular expressions](http://en.wikipedia.org/wiki/Regular_expression) that define the character sequence patterns at which to split tokens. In general regular expressions are a powerful tool NLP practitioners can use when working with text, and they come in handy when you work with command line tools such as [grep](http://en.wikipedia.org/wiki/Grep). In the code below we use a simple pattern `\\\\s` that matches any whitespace. <span class=\"summary\">We can use regular expressions to define where to split tokens.</span>",
      "extraFields" : { }
    }
  }, {
    "id" : 5,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val tokenizer = Tokenizer.fromRegEx(\"\\\\s\")\nval tokenized = tokenizer(doc)\nrenderTokens(tokenized)",
      "extraFields" : {
        "aggregatedCells" : "[\"val text = \\\"Mr. Bob Dobolina is thinkin' of a master plan. Why doesn't he quit?\\\"\\ntext.split(\\\" \\\").toSeq\",\"val doc = Document.fromString(text)\\nrenderTokens(doc)\"]"
      }
    }
  }, {
    "id" : 6,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nOne shortcoming of this tokenization is its treatment of punctuation because it considers \"plan.\" as a token whereas ideally we would prefer \"plan\" and \".\" to be distinct tokens (why?). To achieve this we need to use [lookahead](http://www.regular-expressions.info/lookaround.html) patterns that split at zero-length patterns before and after punctuation. Note how we used Scala [String Interpolation](http://docs.scala-lang.org/overviews/core/string-interpolation.html) to inject smaller patterns into larger ones. <span class=\"summary\">Use **lookahead** to split at zero-length tokens.</span>          ",
      "extraFields" : { }
    }
  }, {
    "id" : 7,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val punct = \"[\\\\.\\\\?]\"\nval beforePunct = s\"(?=$punct)\"\nval afterPunct = s\"(?<=$punct)(?!\\\\s)\" //but not if whitespace follows\nval tokenizer = Tokenizer.fromRegEx(s\"(\\\\s|$beforePunct|$afterPunct)\")\nval tokenized = tokenizer(doc)\nrenderTokens(tokenized)",
      "extraFields" : {
        "aggregatedCells" : "[\"val text = \\\"Mr. Bob Dobolina is thinkin' of a master plan. Why doesn't he quit?\\\"\\ntext.split(\\\" \\\").toSeq\",\"val doc = Document.fromString(text)\\nrenderTokens(doc)\",\"val tokenizer = Tokenizer.fromRegEx(\\\"\\\\\\\\s\\\")\\nval tokenized = tokenizer(doc)\\nrenderTokens(tokenized)\"]"
      }
    }
  }, {
    "id" : 8,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nThis still isn't perfect. First, \"Mr.\" is split into two tokens, but it should be one. Second, and more subtly, many downstream linguistic processors (such as [syntactic parsers](todo)) prefer contractions such as \"doesn't\" to become two tokens, \"does\" and \"'t\" (Why?). <span class=\"summary\">This is still not perfect due to abbreviations such as \"Mr.\" and the representation of **contractions**.</span> \n\n<div class=\"exercise\">\n  <div class=\"exname\">Exercise</div>    \n  <div class=\"extext\">Improve the regular expressions to overcome the two problems above.</div>    \n</div>",
      "extraFields" : { }
    }
  }, {
    "id" : 9,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n###Learning To Tokenize\nFor most English domains powerful and robust tokenizers can be built using the simple pattern matching approach shown above. However, in languages such as Japanese (...), words are not separated by whitespace, and this makes tokenization substantially more challenging (example). Even for certain English domains such as the domain of biomedical papers, tokenization is non-trivial.<span class=\"summary\">Tokenization is relatively easy for English, but not for other languages or domains.</span> \n\nWhen tokenization is more challenging and difficult to capture in a few rules a machine-learning based approach can be useful. In a nutshell, we can treat the tokenization problem as a character classification problem, or if needed, as a sequential labelling problem.<span class=\"summary\">We can **learn** to tokenize using sequence labelling models we discuss later.</span> ",
      "extraFields" : { }
    }
  }, {
    "id" : 10,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n###Sentence Segmentation\nMany NLP tools work on a sentence-by-sentence basis. The next preprocessing step is hence to segment streams of tokens into sentences. In most cases this is straightforward after tokenization, because we only need to split sentences at sentence-ending punctuation tokens. In Wolfe you can implement segmentation using `Segmenter` that splits a sentence at a token whenever that token's content matches a regular expression. If this expression only fires for punctuations, you get the desired behaviour. <span class=\"summary\">Most NLP tools work on a **per-sentence basis**. To find sentence splits we can again use **regular expressions** .</span> ",
      "extraFields" : { }
    }
  }, {
    "id" : 11,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val segmenter = Segmenter.fromRegEx(\"^\\\\.$\")\nval segmented = segmenter(tokenized)\nrenderTokens(segmented)",
      "extraFields" : {
        "aggregatedCells" : "[\"val text = \\\"Mr. Bob Dobolina is thinkin' of a master plan. Why doesn't he quit?\\\"\\ntext.split(\\\" \\\").toSeq\",\"val doc = Document.fromString(text)\\nrenderTokens(doc)\",\"val tokenizer = Tokenizer.fromRegEx(\\\"\\\\\\\\s\\\")\\nval tokenized = tokenizer(doc)\\nrenderTokens(tokenized)\",\"val punct = \\\"[\\\\\\\\.\\\\\\\\?]\\\"\\nval beforePunct = s\\\"(?=$punct)\\\"\\nval afterPunct = s\\\"(?<=$punct)(?!\\\\\\\\s)\\\" //but not if whitespace follows\\nval tokenizer = Tokenizer.fromRegEx(s\\\"(\\\\\\\\s|$beforePunct|$afterPunct)\\\")\\nval tokenized = tokenizer(doc)\\nrenderTokens(tokenized)\"]"
      }
    }
  }, {
    "id" : 12,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nThis is in fact a bad segmentation, as we split at the period of \"Mr.\". However, once you fix tokenization to treat \"Mr.\" as single token, the problem disappears. The default Wolfe tokenizer and segmenter overcome this problem. Below we use these to construct a _pipeline_ of document processors using [Scala function composition](https://twitter.github.io/scala_school/pattern-matching-and-functional-composition.html) via `andThen`. <span class=\"summary\">This is still bad, but we can the default tokenizer and segmenter to </span>       ",
      "extraFields" : { }
    }
  }, {
    "id" : 13,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val pipeline = Tokenizer.default andThen Segmenter.default\nrenderTokens(pipeline(doc))",
      "extraFields" : {
        "aggregatedCells" : "[\"val text = \\\"Mr. Bob Dobolina is thinkin' of a master plan. Why doesn't he quit?\\\"\\ntext.split(\\\" \\\").toSeq\",\"val doc = Document.fromString(text)\\nrenderTokens(doc)\",\"val tokenizer = Tokenizer.fromRegEx(\\\"\\\\\\\\s\\\")\\nval tokenized = tokenizer(doc)\\nrenderTokens(tokenized)\",\"val punct = \\\"[\\\\\\\\.\\\\\\\\?]\\\"\\nval beforePunct = s\\\"(?=$punct)\\\"\\nval afterPunct = s\\\"(?<=$punct)(?!\\\\\\\\s)\\\" //but not if whitespace follows\\nval tokenizer = Tokenizer.fromRegEx(s\\\"(\\\\\\\\s|$beforePunct|$afterPunct)\\\")\\nval tokenized = tokenizer(doc)\\nrenderTokens(tokenized)\",\"val segmenter = Segmenter.fromRegEx(\\\"^\\\\\\\\.$\\\")\\nval segmented = segmenter(tokenized)\\nrenderTokens(segmented)\"]"
      }
    }
  }, {
    "id" : 14,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n#### Exercise\nCreate a regular expression tokenizer and sentence segmenter that can process Hip-Hop lyrics from [OHHLA](todo). Use the `Corpora.ohhla` document collection and split sentences per line. \n\n<div class=\"newslide\"></div>\n### Background Reading\n\n* Jurafsky & Martin, Speech and Language Processing: Chapter 2, Regular Expressions and Automata.\n* Manning, Raghavan & Schuetze, Introduction to Information Retrieval: [Tokenization](http://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)\n",
      "extraFields" : { }
    }
  } ],
  "config" : { }
}
