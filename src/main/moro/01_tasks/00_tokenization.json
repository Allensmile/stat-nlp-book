{
  "name" : "Tokenization",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : " Before a program can process natural language, we need identify the _words_ that constitute a string of characters. This is important because the meaning of text generally depends on the relations of words in that text. For example, to understand the command \"find me a Chinese restaurant\" TODO. \n\nBy default text on a computer is represented through `String` values. These values store a sequence of characters (nowadays mostly in [UTF-8](http://en.wikipedia.org/wiki/UTF-8) format). The first step of an NLP pipeline is therefore to split the text into smaller units corresponding to the words of the language we are considering. In the context of NLP we often refer to these units as _tokens_, and the process of extracting these units is called _tokenization_. \n\nIn Scala (and Java) a simple way to tokenize a text is via the `split` method that divides a text whereever a particular pattern matches. In the code below this pattern is simply the whitespace character, and this seems like a reasonable starting point for an English tokenization approach. ",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 1,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "val text = \"Mr. Bob Dobolina is thinkin' of a master plan. Why doesn't he quit?\"\ntext.split(\" \").toSeq",
      "extraFields" : {
        "aggregatedCells" : "[]"
      },
      "outputFormat" : null
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "There are clearlyshortcomings in this tokenization. However, before we address these we will switch to using the Wolfe [document data structures](linkToImplementationSection). These simplify downstream processing, and also enable tailor-made rendering that will come in handy.\n\nNotice that in wolfe any document is always in a tokenized state, but it may be based on a very coarse grained tokenization where the whole document is one single token. Tokenization in this setting amounts to refining a given tokenization.\n\nBelow `Document.fromString` creates a document where a single token spans the complete source string. \n",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 3,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "val doc = Document.fromString(text)\nrenderTokens(doc)",
      "extraFields" : {
        "aggregatedCells" : "[\"val text = \\\"Mr. Bob Dobolina is thinkin' of a master plan. Why doesn't he quit?\\\"\\ntext.split(\\\" \\\").toSeq\"]"
      },
      "outputFormat" : null
    }
  }, {
    "id" : 4,
    "compiler" : "heading3",
    "input" : {
      "sessionId" : null,
      "code" : "Tokenization with Regular Expressions",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 5,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Wolfe allows users to construct tokenizers using [regular expressions](http://en.wikipedia.org/wiki/Regular_expression) that define the character sequence patterns at which to split tokens. In general regular expressions are a powerful tool NLP practictioners can use when working with text, and they come in handy when you work with command line tools such as [grep](http://en.wikipedia.org/wiki/Grep). In the code below we use a simple pattern `\\\\s` that matches any whitespace. ",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 6,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "val tokenizer = Tokenizer.fromRegEx(\"\\\\s\")\nval tokenized = tokenizer(doc)\nrenderTokens(tokenized)",
      "extraFields" : {
        "aggregatedCells" : "[\"val text = \\\"Thinkin' of a master plan.\\\"\\ntext.split(\\\" \\\").toSeq\",\"val doc = Document.fromString(\\\"Thinkin' of a master plan.\\\")\\nrenderTokens(doc)\"]"
      },
      "outputFormat" : null
    }
  }, {
    "id" : 7,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "One shortcoming of this tokenization is its treatment of punctuation because it considers \"plan.\" as a token whereas ideally we would prefer \"plan\" and \".\" to be distinct tokens (why?). To achieve this we need to use [lookahead](http://www.regular-expressions.info/lookaround.html) patterns that split at zero-length patterns before and after punctuation. Note how we used Scala [String Interpolation](http://docs.scala-lang.org/overviews/core/string-interpolation.html) to inject smaller patterns into larger ones.           ",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 8,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "val punct = \"[\\\\.\\\\?]\"\nval beforePunct = s\"(?=$punct)\"\nval afterPunct = s\"(?<=$punct)(?!\\\\s)\" //but not if whitespace follows\nval tokenizer = Tokenizer.fromRegEx(s\"(\\\\s|$beforePunct|$afterPunct)\")\nval tokenized = tokenizer(doc)\nrenderTokens(tokenized)",
      "extraFields" : {
        "aggregatedCells" : "[\"val text = \\\"Mr. Bob Dobolina is thinkin' of a master plan. Why doesn't he quit?\\\"\\ntext.split(\\\" \\\").toSeq\",\"val doc = Document.fromString(text)\\nrenderTokens(doc)\",\"val tokenizer = Tokenizer.fromRegEx(\\\"\\\\\\\\s\\\")\\nval tokenized = tokenizer(doc)\\nrenderTokens(tokenized)\"]"
      },
      "outputFormat" : null
    }
  }, {
    "id" : 9,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "This still isn't perfect. First, \"Mr.\" is split into two tokens, but it should be one. Second, and more subtly, many downstream linguistic processors (such as [syntactic parsers](todo)) prefer contractions such as \"doesn't\" to become two tokens, \"does\" and \"'t\" (Why?). We leave fixing this to to reader as an **exercise** TODO: have some way to highlight exercises.    ",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 10,
    "compiler" : "heading3",
    "input" : {
      "sessionId" : null,
      "code" : "Learning To Tokenize",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 11,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "For most English domains powerful and robust tokenizers can be built using the simple pattern matching approach shown above. However, in languages such as Japanese (...), words are not separated by whitespace, and this makes tokenization substantially more challenging (example). Even for certain English domains such as the domain of biomedical papers, tokenization is non-trivial (example).\n\nWhen tokenization is more challenging and difficult to capture in a few rules a machine-learning based approach can be useful. In a nutshell, we can treat the tokenization problem as a character [classification problem](todo), or if needed, as a [sequential labelling problem](todo). [Maybe have a mini Wolfe example here?]  ",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 12,
    "compiler" : "heading3",
    "input" : {
      "sessionId" : null,
      "code" : "Sentence Segmentation",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 13,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Many NLP tools work on a sentence-by-sentence basis. The next preprocessing step is hence to segment streams of tokens into sentences. In most cases this is straightforward after tokenization, because we only need to split sentences at sentence-ending punctuation tokens. In Wolfe this behaviour is implemented in the default `SentenceSplitter`.     ",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 14,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "val segmented = SentenceSplitter(tokenized)\nrenderTokens(segmented)",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 15,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "This is in fact a bad segmentation, as we split at the period of \"Mr.\". However, once you fix tokenization to treat \"Mr.\" as single token, the problem disappears. TODO: Call a good tokenizer here, and then show it works.    ",
      "extraFields" : { },
      "outputFormat" : null
    }
  } ],
  "config" : { }
}
