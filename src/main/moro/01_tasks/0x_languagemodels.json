{
  "name" : "Language Models",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Language models (LMs) calculate the probability to see a given sequence of words, as defined through a [tokenization]() algorithm, in a given language or sub-language/domain/genre. For example, an English language model may assign a higher probability to seeing the sequence \"Blah Blub Blah\" than to \"Blub blub blub\", and for a hip-hop language model this proportion may be reversed. There are several use cases for such models: TODO\n\nMore formally, a language model is a stochastic process that models the probability \\\\(\\prob(w_1,\\ldots,w_d)\\\\) of observing sequences of words \\\\(w_1,\\ldots,w_d\\\\). We can, without loss of generality, decompose the probability of such sequences into\n$$\n\\prob(w_1,\\ldots,w_d) = \\prob(w_1) \\prod_{i+1}^d \\prob(w_i|w_1,\\ldots,w_{i-1}).\n$$\nThis means that a language model can be defined by how it models the conditional probablity \\\\(\\prob(w_i|w_1,\\ldots,w_{i-1})\\\\) of seeing a word \\\\(w_i\\\\) after having seen the *history* of previous words \\\\(w_1,\\ldots,w_{i-1}\\\\). We also have to model the prior probability \\\\(\\prob(w_1)\\\\), but as we show later, it is easy to reduce this prior to a conditional probability as well. \n\nIn practice it is common to define language models based on *equivalence classes* of histories instead of having different conditional distributions for each possible history. TODO: Why.\n\n### N-gram Language Models <a name=\"ngram\"></a>\nThe most common type of equivalence class relies on *truncating* histories \\\\(w_1,\\ldots,w_{i-1}\\\\) to length \\\\(n-1\\\\):\n$$\n\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\prob(w_i|w_{i-n},\\ldots,w_{i-1}).\n$$\nThat is, the probability of a word only depends on the last \\\\(n-1\\\\) previous words. We will refer to such model as a *n-gram language model*. TODO: why is this good.\n\n### A Uniform Baseline LM\n*Unigram* models are the simplest 1-gram language models. That is, they model the conditional probability of word using the prior probability of seeing that word:\n$$\n\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\prob(w_i).\n$$\nTo setup datasets and as baseline for more complex language models, we first introduce the simplest instantituation of a unigram model: a *uniform* language model which assigns the prior probability to each word. That is, given a *vocabulary* of words \\\\(\\vocab\\\\), the uniform LM is defined as:\n$$\n\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\frac{1}{|\\vocab|}.\n$$\n\nLet us \"train\" and test such a language model on the OHHLA corpus. First: load a training and\ntesting corpus. ",
      "extraFields" : {
        "aggregatedCells" : "[]"
      }
    }
  }, {
    "id" : 1,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "import chapter.LanguageModels._\nimport corpora.OHHLA._\n\nval docs = JLive.allAlbums flatMap loadDir\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\nval train = words(trainDocs)\nval test = words(testDocs)\ntrain.take(15)",
      "extraFields" : {
        "aggregatedCells" : "[]"
      }
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Next we use the training words to create a vocabulary. These are important TODO. ",
      "extraFields" : { }
    }
  }, {
    "id" : 3,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val vocab = train.toSet\nvocab.size",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15)\"]"
      }
    }
  }, {
    "id" : 4,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "We can now create a uniform language model using a built-in constructor. Language models in this book implement the `LanguageModel` trait we define. \n",
      "extraFields" : { }
    }
  }, {
    "id" : 5,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "trait LanguageModel {\n  def order:Int     \n  def probability(word:String, history:String*):Double    \n}",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15)\",\"val vocab = train.toSet\\nvocab.size\"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 6,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n\nThe most important method they provide is `probability(history,word)` which returns the probability of a word given a history. Let us implement a uniform LM using this trait.  ",
      "extraFields" : { }
    }
  }, {
    "id" : 7,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "case class UniformLM(vocab:Set[String]) extends LanguageModel {\n  def order = 1\n  def probability(word:String, history:String*) = \n    if (vocab(word)) 1.0 / vocab.size else 0.0\n}\nval baseline = UniformLM(vocab)\nbaseline.probability(\"call\") * vocab.size ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15)\",\"val vocab = train.toSet\\nvocab.size\",\"trait LanguageModel {\\n  def order:Int     \\n  def probability(word:String, history:String*):Double    \\n}\"]"
      }
    }
  }, {
    "id" : 8,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Evaluation\nHow do we determine the quality of an (n-gram) LM? Extrinsic methods TODO.\n\nOne intrinsic way is to measure how well the LM plays the \"Shannon Game\": TODO. Given a test sequence \\\\(w_1,\\ldots,w_T\\\\) of \\\\(T\\\\) words, calculate\n$$\n\\perplexity(w_1,\\ldots,w_T) = \\prob(w_1,\\ldots,w_T)^{\\frac{1}{T}} = \\sqrt[T]{\\prod_i^T \\frac{1}{\\prob(w_i|w_{i-n},\\ldots,w_{i-1})}}\n$$\nWe can implement a perplexity function based on the `LanguageModel` interface. After that Let's see how the uniform model does on our test set. ",
      "extraFields" : { }
    }
  }, {
    "id" : 9,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def perplexity(lm:LanguageModel, data:Seq[String]) = {\n  var logProb = 0.0\n  val historyOrder = lm.order - 1\n  for (i <- historyOrder until data.length) {\n    val history = data.slice(i - historyOrder, i)\n    val word = data(i)\n    val p = lm.probability(word,history:_*)\n    logProb += math.log(p)\n  }\n  math.exp(-logProb / (data.length - historyOrder))\n}\nperplexity(baseline, test)  ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15)\",\"val vocab = train.toSet\\nvocab.size\",\"trait LanguageModel {\\n  def order:Int     \\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size \"]"
      }
    }
  }, {
    "id" : 10,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Out-of-Vocabularly Words\n\nThe problem in the above case is that the baseline model assigns zero probability to words that are not in the vocabulary. Test sets will usually contain such words, and this leads to the above result. \n",
      "extraFields" : { }
    }
  }, {
    "id" : 11,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15)\",\"val vocab = train.toSet\\nvocab.size\",\"trait LanguageModel {\\n  def order:Int     \\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size \",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)  \"]"
      }
    }
  }, {
    "id" : 12,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "There are various solutions to this problem, but TODO: introduce OOV preprocessing and say that it's orthogonal to smoothing.\n",
      "extraFields" : { }
    }
  }, {
    "id" : 13,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "injectOOVs(OOV, Seq(\"A\",\"A\",\"B\",\"B\",\"A\")) -> \nreplaceOOVs(OOV, Set(\"A\",\"B\"), Seq(\"A\",\"B\",\"C\"))",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15)\",\"val vocab = train.toSet\\nvocab.size\",\"trait LanguageModel {\\n  def order:Int     \\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size \",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)  \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\"]"
      }
    }
  }, {
    "id" : 14,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Now we can apply this to our training and test set, and create a new uniform model.",
      "extraFields" : { }
    }
  }, {
    "id" : 15,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val train = injectOOVs(OOV, words(trainDocs))\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\nval vocab = train.toSet\nval baseline = UniformLM(vocab)\nperplexity(baseline,test)",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15)\",\"val vocab = train.toSet\\nvocab.size\",\"trait LanguageModel {\\n  def order:Int     \\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size \",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)  \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\"]"
      }
    }
  }, {
    "id" : 16,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Training Language Models\nThe uniform LM is obviously not good at modelling actual language. To improve upon this baseline, we can estimate the conditional n-gram distributions from the training data. To this end let us first introduce one parameter \\\\(\\param_{w,h}\\\\) for each word \\\\(w\\\\) and history \\\\(h\\\\) of length \\\\(n - 1\\\\), and define a parametrized language model \\\\(p_\\params\\\\)\n$$\n\\prob_\\params(w|h) = \\param_{w,h}\n$$\nTraining an n-gram LM amounts to estimating \\\\(\\params\\\\) from some training set \\\\(\\train=(w_1,\\ldots,w_n)\\\\).\nOne way to do this is to choose the \\\\(\\params\\\\) that [maximizes the log-likelihood](todo) of \\\\(\\train\\\\):\n$$\n\\params^* = \\arg \\max_\\params \\sum p_\\params(\\ldots)\n$$\nAs it turns out, this maximum-log-likelihood estimate (MLE) can calculated in closed form, simply by counting:\n$$\n\\param^*_{w,h} = \\frac{\\counts{\\train}{h,w}}{\\counts{\\train}{h}} \n$$\n\nwhere \n\n$$\n\\counts{D}{e} = \\text{Count of } e \\text{ in }  D \n$$\n\nLet us use this to first train a unigram model.  \n",
      "extraFields" : { }
    }
  }, {
    "id" : 17,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "//val unigram = ngram(train,1)(vocab)\n//Plotter.barChart(unigram.distribution().take(10))",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15)\",\"val vocab = train.toSet\\nvocab.size\",\"trait LanguageModel {\\n  def order:Int     \\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size \",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)  \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\"]"
      }
    }
  }, {
    "id" : 18,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "The uniform LM has substantially reduced (better) perplexity:",
      "extraFields" : { }
    }
  }, {
    "id" : 19,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "//unigram.perplexity(test)",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15)\",\"val vocab = train.toSet\\nvocab.size\",\"trait LanguageModel {\\n  def order:Int     \\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size \",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)  \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\",\"//val unigram = ngram(train,1)(vocab)\\n//Plotter.barChart(unigram.distribution().take(10))\"]"
      }
    }
  }, {
    "id" : 20,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Bigram LM\n\nThe unigram model ignores any correlation between consecutive words in a sentence. The next best model to overcome this shortcoming is a bigram model. This model conditions the probability of the current word on the previous word. Let us construct such model from the training data.\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 21,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "//val bigram = ngram(train,2)(vocab)\n//Plotter.barChart(bigram.distribution(\"I\").take(10))",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15)\",\"val vocab = train.toSet\\nvocab.size\",\"trait LanguageModel {\\n  def order:Int     \\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size \",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)  \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\",\"//val unigram = ngram(train,1)(vocab)\\n//Plotter.barChart(unigram.distribution().take(10))\",\"//unigram.perplexity(test)\"]"
      }
    }
  }, {
    "id" : 22,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "You can see a more peaked distribution conditioned on \"I\" than in the case of the unigram model. Let's see if this improves perplexity.\n",
      "extraFields" : { }
    }
  }, {
    "id" : 23,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "//bigram.perplexity(test)",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15)\",\"val vocab = train.toSet\\nvocab.size\",\"trait LanguageModel {\\n  def order:Int     \\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size \",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)  \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\",\"//val unigram = ngram(train,1)(vocab)\\n//Plotter.barChart(unigram.distribution().take(10))\",\"//unigram.perplexity(test)\",\"//val bigram = ngram(train,2)(vocab)\\n//Plotter.barChart(bigram.distribution(\\\"I\\\").take(10))\"]"
      }
    }
  }, {
    "id" : 24,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Unfortunately the bigram model has the problem we tried to avoid using the OOV preprocessing method above. The problem is that there are contexts in which the OOV word hasn't been seen, and hence it receives 0 probability.\n",
      "extraFields" : { }
    }
  }, {
    "id" : 25,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "//bigram.probability(\"[OOV]\",\"money\")",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15)\",\"val vocab = train.toSet\\nvocab.size\",\"trait LanguageModel {\\n  def order:Int     \\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size \",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)  \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\",\"//val unigram = ngram(train,1)(vocab)\\n//Plotter.barChart(unigram.distribution().take(10))\",\"//unigram.perplexity(test)\",\"//val bigram = ngram(train,2)(vocab)\\n//Plotter.barChart(bigram.distribution(\\\"I\\\").take(10))\",\"//bigram.perplexity(test)\"]"
      }
    }
  }, {
    "id" : 26,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Smoothing\n\nThe general problem is that maximum likelhood estimates will always underestimate the true probability of some words, and in turn overestimate the (context-dependent) probabilities of other words. To overcome this issue we aim to _smooth_ the probabilities and move mass from seen events to unseen events.\n\n#### Laplace Smoothing\n\nThe easiest way to overcome the problem of zero probabilities is to simply add pseudo counts to each event in the dataset (in a [Bayesian](todo) setting this amounts to a maximum posteriori estimate under a dirichlet prior on parameters). \n\n$$\n\\param^{\\alpha}_{w,h} = \\frac{\\counts{\\train}{h,w} + \\alpha}{\\counts{\\train}{h} + \\alpha \\lvert V \\rvert } \n$$\n",
      "extraFields" : { }
    }
  }, {
    "id" : 27,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "//val laplaceBigram = bigram.laplace(1.0)\n//laplaceBigram.probability(\"[OOV]\",\"money\")",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15)\",\"val vocab = train.toSet\\nvocab.size\",\"trait LanguageModel {\\n  def order:Int     \\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size \",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)  \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\",\"//val unigram = ngram(train,1)(vocab)\\n//Plotter.barChart(unigram.distribution().take(10))\",\"//unigram.perplexity(test)\",\"//val bigram = ngram(train,2)(vocab)\\n//Plotter.barChart(bigram.distribution(\\\"I\\\").take(10))\",\"//bigram.perplexity(test)\",\"//bigram.probability(\\\"[OOV]\\\",\\\"money\\\")\"]"
      }
    }
  }, {
    "id" : 28,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "This should give a better perplexity value:\n",
      "extraFields" : { }
    }
  }, {
    "id" : 29,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "//laplaceBigram.perplexity(test)",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15)\",\"val vocab = train.toSet\\nvocab.size\",\"trait LanguageModel {\\n  def order:Int     \\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size \",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)  \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\",\"//val unigram = ngram(train,1)(vocab)\\n//Plotter.barChart(unigram.distribution().take(10))\",\"//unigram.perplexity(test)\",\"//val bigram = ngram(train,2)(vocab)\\n//Plotter.barChart(bigram.distribution(\\\"I\\\").take(10))\",\"//bigram.perplexity(test)\",\"//bigram.probability(\\\"[OOV]\\\",\\\"money\\\")\",\"//val laplaceBigram = bigram.laplace(1.0)\\n//laplaceBigram.probability(\\\"[OOV]\\\",\\\"money\\\")\"]"
      }
    }
  }, {
    "id" : 30,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "##### Exercise\nCan you find a better pseudo-count number?\n\n#### Interpolation\n\nBlah blah",
      "extraFields" : { }
    }
  } ],
  "config" : {
    "autosave" : "false"
  }
}
