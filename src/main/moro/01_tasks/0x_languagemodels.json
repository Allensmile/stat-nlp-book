{
  "name" : "Language Models",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nLanguage models (LMs) calculate the probability to see a given sequence of words, as defined through a [tokenization](/template/statnlpbook/01_tasks/00_tokenization) algorithm, in a given language or sub-language/domain/genre. For example, an English language model may assign a higher probability to seeing the sequence \"How are you?\" than to \"Wassup' dawg?\", and for a hip-hop language model this proportion may be reversed. <span class=\"summary\">Language models (LMs) calculate the probability to see a given sequence of words.</span>\n\n<div class=\"newslide\"></div>\nThere are several use cases for such models: \n\n* To filter out bad translations in [machine translation](/template/statnlpbook/01_tasks/02_wordmt).\n* To rank speech recognition output. \n* In concept-to-text generation.\n\n<div class=\"newslide\"></div>\nMore formally, a language model is a stochastic process that models the probability \\\\(\\prob(w_1,\\ldots,w_d)\\\\) of observing sequences of words \\\\(w_1,\\ldots,w_d\\\\). We can, without loss of generality, decompose the probability of such sequences into<span class=\"summary\">Without loss of generality</span>  \n\n$$\n\\prob(w_1,\\ldots,w_d) = \\prob(w_1) \\prod_{i+1}^d \\prob(w_i|w_1,\\ldots,w_{i-1}).\n$$\n\nThis means that a language model can be defined by how it models the conditional probablity \\\\(\\prob(w_i|w_1,\\ldots,w_{i-1})\\\\) of seeing a word \\\\(w_i\\\\) after having seen the *history* of previous words \\\\(w_1,\\ldots,w_{i-1}\\\\). We also have to model the prior probability \\\\(\\prob(w_1)\\\\), but it is easy to reduce this prior to a conditional probability as well. <span class=\"summary\">We only need to model how words are generated based on a **history**.</span>\n\n<div class=\"newslide\"></div>\nIn practice it is common to define language models based on *equivalence classes* of histories instead of having different conditional distributions for each possible history. This overcomes sparsity and efficiency problems when working with full histories. <span class=\"summary\">In practice it is common to define language models based on **equivalence classes** of histories.</span>   \n\n\n<div class=\"newslide\"></div>\n### N-gram Language Models\nThe most common type of equivalence class relies on *truncating* histories \\\\(w_1,\\ldots,w_{i-1}\\\\) to length \\\\(n-1\\\\):\n$$\n\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\prob(w_i|w_{i-n},\\ldots,w_{i-1}).\n$$\nThat is, the probability of a word only depends on the last \\\\(n-1\\\\) previous words. We will refer to such model as a *n-gram language model*.\n\n<div class=\"newslide\"></div>\n###A Uniform Baseline LM\n\n*Unigram* models are the simplest 1-gram language models. That is, they model the conditional probability of word using the prior probability of seeing that word:\n$$\n\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\prob(w_i).\n$$\n\n<div class=\"newslide\"></div>\nTo setup datasets and as baseline for more complex language models, we first introduce the simplest instantituation of a unigram model: a *uniform* language model which assigns the same prior probability to each word. That is, given a *vocabulary* of words \\\\(\\vocab\\\\), the uniform LM is defined as:<span class=\"summary\">Given a *vocabulary* of words \\\\(\\vocab\\\\), the **uniform LM** is defined as:</span>\n\n$$\n\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\frac{1}{|\\vocab|}.\n$$\n\n<div class=\"newslide\"></div>\nLet us \"train\" and test such a language model on the OHHLA corpus. First we need to load this corpus. Below we focus on a subset to make our code more responsive and to allow us to test models more quickly.",
      "extraFields" : {
        "aggregatedCells" : "[]"
      }
    }
  }, {
    "id" : 1,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "import chapter.LanguageModels._\nimport corpora.OHHLA._\n\nval docs = JLive.allAlbums flatMap loadDir\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\nval train = words(trainDocs)\nval test = words(testDocs)\ntrain.take(15).mkString(\" \")",
      "extraFields" : {
        "aggregatedCells" : "[]"
      }
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nWe can now create a uniform language model using a built-in constructor. Language models in this book implement the `LanguageModel` trait we define. ",
      "extraFields" : { }
    }
  }, {
    "id" : 3,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "trait LanguageModel {\n  def order:Int     \n  def vocab:Set[String]\n  def probability(word:String, history:String*):Double    \n}",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 4,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n<div class=\"newslide\"></div>\nThe most important method we have to provide is `probability(word,history)` which returns the probability of a word given a history. Let us implement a uniform LM using this trait.<span class=\"summary\">Let us implement a uniform LM using this trait.</span>  ",
      "extraFields" : { }
    }
  }, {
    "id" : 5,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "case class UniformLM(vocab:Set[String]) extends LanguageModel {\n  def order = 1\n  def probability(word:String, history:String*) = \n    if (vocab(word)) 1.0 / vocab.size else 0.0\n}\nval vocab = train.toSet\nval baseline = UniformLM(vocab)\nbaseline.probability(\"call\") ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\"]"
      }
    }
  }, {
    "id" : 6,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n### Sampling\nIt is instructive and easy to sample language from a language model. In many, but not all, cases the more natural the generated language of an LM looks, the better this LM is. <span class=\"summary\">The quality of an LM can often be gauged by looking at its **samples**, but models with poorer samples can still be useful.</span>\n\n<div class=\"newslide\"></div>\nTo sample from an LM one simply needs to iteratively sample from the LM conditional probability over words, and add newly sampled words to the next history. The only challenge in implementing this is to sample from a categorical distribution over words. Here we provide this functionality via `sampleCategorical`. <span class=\"summary\">You can sample **word-by-word**, the current word based on previously sampled ones.</span>  \n",
      "extraFields" : { }
    }
  }, {
    "id" : 7,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "import scala.collection.mutable.ArrayBuffer\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\n  val words = lm.vocab.toIndexedSeq\n  val result = new ArrayBuffer[String]\n  result ++= init\n  for (_ <- 0 until amount) {\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\n    result += words(sampleCategorical(probs))\n  }\n  result.toIndexedSeq\n}\nsample(baseline,Nil,10).mkString(\" \")",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size\"]"
      }
    }
  }, {
    "id" : 8,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n### Evaluation\nHow do we determine the quality of an (n-gram) LM? One way is through *extrinsic* evaluation: assess how much the LM improves performance on *downstream tasks* such as machine translation or speech recognition. Arguably this is the most important measure of LM quality, but it can be costly as re-training such systems may take days, and when we seek to develop general-purpose LMs we may have to evaluate performance on several tasks. This is problematic when one wants to iteratively improving LMs and test new models and parameters. It is hence useful to find *intrinsic* means of evaluation that assess the stand-alone quality of LMs with minimal overhead. <span class=\"summary\">Most important for LM quality is their impact on **downstream tasks** in **extrinsic** evaluations. This can be expensive to evaluate, hence **intrinsic** evaluations can be useful.</span>   \n\n<div class=\"newslide\"></div>\nOne intrinsic way is to measure how well the LM plays the \"Shannon Game\": Predict what the next word in actual context should be, and win if your predictions match the words in an actual corpus. This can be formalized  using the notion of *perplexity* of the LM on a given dataset. Given a test sequence \\\\(w_1,\\ldots,w_T\\\\) of \\\\(T\\\\) words, we calculate the perplexity \\\\(\\perplexity\\\\) as follows: <span class=\"summary\">The **perplexity** of an LM on a sample \\\\(w_1,\\ldots,w_T\\\\) is a measure of intrinsic quality:</span>  \n\n$$\n\\perplexity(w_1,\\ldots,w_T) = \\prob(w_1,\\ldots,w_T)^{-\\frac{1}{T}} = \\sqrt[T]{\\prod_i^T \\frac{1}{\\prob(w_i|w_{i-n},\\ldots,w_{i-1})}}\n$$\n\n<div class=\"newslide\"></div>\nWe can implement a perplexity function based on the `LanguageModel` interface. After that let's see how the uniform model does on our test set. ",
      "extraFields" : { }
    }
  }, {
    "id" : 9,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def perplexity(lm:LanguageModel, data:Seq[String]) = {\n  var logProb = 0.0\n  val historyOrder = lm.order - 1\n  for (i <- historyOrder until data.length) {\n    val history = data.slice(i - historyOrder, i)\n    val word = data(i)\n    val p = lm.probability(word,history:_*)\n    logProb += math.log(p)\n  }\n  math.exp(-logProb / (data.length - historyOrder))\n}\nperplexity(baseline, test)   ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") \",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n}\\nsample(baseline,Nil,10).mkString(\\\" \\\")\"]"
      }
    }
  }, {
    "id" : 10,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n### Out-of-Vocabularly Words\nThe problem in the above example is that the baseline model assigns zero probability to words that are not in the vocabulary. Test sets will usually contain such words, and this leads to the above result of infinite perplexity. \n",
      "extraFields" : { }
    }
  }, {
    "id" : 11,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n}\\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)   \"]"
      }
    }
  }, {
    "id" : 12,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n#### Zipf's Law\nThe fact that we regularly encounter new words in our corpus is a common phenomenon not specific to our corpus. Generally we will see a few words that appear repeatedly, and a long tail of words that appear a few times. While each individual long-tail word is rare, the probability of seeing any long-tail word is quite high (the long tail covers a lot of the frequency mass). \n\n<div class=\"newslide\"></div>\nLet us observe this phenomenon for our data: we will rank the words according to their frequency, and plot this frequency against the rank.  ",
      "extraFields" : { }
    }
  }, {
    "id" : 13,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\nval ranks = counts.map(_._2).zipWithIndex.map(p => p._2.toDouble -> p._1.toDouble) \nlineplot(ranks)",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") \",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n}\\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\"]"
      }
    }
  }, {
    "id" : 14,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n#### Out-of-Vocabularly Tokens\nThere are various solutions to this problem, but TODO: introduce OOV preprocessing and say that it's orthogonal to smoothing.",
      "extraFields" : { }
    }
  }, {
    "id" : 15,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "injectOOVs(OOV, Seq(\"A\",\"A\",\"B\",\"B\",\"A\")) -> \nreplaceOOVs(OOV, Set(\"A\",\"B\"), Seq(\"A\",\"B\",\"C\"))",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n}\\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => p._2.toDouble -> p._1.toDouble) \\n//lineplot(ranks)\\n1\"]"
      }
    }
  }, {
    "id" : 16,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Now we can apply this to our training and test set, and create a new uniform model.",
      "extraFields" : { }
    }
  }, {
    "id" : 17,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val train = injectOOVs(OOV, words(trainDocs))\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\nval vocab = train.toSet\nval baseline = UniformLM(vocab)\nperplexity(baseline,test)",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n}\\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => p._2.toDouble -> p._1.toDouble) \\n//lineplot(ranks)\\n1\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\"]"
      }
    }
  }, {
    "id" : 18,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Training Language Models\nThe uniform LM is obviously not good at modelling actual language. To improve upon this baseline, we can estimate the conditional n-gram distributions from the training data. To this end let us first introduce one parameter \\\\(\\param_{w,h}\\\\) for each word \\\\(w\\\\) and history \\\\(h\\\\) of length \\\\(n - 1\\\\), and define a parametrized language model \\\\(p_\\params\\\\)\n$$\n\\prob_\\params(w|h) = \\param_{w,h}\n$$\nTraining an n-gram LM amounts to estimating \\\\(\\params\\\\) from some training set \\\\(\\train=(w_1,\\ldots,w_n)\\\\).\nOne way to do this is to choose the \\\\(\\params\\\\) that [maximizes the log-likelihood](todo) of \\\\(\\train\\\\):\n$$\n\\params^* = \\arg \\max_\\params \\sum p_\\params(\\ldots)\n$$\nAs it turns out, this maximum-log-likelihood estimate (MLE) can calculated in closed form, simply by counting:\n$$\n\\param^*_{w,h} = \\frac{\\counts{\\train}{h,w}}{\\counts{\\train}{h}} \n$$\n\nwhere \n\n$$\n\\counts{D}{e} = \\text{Count of } e \\text{ in }  D \n$$\n\nIt turns out that many LM variants can be implemented simply by estimating the counts in the nominator and denominator differently. We therefore introduce a trait for such count-based LMs. This will help us later to implement LM variants by modifying the counts of a base-LM. \n",
      "extraFields" : { }
    }
  }, {
    "id" : 19,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "trait CountLM extends LanguageModel {\n  def counts:List[String] => Double\n  def norm:List[String] => Double\n  def probability(word:String, history:String*) = {\n    counts(word :: history.toList) / norm(history.toList)  \n  }\n}",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n}\\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => p._2.toDouble -> p._1.toDouble) \\n//lineplot(ranks)\\n1\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 20,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\nLet us use this to code a generic NGram model and then train a unigram model.  ",
      "extraFields" : { }
    }
  }, {
    "id" : 21,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "import scala.collection.mutable.HashMap\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\n  val vocab = train.toSet\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\n  for (i <- order until train.length) {\n    val history = train.slice(i - order + 1, i).toList\n    val word = train(i)\n    counts(word :: history) += 1.0\n    norm(history) += 1.0\n  }\n}\nval unigram = NGramLM(train,1)\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n}\\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => p._2.toDouble -> p._1.toDouble) \\n//lineplot(ranks)\\n1\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\"]"
      }
    }
  }, {
    "id" : 22,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "The uniform LM has substantially reduced (better) perplexity:",
      "extraFields" : { }
    }
  }, {
    "id" : 23,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "perplexity(unigram,test)",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n}\\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => p._2.toDouble -> p._1.toDouble) \\n//lineplot(ranks)\\n1\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\\nval unigram = NGramLM(train,1)\\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \"]"
      }
    }
  }, {
    "id" : 24,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : " Let us also look at the language the unigram LM generates.",
      "extraFields" : { }
    }
  }, {
    "id" : 25,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "sample(unigram, Nil, 10).mkString(\" \")",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n}\\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => p._2.toDouble -> p._1.toDouble) \\n//lineplot(ranks)\\n1\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\\nval unigram = NGramLM(train,1)\\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test)\"]"
      }
    }
  }, {
    "id" : 26,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Bigram LM\n\nThe unigram model ignores any correlation between consecutive words in a sentence. The next best model to overcome this shortcoming is a bigram model. This model conditions the probability of the current word on the previous word. Let us construct such model from the training data.\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 27,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val bigram = NGramLM(train,2)\nbarChart(vocab.map(w => w -> bigram.probability(w,\"I\")).toSeq.sortBy(-_._2).take(10)) ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n}\\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => p._2.toDouble -> p._1.toDouble) \\n//lineplot(ranks)\\n1\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\\nval unigram = NGramLM(train,1)\\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test)\",\"sample(unigram, Nil, 10).mkString(\\\" \\\")\"]"
      }
    }
  }, {
    "id" : 28,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "You can see a more peaked distribution conditioned on \"I\" than in the case of the unigram model. Let us see how the bigram LM generates language.",
      "extraFields" : { }
    }
  }, {
    "id" : 29,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "sample(bigram, List(\"[BAR]\"), 10).mkString(\" \")",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n}\\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => p._2.toDouble -> p._1.toDouble) \\n//lineplot(ranks)\\n1\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\\nval unigram = NGramLM(train,1)\\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test)\",\"sample(unigram, Nil, 10).mkString(\\\" \\\")\",\"val bigram = NGramLM(train,2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \"]"
      }
    }
  }, {
    "id" : 30,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Does the bigram model improve perplexity?",
      "extraFields" : { }
    }
  }, {
    "id" : 31,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "perplexity(bigram,test) ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n}\\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => p._2.toDouble -> p._1.toDouble) \\n//lineplot(ranks)\\n1\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\\nval unigram = NGramLM(train,1)\\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test)\",\"sample(unigram, Nil, 10).mkString(\\\" \\\")\",\"val bigram = NGramLM(train,2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\")\"]"
      }
    }
  }, {
    "id" : 32,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Unfortunately the bigram model has the problem we tried to avoid using the OOV preprocessing method above. The problem is that there are contexts in which the OOV word hasn't been seen, and hence it receives 0 probability.\n",
      "extraFields" : { }
    }
  }, {
    "id" : 33,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "bigram.probability(\"[OOV]\",\"money\") ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n}\\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => p._2.toDouble -> p._1.toDouble) \\n//lineplot(ranks)\\n1\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\\nval unigram = NGramLM(train,1)\\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test)\",\"sample(unigram, Nil, 10).mkString(\\\" \\\")\",\"val bigram = NGramLM(train,2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\")\",\"perplexity(bigram,test) \"]"
      }
    }
  }, {
    "id" : 34,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Smoothing\n\nThe general problem is that maximum likelhood estimates will always underestimate the true probability of some words, and in turn overestimate the (context-dependent) probabilities of other words. To overcome this issue we aim to _smooth_ the probabilities and move mass from seen events to unseen events.\n\n#### Laplace Smoothing\n\nThe easiest way to overcome the problem of zero probabilities is to simply add pseudo counts to each event in the dataset (in a [Bayesian](todo) setting this amounts to a maximum posteriori estimate under a dirichlet prior on parameters). \n\n$$\n\\param^{\\alpha}_{w,h} = \\frac{\\counts{\\train}{h,w} + \\alpha}{\\counts{\\train}{h} + \\alpha \\lvert V \\rvert } \n$$\n",
      "extraFields" : { }
    }
  }, {
    "id" : 35,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\n  def vocab = base.vocab\n  def order = base.order\n  def counts = h => base.counts(h) + alpha\n  def norm = h => base.norm(h) + alpha * base.vocab.size\n}\nval laplaceBigram = LaplaceLM(bigram,0.1)\nlaplaceBigram.probability(\"[OOV]\",\"money\")",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n}\\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => p._2.toDouble -> p._1.toDouble) \\n//lineplot(ranks)\\n1\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\\nval unigram = NGramLM(train,1)\\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test)\",\"sample(unigram, Nil, 10).mkString(\\\" \\\")\",\"val bigram = NGramLM(train,2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\")\",\"perplexity(bigram,test) \",\"bigram.probability(\\\"[OOV]\\\",\\\"money\\\") \"]"
      }
    }
  }, {
    "id" : 36,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "This should give a better perplexity value:\n",
      "extraFields" : { }
    }
  }, {
    "id" : 37,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "perplexity(laplaceBigram,test) ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n}\\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => p._2.toDouble -> p._1.toDouble) \\n//lineplot(ranks)\\n1\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\\nval unigram = NGramLM(train,1)\\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test)\",\"sample(unigram, Nil, 10).mkString(\\\" \\\")\",\"val bigram = NGramLM(train,2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\")\",\"perplexity(bigram,test) \",\"bigram.probability(\\\"[OOV]\\\",\\\"money\\\") \",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => base.counts(h) + alpha\\n  def norm = h => base.norm(h) + alpha * base.vocab.size\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1)\\nlaplaceBigram.probability(\\\"[OOV]\\\",\\\"money\\\")\"]"
      }
    }
  }, {
    "id" : 38,
    "compiler" : "html",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"exercise\">\n  <div class=\"exname\">Exercise 2</div>\n  <div class=\"extext\">Can you find a better pseudo-count number?</div>\n</div>\n",
      "extraFields" : { }
    }
  }, {
    "id" : 39,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Adjusted counts\nIt is often useful to think of smoothing algorithms as un-smoothed Maximum-Likelhood estimators that work with <span class=\"emphasize\"> *adjusted* n-gram counts in the numerator, and fixed history counts in the denominator</span>. This allows us to see how counts from high-frequency words a reduced, and counts of unseen words increased. If these changes are too big, the smoothing method is likely not very effective. \n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 40,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Let us reformulate the laplace LM using adjusted counts. Note that we since we have histories with count 0, we do need to increase the original denominator by a small \\\\(\\epsilon\\\\) to avoid division by zero. \n$$\n\\begin{split}\n\\counts{\\train,\\alpha}{h,w} &= \\param^{\\alpha}_{w,h} \\cdot (\\counts{\\train}{h} +  \\epsilon)\\\\\\\\\n\\counts{\\train,\\alpha}{h} &= \\counts{\\train}{h} + \\epsilon\n\\end{split}\n$$",
      "extraFields" : { }
    }
  }, {
    "id" : 41,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\n  val eps = 0.001\n  def vocab = base.vocab\n  def order = base.order\n  def counts = h => \n      (base.counts(h) + alpha) / (base.norm(h.tail) + alpha * base.vocab.size) * (base.norm(h.tail) + eps)\n  def norm = h => base.norm(h) + eps\n}\nval laplaceBigram = LaplaceLM(bigram,0.1)\nbigram.counts(\"[OOV]\"::\"[OOV]\":: Nil) -> laplaceBigram.counts(\"[OOV]\"::\"[OOV]\":: Nil)",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n}\\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => p._2.toDouble -> p._1.toDouble) \\n//lineplot(ranks)\\n1\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\\nval unigram = NGramLM(train,1)\\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test)\",\"sample(unigram, Nil, 10).mkString(\\\" \\\")\",\"val bigram = NGramLM(train,2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\")\",\"perplexity(bigram,test) \",\"bigram.probability(\\\"[OOV]\\\",\\\"money\\\") \",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => base.counts(h) + alpha\\n  def norm = h => base.norm(h) + alpha * base.vocab.size\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1)\\nlaplaceBigram.probability(\\\"[OOV]\\\",\\\"money\\\")\",\"perplexity(laplaceBigram,test) \"]"
      }
    }
  }, {
    "id" : 42,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "We see above that for high frequency words the absolute counts are altered quite substantially. This is unfortunate because for high frequency words we would expect the counts to be relatively accurate. Can we test more generally wether our adjusted counts are sensible? \n\nOne option is to compare the adjusted counts to average counts in a held-out set. For example, for words of count 0 in the training set, how does their average count in the held-out set compare to their adjusted count in the smoothed model?       ",
      "extraFields" : { }
    }
  }, {
    "id" : 43,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val testBigram = NGramLM(test,2)\nval jointVocab = (train ++ test).toSet\ndef avgCounts(trainLM:CountLM, testLM:CountLM, vocab:Set[String]) = {\n  val avgTestCounts = new HashMap[Double,Double] withDefaultValue 0.0\n  val norm = new HashMap[Double,Double] withDefaultValue 0.0\n  for (ngram <- cross(List.fill(trainLM.order)(vocab.toList))) {\n    val trainCount = trainLM.counts(ngram)\n    val testCount = testLM.counts(ngram)\n    avgTestCounts(trainCount) += testCount\n    norm(trainCount) += 1.0\n  }\n  for (c <- avgTestCounts.keys) avgTestCounts(c) /= norm(c)\n  avgTestCounts\n}\nval avgTestCounts = avgCounts(bigram,testBigram,jointVocab)\nval avgLaplaceCounts = avgCounts(bigram,LaplaceLM(bigram,0.1),jointVocab)\nval compared = for (i <- 0 until 8) yield Seq(i,avgTestCounts(i),avgLaplaceCounts(i)) \ntable(Seq(\"count\",\"test\",\"smooth\") +:compared) ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n}\\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => p._2.toDouble -> p._1.toDouble) \\n//lineplot(ranks)\\n1\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\\nval unigram = NGramLM(train,1)\\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test)\",\"sample(unigram, Nil, 10).mkString(\\\" \\\")\",\"val bigram = NGramLM(train,2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\")\",\"perplexity(bigram,test) \",\"bigram.probability(\\\"[OOV]\\\",\\\"money\\\") \",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => base.counts(h) + alpha\\n  def norm = h => base.norm(h) + alpha * base.vocab.size\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1)\\nlaplaceBigram.probability(\\\"[OOV]\\\",\\\"money\\\")\",\"perplexity(laplaceBigram,test) \",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  val eps = 0.001\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => \\n      (base.counts(h) + alpha) / (base.norm(h.tail) + alpha * base.vocab.size) * (base.norm(h.tail) + eps)\\n  def norm = h => base.norm(h) + eps\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1)\\nbigram.counts(\\\"[OOV]\\\"::\\\"[OOV]\\\":: Nil) -> laplaceBigram.counts(\\\"[OOV]\\\"::\\\"[OOV]\\\":: Nil)\"]"
      }
    }
  }, {
    "id" : 44,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Interpolation\n\nBlah blah\n",
      "extraFields" : { }
    }
  }, {
    "id" : 45,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "laplaceBigram.probability(\"money\",\"[OOV]\")\nunigram.probability(\"money\") ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(15).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\") * vocab.size\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val probs = words.map(lm.probability(_,result.takeRight(lm.order - 1):_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n}\\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n}\\nperplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w))\",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => p._2.toDouble -> p._1.toDouble) \\n//lineplot(ranks)\\n1\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), Seq(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test)\",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\\nval unigram = NGramLM(train,1)\\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test)\",\"sample(unigram, Nil, 10).mkString(\\\" \\\")\",\"val bigram = NGramLM(train,2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\")\",\"perplexity(bigram,test) \",\"bigram.probability(\\\"[OOV]\\\",\\\"money\\\") \",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => base.counts(h) + alpha\\n  def norm = h => base.norm(h) + alpha * base.vocab.size\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1)\\nlaplaceBigram.probability(\\\"[OOV]\\\",\\\"money\\\")\",\"perplexity(laplaceBigram,test) \",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  val eps = 0.001\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => \\n      (base.counts(h) + alpha) / (base.norm(h.tail) + alpha * base.vocab.size) * (base.norm(h.tail) + eps)\\n  def norm = h => base.norm(h) + eps\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1)\\nbigram.counts(\\\"[OOV]\\\"::\\\"[OOV]\\\":: Nil) -> laplaceBigram.counts(\\\"[OOV]\\\"::\\\"[OOV]\\\":: Nil)\",\"val testBigram = NGramLM(test,2)\\nval jointVocab = (train ++ test).toSet\\ndef avgCounts(trainLM:CountLM, testLM:CountLM, vocab:Set[String]) = {\\n  val avgTestCounts = new HashMap[Double,Double] withDefaultValue 0.0\\n  val norm = new HashMap[Double,Double] withDefaultValue 0.0\\n  for (ngram <- cross(List.fill(trainLM.order)(vocab.toList))) {\\n    val trainCount = trainLM.counts(ngram)\\n    val testCount = testLM.counts(ngram)\\n    avgTestCounts(trainCount) += testCount\\n    norm(trainCount) += 1.0\\n  }\\n  for (c <- avgTestCounts.keys) avgTestCounts(c) /= norm(c)\\n  avgTestCounts\\n}\\nval avgTestCounts = avgCounts(bigram,testBigram,jointVocab)\\nval avgLaplaceCounts = avgCounts(bigram,LaplaceLM(bigram,0.1),jointVocab)\\nval compared = for (i <- 0 until 8) yield Seq(i,avgTestCounts(i),avgLaplaceCounts(i)) \\ntable(Seq(\\\"count\\\",\\\"test\\\",\\\"smooth\\\") +:compared) \"]"
      }
    }
  } ],
  "config" : {
    "autosave" : "false"
  }
}
