{
  "name" : "Word-based Machine Translation",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Machine Translation (MT) is one of the canonical NLP applications, and one that nowadays most people are familiar with, primarily through online translation services of the major search engine providers. While there is still some way to go before machines can provide fluent and flawless translations, in particular for more distant language pairs like English and Japanese, progress in this field has been remarkable. \n\nIn this chapter we will illustrate the foundations of this progress, and focus on word-based machine translation models. Nowaways the field has mostly moved to phrase and syntax-based approaches, but the word-based approach is still important, both from a foundational point of view, and as sub-component in more complex approaches.  \n\n### MT as Structured Prediction \n\nFormally we will see MT as the task of translating a _source_ sentence \\\\(\\source\\\\) to a _target_ sentence \\\\(\\target\\\\). We can again, conceptually, tackle the problem using the [structured prediction recipe](todo): We define a parametrised model \\\\(s_\\params(\\target,\\source)\\\\) that measures how well a target  \\\\(\\target\\\\) sentence matches a source sentence \\\\(\\source\\\\), learn the parameters \\\\(\\params\\\\) from training data, and then find \n$$\n  \\argmax_\\target s_\\params(\\target,\\source)  \n$$\nas translation of \\\\(\\source\\\\). Different _statistical_ MT approaches, in this view, differ primarily in how \\\\(s\\\\) is defined, \\\\(\\params\\\\) are learned, and how the \\\\(\\argmax\\\\) is found. \n\n### Noisy Channel Model for MT\nMany Word-based MT systems, as well as those based on more advanced representations, rely on a [Noisy Channel](todo) model. In this approach to MT we effectively model the translation process *in reverse*. That is, we assume that a probabilistic process (the speaker's brain) first generates the target sentence \\\\(\\target\\\\) according to the distribution \\\\(\\prob(\\target)\\\\). Then the target sentence \\\\(\\target\\\\) is transmitted through a _noisy channel_ \\\\(\\prob(\\source|\\target)\\\\) that translates \\\\(\\target\\\\) into \\\\(\\source\\\\). Hence translation is seen as adding noise to a clean \\\\(\\target\\\\). This _generative story_ defines a _joint distribution_ over target and source sentences \\\\(\\prob(\\source,\\target) = \\prob(\\target) \\prob(\\source|\\target) \\\\). We can in turn operate this distribution in the direction we actually care about: to infer a target sentence \\\\(\\target\\\\) given a source sentence \\\\(\\source\\\\) we find the _maximum a posteriori_ sentence\n$$\n\\target^* = \\argmax_\\target \\prob(\\target | \\source) = \\argmax_\\target \\prob(\\target) \\, \\prob(\\source | \\target). \n$$\n\nIn the noisy channel approach for MT the distribution that generates the target sentence is usually referred to as [language model](/template/statnlpbook/01_tasks/01_languagemodels), and the noisy channel is called the _translation model_. As we have discussed language models earlier, in this chapter we focus on the translation model \\\\(\\prob(\\source|\\target)\\\\).\n\n### A Naive Baseline Translation Model\nThe most straightforward translation model translates words one-by-one, in the order of appearance:\n$$\n\\prob(\\source|\\target) = \\prod_i^{\\length{\\source}} \\prob(\\ssource_i | \\starget_i) \n$$\nwhere \\\\(\\prob(\\ssource_i | \\starget_i) \\\\) is the probability of translating \\\\(\\starget_i\\\\) as \\\\(\\ssource_i\\\\). These per-word distributions appear in many word-based models, are often refered to _translation tables_ and constitute the main parameters of the model. We will identify \\\\(\\prob(\\ssource | \\starget) \\\\) with \\\\(\\param_{\\ssource,\\starget}\\\\) and will define our naive model as \\\\(\\prob_\\params^\\text{Naive}(\\ssource|\\starget) = \\prod_i^{\\length{\\target}} \\param_{\\ssource_i,\\starget_i} \\\\).  \n\nFor many language pairs one can acquire training sets \\\\(\\train=\\left( \\left(\\source_i,\\target_i\\right) \\right)_{i=1}^n \\\\) of paired source and target sentences. For example, for French and English the [Aligned Hansards](http://www.isi.edu/natural-language/download/hansard/) of the Parliament of Canada can be used. Given such a training set \\\\(\\train\\\\) we can learn the parameters \\\\(\\params\\\\) using the [Maximum Likelhood estimator](todo). In the case of our Naive model this amounts to setting\n$$\n\\prob(\\ssource | \\starget) = \\param_{\\ssource,\\starget} = \\frac{\\text{count TODO}}{\\text{count TODO}} \n$$\n##### Exercise\nDerive the maximum likelhood estimate for \\\\(\\prob_\\params^\\text{Naive}\\\\) for training data \\\\(\\train=\\left( \\left(\\source_i,\\target_i\\right) \\right)_{i=1}^n \\\\). \n\n#### Training the Naive Model\nLet us preprare some toy data to show how train this naive model.\n",
      "extraFields" : { }
    }
  }, {
    "id" : 1,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val train = Seq(\n  \"the house is small\" -> \"das Haus ist klein\",\n  \"the house is small\" -> \"klein ist das Haus\",\n  \"a man is tall\" -> \"ein Mann ist gross\",\n  \"my house is small\" -> \"klein ist mein Haus\"\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\ntrain.size",
      "extraFields" : {
        "aggregatedCells" : "[]"
      }
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Notice how we transformed raw strings into [Document](todo) objects via `segment`, and how we then fill the training set with [Sentence](todo) objects by extracting the documents `head` sentence from each document. This dataset can be used to train the naive model as follows.\n\n##### Exercise\nImplement an non-procedural [functional](https://www.coursera.org/course/progfun) version of this code.\n\n##### Exercise\nMake this work even when sentences don't have the same length. \n\n##### Exercise\nWhich sentences make the problem easier, which harder?",
      "extraFields" : { }
    }
  }, {
    "id" : 3,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "import scala.collection.mutable.HashMap\ncase class Param(src:String, tgt: String, prob: Double)\ntype Model = Seq[Param]\ndef learn(data:Seq[(Sentence,Sentence)]):Model = {\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\n  for ((target,source) <- data) {\n    for (i <- 0 until target.tokens.length) {\n      norm(target.tokens(i).word) += 1.0\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\n    }    \n  }\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\n}\nval model = learn(train)\nbarChart(model sortBy(_.tgt) map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \"is\")) ",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\ntrain.size\"]"
      }
    }
  }, {
    "id" : 4,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Decoding with the Naive Model\n\nBlah",
      "extraFields" : { }
    }
  }, {
    "id" : 5,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def decode(source:Sentence, model:Model) = {\n  val source2targets = model groupBy (_.src) //create one sub-model per source word\n  val source2Best = source2targets mapValues (m => (m maxBy (_.prob)).tgt) //only keep best target\n  val words = source.tokens map (s => source2Best(s.word))\n  Document(IndexedSeq(words))\n}\nval source = train(1)._2\nval target = decode(source, model)\nrenderTokens(target)",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\ntrain.size\",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype Model = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):Model = {\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n}\\nval model = learn(train)\\nbarChart(model sortBy(_.tgt) map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \"]"
      }
    }
  }, {
    "id" : 6,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "The naive model is broken in several ways. Most severly, it ignores the fact that word order can differ and still yield (roughly) the same meaning.   \n\n### IBM Model 2\nThe IBM Model 2 is one of the most influential translation models, even though these days it is only indirectly used in actual MT systems, for example to initialize translation (?) and alignment models. As IBM Model 2 can be understood as generalization of IBM Model 1, we omit the latter for now and briefly illustrate it afterward our introduction of Model 2. Notice that parts of these exposition are based on the excellent [lecture notes on IBM Model 1 and 2](http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/ibm12.pdf) of Mike Collins.  \n\n#### Alignment\nThe core difference of Model 2 to our naive baseline model is the introduction of a _latent_ auxiliary variables: the word to word _alignment_ \\\\(\\aligns\\\\) between words. In particular, we introduce a variable \\\\(a_i \\in [0 \\ldots \\length{\\target}]\\\\) for each source sentence index \\\\(i \\in [1 \\ldots \\length{\\source}]\\\\). The word alignment \\\\(a_i = j \\\\) means that the source word at token \\\\(i\\\\) is _aligned_ with the target word at index \\\\(j\\\\). Notice that \\\\(\\align_i\\\\) can be \\\\(0\\\\). This corresponds to a imaginary _NULL_ token \\\\(\\starget_0\\\\) in the target sentence and allows source words to be omitted in an aligment. \n\n<p class=\"exercise\">\nHow can _target_ words be omitted in an alignment?\n</p>\nBelow you see a simple \n\n\n\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 7,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val target = segment(\"NULL the house is small\").sentences(0)\nval source = segment(\"klein ist das Haus\").sentences(0)\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\ntrain.size\",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype Model = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):Model = {\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n}\\nval model = learn(train)\\nbarChart(model sortBy(_.tgt) map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"def decode(source:Sentence, model:Model) = {\\n  val source2targets = model groupBy (_.src) //create one sub-model per source word\\n  val source2Best = source2targets mapValues (m => (m maxBy (_.prob)).tgt) //only keep best target\\n  val words = source.tokens map (s => source2Best(s.word))\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2\\nval target = decode(source, model)\\nrenderTokens(target)\"]"
      }
    }
  }, {
    "id" : 8,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n\nIBM Model 2 defines a conditional distribution \\\\(\\prob(\\source,\\aligns|\\target)\\\\) over both the source sentence \\\\(\\source\\\\) and its alignment \\\\(\\aligns\\\\) to the target sentence \\\\(\\target\\\\). Such a model can be used as translation model \\\\(\\prob(\\source|\\target)\\\\), as defined above, by marginalizing out the alignment\n\n$$\n\\prob(\\source|\\target) = \\sum_{\\aligns} \\prob(\\source,\\aligns|\\target).\n$$\n\nNotice that pluging this translation model into the _argmax_ decoding (todo: Do I define decoding?) step above results in a nested summation and maximization problem---an [NP-sharp?](todo) problem. To avoid the computational issues that arise in this setting, it is common (but not always [necessary](linktobayespaper)) to instead maximize over both target and alignment at decoding time:\n\n$$\n(\\target^*,\\aligns^*) = \\argmax_{\\target,\\aligns} \\prob(\\target, \\aligns | \\source).\n$$\n\nThis amounts to finding the optimal target and alignment for a given source sentence.  \n\n#### Model\nIBM Model 2 defines its conditional distribution over source and alignments using two sets of parameters \\\\(\\params=(\\balpha,\\bbeta)\\\\). Here \\\\(\\alpha(\\ssource|\\starget)\\\\) is a parameter defining the probability of translation target word \\\\(\\starget\\\\) into source word \\\\(\\ssource\\\\), and \\\\(\\beta(j|i,l_\\starget,l_\\ssource)\\\\) a parameter that defines the probability of aligning the source word at token \\\\(i\\\\) with the target word at token \\\\(j\\\\), conditioned on the length \\\\(l_\\starget\\\\) of the target sentence, and the length \\\\(l_\\ssource\\\\) of the source sentence.\n\nWith the above parameters, IBM Model 2 defines a conditional distribution over source sentences and alignments, conditioned on a target sentence _and a desired source sentence length_ \\\\(l_\\ssource\\\\):\n$$\n  p_\\params^\\text{IBM2}(\\ssource_1 \\ldots \\ssource_{l_\\ssource},\\align_1 \\ldots \\align_{l_\\ssource}|\\starget_1 \\ldots \\starget_{l_\\starget}, l_\\ssource) = \\prod_i^{l_\\ssource} \\alpha(\\ssource_i|\\starget_{a_i}) \\beta(a_i|i,l_\\starget,l_\\ssource)\n$$\n\n### Training IBM Model 2 with the EM Algorithm\nTraining IBM Model 2 is less straightforward than training our naive baseline. The main reason is the lack of _gold alignments_ in the training data. That is, while we can quite easily find, or heuristically construct, _sentence-aligned_ corpora like our toy dataset, we generally do not _word aligned_ sentences.   \n\nTo overcome this problem, IBM Model can be trained using the [EM Algorithm](todo), a general recipe when learning with partially observed data&mdash;in our case the data is partially observed because we observe the source and target sentences, but not their alignments. The EM algorithm is described and derived [elsewhere in this book](todo), hence here we focus primarily on its implementation for IBM Model 2. The EM algorithm is an iterative method that iterates between two steps, the E-step and the M-Step, until convergence. For the case of IBM Model 2 the E and M steps are instantiated as follows:\n\n  * **E-Step**: given a current set of parameters \\\\(\\params\\\\), calculate the **expectations** \\\\(\\pi\\\\) of the latent alignment variables under the model \\\\(p_\\params^\\text{IBM2}\\\\) &mdash; this amounts to estimating a _soft alignment_ for each sentence.    \n  * **M-Step**: Given training set of soft alignments \\\\(\\pi\\\\), find new parameters \\\\(\\params\\\\) that **maximize** the log likelihood of this training set. This amounts to soft counting. \n\nBefore we look at the implementation of this algorithm we will set up the training data to be compatible with our formulation. This involves introducing a 'NULL' token to each target sentence to allow source tokens to remain unalligned. We also gather a few statistics that will be useful in our implementation later on.   \n",
      "extraFields" : {
        "aggregatedCells" : "[\"1 + 3\\n\"]"
      }
    }
  }, {
    "id" : 9,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val train = Seq(\n  \"NULL the house is small\" -> \"das Haus ist klein\",\n  \"NULL the house is small\" -> \"klein ist das Haus\",\n  \"NULL a man is tall\" -> \"ein Mann ist gross\",\n  \"NULL my house is small\" -> \"klein ist mein Haus\"\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\n\nval targetVocab = train.flatMap(_._1.tokens.map(_.word)).toSet\nval sourceVocab = train.flatMap(_._2.tokens.map(_.word)).toSet ",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\ntrain.size\",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype Model = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):Model = {\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n}\\nval model = learn(train)\\nbarChart(model sortBy(_.tgt) map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"def decode(source:Sentence, model:Model) = {\\n  val source2targets = model groupBy (_.src) //create one sub-model per source word\\n  val source2Best = source2targets mapValues (m => (m maxBy (_.prob)).tgt) //only keep best target\\n  val words = source.tokens map (s => source2Best(s.word))\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2\\nval target = decode(source, model)\\nrenderTokens(target)\",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\"]"
      }
    }
  }, {
    "id" : 10,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### E-Step\nWe can now implement the E-Step and run using some initial parameters.",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\ntrain.size\",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype Model = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):Model = {\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n}\\nval model = learn(train)\\nbarChart(model sortBy(_.tgt) map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"def decode(source:Sentence, model:Model) = {\\n  val source2targets = model groupBy (_.src) //create one sub-model per source word\\n  val source2Best = source2targets mapValues (m => (m maxBy (_.prob)).tgt) //only keep best target\\n  val words = source.tokens map (s => source2Best(s.word))\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2\\nval target = decode(source, model)\\nrenderTokens(target)\",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val train = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\n\\nval targetVocab = train.flatMap(_._1.tokens.map(_.word)).toSet\\nval sourceVocab = train.flatMap(_._2.tokens.map(_.word)).toSet \"]"
      }
    }
  }, {
    "id" : 11,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "type Align = IndexedSeq[IndexedSeq[Double]]\ncase class Model(\n    alpha:Map[(String,String),Double],\n    beta:Map[(Int,Int,Int,Int),Double]) \n\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \n\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\n  for ((target,source) <- data) yield {\n    def score(si:Int, ti:Int) = \n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \n      model.beta(ti,si,target.tokens.length, source.tokens.length)\n    for (si <- source.tokens.indices) yield \n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\n  }\n}\n\nval init = Model(\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\n\nval alignments = eStep(init,train)\nRenderer.renderWeightedAlignment(train.head._1, train.head._2, alignments.head)",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\ntrain.size\",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype Model = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):Model = {\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n}\\nval model = learn(train)\\nbarChart(model sortBy(_.tgt) map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"def decode(source:Sentence, model:Model) = {\\n  val source2targets = model groupBy (_.src) //create one sub-model per source word\\n  val source2Best = source2targets mapValues (m => (m maxBy (_.prob)).tgt) //only keep best target\\n  val words = source.tokens map (s => source2Best(s.word))\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2\\nval target = decode(source, model)\\nrenderTokens(target)\",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val train = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\n\\nval targetVocab = train.flatMap(_._1.tokens.map(_.word)).toSet\\nval sourceVocab = train.flatMap(_._2.tokens.map(_.word)).toSet \"]"
      }
    }
  }, {
    "id" : 12,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "You can play around with the initialization of \\\\(\\bbeta\\\\) to see how the alignments react to changes of the word-to-word translation probabilities.\n\n#### M-Step\nLet us implement the M-Step now. In this step we estimate parameters \\\\(\\params\\\\) from a given set of (soft) alignments \\\\(\\aligns\\\\). \n\n",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\ntrain.size\",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype Model = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):Model = {\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n}\\nval model = learn(train)\\nbarChart(model sortBy(_.tgt) map(p => (p.src,p.tgt) -> p.prob))\",\"def decode(source:Sentence, model:Model) = {\\n  val source2targets = model groupBy (_.src) //create one sub-model per source word\\n  val source2Best = source2targets mapValues (m => (m maxBy (_.prob)).tgt) //only keep best target\\n  val words = source.tokens map (s => source2Best(s.word))\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2\\nval target = decode(source, model)\\nrenderTokens(target)\",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"def eStep() = {\\n  1\\n}\\neStep()\"]"
      }
    }
  }, {
    "id" : 13,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\n  for ((a,(t,s)) <- aligns zip data) {\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\n      alpha(s.tokens(si).word,t.tokens(ti).word) += a(si)(ti) \n      alphaNorm(t.tokens(ti).word) += a(si)(ti) \n      beta(ti,si,t.tokens.length,s.tokens.length) += a(si)(ti)\n      betaNorm(si,t.tokens.length,s.tokens.length) += a(si)(ti)\n    }  \n  }                 \n  Model(\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t)) },\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))})    \n}\nval theta1 = mStep(alignments,train)\nbarChart(theta1.alpha.filter(_._1._2 == \"small\"))",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\ntrain.size\",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype Model = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):Model = {\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n}\\nval model = learn(train)\\nbarChart(model sortBy(_.tgt) map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"def decode(source:Sentence, model:Model) = {\\n  val source2targets = model groupBy (_.src) //create one sub-model per source word\\n  val source2Best = source2targets mapValues (m => (m maxBy (_.prob)).tgt) //only keep best target\\n  val words = source.tokens map (s => source2Best(s.word))\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2\\nval target = decode(source, model)\\nrenderTokens(target)\",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val train = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\n\\nval targetVocab = train.flatMap(_._1.tokens.map(_.word)).toSet\\nval sourceVocab = train.flatMap(_._2.tokens.map(_.word)).toSet \",\"type Align = IndexedSeq[IndexedSeq[Double]]\\ncase class Model(\\n    alpha:Map[(String,String),Double],\\n    beta:Map[(Int,Int,Int,Int),Double]) \\n\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\\n\\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,train)\\nRenderer.renderWeightedAlignment(train.head._1, train.head._2, alignments.head)\"]"
      }
    }
  }, {
    "id" : 14,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Initialization (IBM Model 1)\nWe could already iteratively call `eStep` and `mStep`until convergence. However, a crucial question is how to initialize the model parameters for the first call to 'eStep'. So far we used a uniform initialization, but given that the EM algorithm's results usually depend significantly on initialization, using a more informed starting point can be useful. \n\nA common way to initialize EM for IBM Model 2 training is to first train the so called IBM Model 1 using EM. This model really is an instantiation of Model 2 with a specific and **fixed** alignment parameter set \\\\(\\bbeta\\\\). Instead of estimating \\\\(\\bbeta\\\\) it is set to assign uniform probability to all target tokens with respect to a given length:\n$$\n  \\beta(a_i|i,l_\\starget,l_\\ssource) = \\frac{1}{l_\\starget + 1}\n$$\nTraining Model 1 using EM could have the same initialization problem. Fortunately it turns out that with \\\\(\\bbeta\\\\) fixed in this way it can be shown, under mild conditions, that EM will converge to a global optimum, making IBM Model 1 robust to choices of initialization.\n\nLet us train IBM Model 1 now. This amounts to using our previous `eStep` and `mStep` methods, initializing \\\\(\\bbeta\\\\) as above and not updating it during `mStep`. You can see below that the alignments converge relatively quickly.   \n",
      "extraFields" : { }
    }
  }, {
    "id" : 15,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def change(a1:Seq[Align],a2:Seq[Align]) = {\n  val flatA1 = a1.flatMap(_.flatten) \n  val flatA2 = a2.flatMap(_.flatten)\n  val diffs = (flatA1 zip flatA2) map {case(p1,p2) => math.abs(p1 - p2)}\n  diffs.sum / flatA1.length\n}\ndef emModel1(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\n  var model = init\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\n  for (_ <- 0 until iterations) yield {\n    val old = alignments\n    alignments = eStep(model, data)\n    model = mStep(alignments, data).copy(beta = init.beta) \n    (alignments,model,change(old,alignments))\n  }                   \n} \nval ibm1Iterations = emModel1(init, train, 10)\nval xyData = ibm1Iterations.indices.map(_.toDouble) -> ibm1Iterations.map(_._3) \nlineplot(xyData)",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\ntrain.size\",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype Model = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):Model = {\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n}\\nval model = learn(train)\\nbarChart(model sortBy(_.tgt) map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"def decode(source:Sentence, model:Model) = {\\n  val source2targets = model groupBy (_.src) //create one sub-model per source word\\n  val source2Best = source2targets mapValues (m => (m maxBy (_.prob)).tgt) //only keep best target\\n  val words = source.tokens map (s => source2Best(s.word))\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2\\nval target = decode(source, model)\\nrenderTokens(target)\",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val train = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\n\\nval targetVocab = train.flatMap(_._1.tokens.map(_.word)).toSet\\nval sourceVocab = train.flatMap(_._2.tokens.map(_.word)).toSet \",\"type Align = IndexedSeq[IndexedSeq[Double]]\\ncase class Model(\\n    alpha:Map[(String,String),Double],\\n    beta:Map[(Int,Int,Int,Int),Double]) \\n\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\\n\\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,train)\\nRenderer.renderWeightedAlignment(train.head._1, train.head._2, alignments.head)\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((a,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += a(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += a(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += a(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += a(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t)) },\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))})    \\n}\\nval theta1 = mStep(alignments,train)\\nbarChart(theta1.alpha.filter(_._1._2 == \\\"small\\\"))\"]"
      }
    }
  }, {
    "id" : 16,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"exercise\">\n  <div class=\"exname\">Exercise 1</div>\n  <div class=\"extext\">Can you think of other reasonable measures for convergence? Hint: consider the formal derivation of EM. </div>\n</div>\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 17,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Let us have a look at the translation table.\n",
      "extraFields" : { }
    }
  }, {
    "id" : 18,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "barChart(ibm1Iterations.last._2.alpha.filter(_._1._2 == \"small\"))",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\ntrain.size\",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype Model = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):Model = {\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n}\\nval model = learn(train)\\nbarChart(model sortBy(_.tgt) map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"def decode(source:Sentence, model:Model) = {\\n  val source2targets = model groupBy (_.src) //create one sub-model per source word\\n  val source2Best = source2targets mapValues (m => (m maxBy (_.prob)).tgt) //only keep best target\\n  val words = source.tokens map (s => source2Best(s.word))\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2\\nval target = decode(source, model)\\nrenderTokens(target)\",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val train = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\n\\nval targetVocab = train.flatMap(_._1.tokens.map(_.word)).toSet\\nval sourceVocab = train.flatMap(_._2.tokens.map(_.word)).toSet \",\"type Align = IndexedSeq[IndexedSeq[Double]]\\ncase class Model(\\n    alpha:Map[(String,String),Double],\\n    beta:Map[(Int,Int,Int,Int),Double]) \\n\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\\n\\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,train)\\nRenderer.renderWeightedAlignment(train.head._1, train.head._2, alignments.head)\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((a,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += a(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += a(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += a(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += a(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t)) },\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))})    \\n}\\nval theta1 = mStep(alignments,train)\\nbarChart(theta1.alpha.filter(_._1._2 == \\\"small\\\"))\",\"def change(a1:Seq[Align],a2:Seq[Align]) = {\\n  val flatA1 = a1.flatMap(_.flatten) \\n  val flatA2 = a2.flatMap(_.flatten)\\n  val diffs = (flatA1 zip flatA2) map {case(p1,p2) => math.abs(p1 - p2)}\\n  diffs.sum / flatA1.length\\n}\\ndef emModel1(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data).copy(beta = init.beta) \\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \\nval ibm1Iterations = emModel1(init, train, 10)\\nval xyData = ibm1Iterations.indices.map(_.toDouble) -> ibm1Iterations.map(_._3) \\nlineplot(xyData)\"]"
      }
    }
  }, {
    "id" : 19,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "We can also inspect the alignments generated during EM.",
      "extraFields" : { }
    }
  }, {
    "id" : 20,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "Renderer.renderWeightedAlignment(train.head._1, train.head._2, ibm1Iterations.last._1.head)",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\ntrain.size\",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype Model = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):Model = {\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n}\\nval model = learn(train)\\nbarChart(model sortBy(_.tgt) map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"def decode(source:Sentence, model:Model) = {\\n  val source2targets = model groupBy (_.src) //create one sub-model per source word\\n  val source2Best = source2targets mapValues (m => (m maxBy (_.prob)).tgt) //only keep best target\\n  val words = source.tokens map (s => source2Best(s.word))\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2\\nval target = decode(source, model)\\nrenderTokens(target)\",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val train = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\n\\nval targetVocab = train.flatMap(_._1.tokens.map(_.word)).toSet\\nval sourceVocab = train.flatMap(_._2.tokens.map(_.word)).toSet \",\"type Align = IndexedSeq[IndexedSeq[Double]]\\ncase class Model(\\n    alpha:Map[(String,String),Double],\\n    beta:Map[(Int,Int,Int,Int),Double]) \\n\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\\n\\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,train)\\nRenderer.renderWeightedAlignment(train.head._1, train.head._2, alignments.head)\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((a,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += a(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += a(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += a(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += a(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t)) },\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))})    \\n}\\nval theta1 = mStep(alignments,train)\\nbarChart(theta1.alpha.filter(_._1._2 == \\\"small\\\"))\",\"def change(a1:Seq[Align],a2:Seq[Align]) = {\\n  val flatA1 = a1.flatMap(_.flatten) \\n  val flatA2 = a2.flatMap(_.flatten)\\n  val diffs = (flatA1 zip flatA2) map {case(p1,p2) => math.abs(p1 - p2)}\\n  diffs.sum / flatA1.length\\n}\\ndef emModel1(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data).copy(beta = init.beta) \\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \\nval ibm1Iterations = emModel1(init, train, 10)\\nval xyData = ibm1Iterations.indices.map(_.toDouble) -> ibm1Iterations.map(_._3) \\nlineplot(xyData)\",\"barChart(ibm1Iterations.last._2.alpha.filter(_._1._2 == \\\"small\\\"))\"]"
      }
    }
  }, {
    "id" : 21,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Training IBM Model 2\nNow that we have a reasonable initial model we can use it to initialize EM for IBM Model 2.\n",
      "extraFields" : { }
    }
  }, {
    "id" : 22,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def emModel2(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\n  var model = init\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\n  for (_ <- 0 until iterations) yield {\n    val old = alignments\n    alignments = eStep(model, data)\n    model = mStep(alignments, data)\n    (alignments,model,change(old,alignments))\n  }                   \n} \nval ibm1 = ibm1Iterations.last._2\nval ibm2Iterations = emModel2(ibm1, train, 10)\nibm2Iterations.last._3",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\ntrain.size\",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype Model = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):Model = {\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n}\\nval model = learn(train)\\nbarChart(model sortBy(_.tgt) map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"def decode(source:Sentence, model:Model) = {\\n  val source2targets = model groupBy (_.src) //create one sub-model per source word\\n  val source2Best = source2targets mapValues (m => (m maxBy (_.prob)).tgt) //only keep best target\\n  val words = source.tokens map (s => source2Best(s.word))\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2\\nval target = decode(source, model)\\nrenderTokens(target)\",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val train = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\n\\nval targetVocab = train.flatMap(_._1.tokens.map(_.word)).toSet\\nval sourceVocab = train.flatMap(_._2.tokens.map(_.word)).toSet \",\"type Align = IndexedSeq[IndexedSeq[Double]]\\ncase class Model(\\n    alpha:Map[(String,String),Double],\\n    beta:Map[(Int,Int,Int,Int),Double]) \\n\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\\n\\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,train)\\nRenderer.renderWeightedAlignment(train.head._1, train.head._2, alignments.head)\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((a,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += a(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += a(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += a(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += a(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t)) },\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))})    \\n}\\nval theta1 = mStep(alignments,train)\\nbarChart(theta1.alpha.filter(_._1._2 == \\\"small\\\"))\",\"def change(a1:Seq[Align],a2:Seq[Align]) = {\\n  val flatA1 = a1.flatMap(_.flatten) \\n  val flatA2 = a2.flatMap(_.flatten)\\n  val diffs = (flatA1 zip flatA2) map {case(p1,p2) => math.abs(p1 - p2)}\\n  diffs.sum / flatA1.length\\n}\\ndef emModel1(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data).copy(beta = init.beta) \\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \\nval ibm1Iterations = emModel1(init, train, 10)\\nval xyData = ibm1Iterations.indices.map(_.toDouble) -> ibm1Iterations.map(_._3) \\nlineplot(xyData)\",\"barChart(ibm1Iterations.last._2.alpha.filter(_._1._2 == \\\"small\\\"))\",\"Renderer.renderWeightedAlignment(train.head._1, train.head._2, ibm1Iterations.last._1.head)\"]"
      }
    }
  }, {
    "id" : 23,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Let us look at the distortion probabilities for a given source position and source and target lengths.",
      "extraFields" : { }
    }
  }, {
    "id" : 24,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def distort(si:Int) = for (ti <- 0 until 5) yield ti -> ibm2Iterations.last._2.beta(ti,si,5,4) \nbarChart(distort(0), color = Some(\"#fc0\")) ",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\ntrain.size\",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype Model = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):Model = {\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n}\\nval model = learn(train)\\nbarChart(model sortBy(_.tgt) map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"def decode(source:Sentence, model:Model) = {\\n  val source2targets = model groupBy (_.src) //create one sub-model per source word\\n  val source2Best = source2targets mapValues (m => (m maxBy (_.prob)).tgt) //only keep best target\\n  val words = source.tokens map (s => source2Best(s.word))\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2\\nval target = decode(source, model)\\nrenderTokens(target)\",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val train = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"ein Mann ist gross\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\n\\nval targetVocab = train.flatMap(_._1.tokens.map(_.word)).toSet\\nval sourceVocab = train.flatMap(_._2.tokens.map(_.word)).toSet \",\"type Align = IndexedSeq[IndexedSeq[Double]]\\ncase class Model(\\n    alpha:Map[(String,String),Double],\\n    beta:Map[(Int,Int,Int,Int),Double]) \\n\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\\n\\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,train)\\nRenderer.renderWeightedAlignment(train.head._1, train.head._2, alignments.head)\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((a,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += a(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += a(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += a(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += a(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t)) },\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))})    \\n}\\nval theta1 = mStep(alignments,train)\\nbarChart(theta1.alpha.filter(_._1._2 == \\\"small\\\"))\",\"def change(a1:Seq[Align],a2:Seq[Align]) = {\\n  val flatA1 = a1.flatMap(_.flatten) \\n  val flatA2 = a2.flatMap(_.flatten)\\n  val diffs = (flatA1 zip flatA2) map {case(p1,p2) => math.abs(p1 - p2)}\\n  diffs.sum / flatA1.length\\n}\\ndef emModel1(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data).copy(beta = init.beta) \\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \\nval ibm1Iterations = emModel1(init, train, 10)\\nval xyData = ibm1Iterations.indices.map(_.toDouble) -> ibm1Iterations.map(_._3) \\nlineplot(xyData)\",\"barChart(ibm1Iterations.last._2.alpha.filter(_._1._2 == \\\"small\\\"))\",\"Renderer.renderWeightedAlignment(train.head._1, train.head._2, ibm1Iterations.last._1.head)\",\"def emModel2(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data)\\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \\nval ibm1 = ibm1Iterations.last._2\\nval ibm2Iterations = emModel2(ibm1, train, 10)\\nibm2Iterations.last._3\"]"
      }
    }
  } ],
  "config" : { }
}
