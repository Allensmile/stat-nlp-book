{
  "name" : "Language Models",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Language models (LMs) calculate the probability to see a given sequence of words, as defined through a [tokenization]() algorithm, in a given language or sub-language/domain/genre. For example, an English language model may assign a higher probability to seeing the sequence \"Blah Blub Blah\" than to \"Blub blub blub\", and for a hip-hop language model this proportion may be reversed. There are several use cases for such models: TODO\n\nMore formally, a language model is a stochastic process that models the probability \\\\(\\prob(w_1,\\ldots,w_d)\\\\) of observing sequences of words \\\\(w_1,\\ldots,w_d\\\\). We can, without loss of generality, decompose the probability of such sequences into\n$$\n\\prob(w_1,\\ldots,w_d) = \\prob(w_1) \\prod_{i+1}^d \\prob(w_i|w_1,\\ldots,w_{i-1}).\n$$\nThis means that a language model can be defined by how it models the conditional probablity \\\\(\\prob(w_i|w_1,\\ldots,w_{i-1})\\\\) of seeing a word \\\\(w_i\\\\) after having seen the *history* of previous words \\\\(w_1,\\ldots,w_{i-1}\\\\). We also have to model the prior probability \\\\(\\prob(w_1)\\\\), but as we show later, it is easy to reduce this prior to a conditional probability as well. \n\nIn practice it is common to define language models based on *equivalence classes* of histories instead of having different conditional distributions for each possible history. TODO: Why.\n\n### N-gram Language Models <a name=\"ngram\"></a>\nThe most common type of equivalence class relies on *truncating* histories \\\\(w_1,\\ldots,w_{i-1}\\\\) to length \\\\(n-1\\\\):\n$$\n\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\prob(w_i|w_{i-n},\\ldots,w_{i-1}).\n$$\nThat is, the probability of a word only depends on the last \\\\(n-1\\\\) previous words. We will refer to such model as a *n-gram language model*. TODO: why is this good.\n\n### A Uniform Baseline LM\n*Unigram* models are the simplest 1-gram language models. That is, they model the conditional probability of word using the prior probability of seeing that word:\n$$\n\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\prob(w_i).\n$$\nTo setup datasets and as baseline for more complex language models, we first introduce the simplest instantituation of a unigram model: a *uniform* language model which assigns the prior probability to each word. That is, given a *vocabulary* of words \\\\(\\vocab\\\\), the uniform LM is defined as:\n$$\n\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\frac{1}{|\\vocab|}.\n$$\n\nLet us \"train\" and test such a language model on the OHHLA corpus. First: load a training and\ntesting corpus. ",
      "extraFields" : {
        "aggregatedCells" : "[]"
      },
      "outputFormat" : null
    }
  }, {
    "id" : 1,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "import chapter.LanguageModels._\nimport chapter.LanguageModel._\nimport corpora.OHHLA._\n\nval docs = JLive.allAlbums flatMap loadDir\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\nval train = words(trainDocs)\nval test = words(testDocs)\ntrain.take(5)",
      "extraFields" : {
        "aggregatedCells" : "[]"
      },
      "outputFormat" : null
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Next we use the training words to create a vocabulary. These are important TODO. ",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 3,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "val vocab = Vocab(train.distinct)\nvocab.size",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport chapter.LanguageModel._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(5)\"]"
      },
      "outputFormat" : null
    }
  }, {
    "id" : 4,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "We can now create a uniform language model using a built-in constructor. Language models (in this book/in Wolfe?) implment the `LanguageModel` trait. The most important method they provide is `prob(history,word)` which returns the probability of a word given a history. ",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 5,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "val baseline = uniform(vocab)\nbaseline.prob(Vector.empty,\"call\") * vocab.size",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport chapter.LanguageModel._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(5)\",\"val vocab = Vocab(train.distinct)\\nvocab.size\"]"
      },
      "outputFormat" : null
    }
  }, {
    "id" : 6,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Evaluation\nHow do we determine the quality of an (n-gram) LM? Extrinsic methods TODO.\n\nOne intrinsic way is to measure how well the LM plays the \"Shannon Game\": TODO. Given a test sequence \\\\(w_1,\\ldots,w_T\\\\) of \\\\(T\\\\) words, calculate\n$$\n\\perplexity(w_1,\\ldots,w_T) = \\ldots \n$$\nThe `LanguageModel` interface provides a `perplexity` method to calculate this measure on a given test. Let's see how the uniform model does on our test set. ",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 7,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "baseline.perplexity(test)",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport chapter.LanguageModel._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(5)\",\"val vocab = Vocab(train.distinct)\\nvocab.size\",\"val baseline = uniform(vocab)\\nbaseline.prob(Vector.empty,\\\"call\\\") * vocab.size\"]"
      },
      "outputFormat" : null
    }
  }, {
    "id" : 8,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Out-of-Vocabularly Words\n\nThe problem in the above case is that the baseline model assigns zero probability to words that are not in the vocabulary. Test sets will usually contain such words, and this leads to the above result. \n",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 9,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "test.filterNot(train.toSet).take(3) map (w => w -> baseline.prob(Vector.empty,w))",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport chapter.LanguageModel._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(5)\",\"val vocab = Vocab(train.distinct)\\nvocab.size\",\"val baseline = uniform(vocab)\\nbaseline.prob(Vector.empty,\\\"call\\\") * vocab.size\",\"baseline.perplexity(test)\"]"
      },
      "outputFormat" : null
    }
  }, {
    "id" : 10,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "There are various solutions to this problem, but TODO: introduce OOV preprocessing and say that it's orthogonal to smoothing.\n",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 11,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "injectOOVs(OOV, List(\"A\",\"A\",\"B\",\"B\")) -> \nreplaceOOVs(OOV, Set(\"A\",\"B\"), List(\"A\",\"B\",\"C\"))",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport chapter.LanguageModel._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(5)\",\"val vocab = Vocab(train.distinct)\\nvocab.size\",\"val baseline = uniform(vocab)\\nbaseline.prob(Vector.empty,\\\"call\\\") * vocab.size\",\"baseline.perplexity(test)\",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.prob(Vector.empty,w))\"]"
      },
      "outputFormat" : null
    }
  }, {
    "id" : 12,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Now we can apply this to our training and test set\n",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 13,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "val train = injectOOVs(OOV, words(trainDocs))\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\nval vocab = Vocab(train.distinct)\nvocab.size",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport chapter.LanguageModel._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(5)\",\"val vocab = Vocab(train.distinct)\\nvocab.size\",\"val baseline = uniform(vocab)\\nbaseline.prob(Vector.empty,\\\"call\\\") * vocab.size\",\"baseline.perplexity(test)\",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.prob(Vector.empty,w))\",\"injectOOVs(OOV, List(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\")) -> \\nreplaceOOVs(OOV, Set(\\\"A\\\",\\\"B\\\"), List(\\\"A\\\",\\\"B\\\",\\\"C\\\"))\"]"
      },
      "outputFormat" : null
    }
  } ],
  "config" : { }
}
