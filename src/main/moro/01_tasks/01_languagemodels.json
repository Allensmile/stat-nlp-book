{
  "name" : "Language Models",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Language models (LMs) calculate the probability to see a given sequence of words, as defined through a [tokenization]() algorithm, in a given language or sub-language/domain/genre. For example, an English language model may assign a higher probability to seeing the sequence \"Blah Blub Blah\" than to \"Blub blub blub\", and for a hip-hop language model this proportion may be reversed. There are several use cases for such models: TODO\n\nMore formally, a language model is a stochastic process that models the probability \\\\(\\prob(w_1,\\ldots,w_d)\\\\) of observing sequences of words \\\\(w_1,\\ldots,w_d\\\\). We can, without loss of generality, decompose the probability of such sequences into\n$$\n\\prob(w_1,\\ldots,w_d) = \\prob(w_1) \\prod_{i+1}^d \\prob(w_i|w_1,\\ldots,w_{i-1}).\n$$\nThis means that a language model can be defined by how it models the conditional probablity \\\\(\\prob(w_i|w_1,\\ldots,w_{i-1})\\\\) of seeing a word \\\\(w_i\\\\) after having seen the *history* of previous words \\\\(w_1,\\ldots,w_{i-1}\\\\). We also have to model the prior probability \\\\(\\prob(w_1)\\\\), but as we show later, it is easy to reduce this prior to a conditional probability as well. \n\nIn practice it is common to define language models based on *equivalence classes* of histories instead of having different conditional distributions for each possible history. TODO: Why.\n\n### N-gram Language Models <a name=\"ngram\"></a>\nThe most common type of equivalence class relies on *truncating* histories \\\\(w_1,\\ldots,w_{i-1}\\\\) to length \\\\(n-1\\\\):\n$$\n\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\prob(w_i|w_{i-n},\\ldots,w_{i-1}).\n$$\nThat is, the probability of a word only depends on the last \\\\(n-1\\\\) previous words. We will refer to such model as a *n-gram language model*. TODO: why is this good.\n\n#### A Uniform Baseline LM\n*Unigram* models are the simplest 1-gram language models. That is, they model the conditional probability of word using the prior probability of seeing that word:\n$$\n\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\prob(w_i).\n$$\nTo setup datasets and as baseline for more complex language models, we first introduce the simplest instantituation of a unigram model: a *uniform* language model which assigns the prior probability to each word. That is, given a *vocabulary* of words \\\\(\\vocab\\\\), the uniform LM is defined as:\n$$\n\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\frac{1}{|\\vocab|}.\n$$\n\nLet us \"train\" and test such a language model on the OHHLA corpus. First: load a training and\ntesting corpus. ",
      "extraFields" : {
        "aggregatedCells" : "[]"
      },
      "outputFormat" : null
    }
  }, {
    "id" : 1,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "import chapter.LanguageModels._\nimport chapter.LanguageModel._\nimport corpora.OHHLA._\n\nval docs = JLive.allAlbums flatMap loadDir\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\nval train = words(trainDocs)\nval test = words(testDocs)\ntrain.take(5)",
      "extraFields" : {
        "aggregatedCells" : "[]"
      },
      "outputFormat" : null
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Next we use the training words to create a vocabulary. These are important TODO. ",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 3,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "val vocab = Vocab(train.distinct)\nvocab.size",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport chapter.LanguageModel._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(5)\"]"
      },
      "outputFormat" : null
    }
  }, {
    "id" : 4,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "We can now create a uniform language model using a built-in constructor. Language models (in this book/in Wolfe?) have a simple interface. The most important method they provide is `prob(history,word)` which returns the probability of a word given a history. ",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 5,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "val baseline = uniform(vocab)\nbaseline.prob(Vector.empty,\"call\") * vocab.size",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.LanguageModels._\\nimport chapter.LanguageModel._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length - 5)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(5)\",\"val vocab = Vocab(train.distinct)\\nvocab.words.length\"]"
      },
      "outputFormat" : null
    }
  } ],
  "config" : { }
}
