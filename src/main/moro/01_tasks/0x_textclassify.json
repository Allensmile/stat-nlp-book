{
  "name" : "Text Classification",
  "cells" : [ {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "In many applications we need to automatically classify some input text with respect to a set of classes or labels. For example,\n\n* for information retrieval it is useful to classify documents into a set of topics, such as \"sport\" or \"business\",\n* for sentiment analysis we classify tweets into being \"positive\" or \"negative\" and \n* ???\n\nTODO: Some example data loaded here (Reuters?)\n\nTODO: Say something on feature engineering?\n\n### Text Classification as Structured Prediction\nWe can formalize text classification as the simplest instance of [structured prediction](/template/statnlpbook/02_methods/00_structuredprediction) where the input space \\\\(\\Xs\\\\) are texts, and the output space \\\\(\\Ys\\\\) is a set of labels such as \\\\(\\Ys=\\\\{ \\text{sports},\\text{business}\\\\} \\\\). On a high level, our goal is to define a model a model \\\\(s_{\\params}(\\x,y)\\\\) that assigns high *scores* to the label \\\\(y\\\\) that fits the text \\\\(\\x\\\\), and lower scores otherwise. The model will be parametrized by \\\\(\\params\\\\), and these parameters we will learn from some training set \\\\(\\train\\\\) of \\\\((\\x,y)\\\\) pairs. When we need to classify a text \\\\(\\x\\\\) we have to solve the trivial (if the number of classes is low) maximization problem \\\\(\\argmax_y s_{\\params}(\\x,y)\\\\). \n\nTODO: Show a silly classifier example? \n\nIn the following we will present two typical families of models: Naive Bayes and discriminative linear classifiers.\n\n### Naive Bayes\nOne of the most widely used approaches to text classification relies on the so-called Naive Bayes (NB) Model. In NB we use a distribution \\\\(p^{\\mbox{NB}}_{\\params}\\\\) in place of \\\\(s_\\params\\\\). In particular, we use the *a posteriori* probability of a label \\\\(y\\\\) given the input text \\\\(\\x\\\\) as a score for that label given the text.   \n\n\\begin{equation}\n  s_{\\params}(\\x,\\y)\\ = p^{\\mbox{NB}}_{\\params}(y|\\x)\n\\end{equation}\n\nBy Bayes Law we get\n\n\\begin{equation}\n    p^{\\mbox{NB}}_{\\params}(y|\\x) =\n  \\frac{p^{\\mbox{NB}}_{\\params}(\\x|y) p^\\mbox{NB}_{\\params}(y)}{p^{\\mbox{NB}}_{\\params}(x)}  \n\\end{equation}\n\nand when a input \\\\(\\x\\\\) is fixed we can focus on \\\\(p^{\\mbox{NB}}_{\\params}(\\x|y) p^\\mbox{NB}_{\\params}(y) \\\\) because in this case  \\\\(p^{\\mbox{NB}}_{\\params}(x)\\\\) is a constant factor. In the above \\\\(p^{\\mbox{NB}}_{\\params}(\\x|y)\\\\) is the *likelihood*, and \\\\(p^\\mbox{NB}_{\\params}(y) \\\\) is the *prior*.\n\nLet us assume that we have a number \\\\(K(\\x)\\\\) of feature functions \\\\(f_k(\\x)\\\\) that represent the input \\\\(\\x\\\\). For example, in document classification this set could be used to represent the text \\\\(\\x = (x_1,\\ldots,x_n)\\\\) as a bag of words by setting \\\\(f_k(\\x) = x_k\\\\) and \\\\(K(\\x) = n\\\\). We could also use bigrams instead, setting \\\\(f_k(\\x) = (x_k,x_{k+1})\\\\) and \\\\(K(\\x) = n-1\\\\), or any other representation that is effective for distinguishing between classes of text.\n\nThe \"naivity\" of NB stems from a certain conditional independence assumption we make for the likelihood \\\\(p^{\\mbox{NB}}_{\\params}(\\x|y)\\\\). Note that conditional independence of two events \\\\(a\\\\) and \\\\(b\\\\) given a third event \\\\(c\\\\) requires that \\\\(p(a,b|c) = p(a|c) p(b|c)\\\\). In particular, for the likelihood in NB we have:\n\n\\begin{equation}\n  p^{\\mbox{NB}}_{\\params}(\\x|y) = \n  \\prod_k^{K(\\x)} p^{\\mbox{NB}}_{\\params}(f_k(\\x)|y)\n\\end{equation}\n\nThat is, NB makes the assumption that the features are independent of each other when *conditioned on the label* \\\\(y\\\\). \n\nTODO: Evaluate NB assumption on actual data. Show that features aren't independent, but more independent when conditioned on the label.\n\n\n#### Parametrization\nThe NB model has the parameters \\\\(\\params=(\\balpha,\\bbeta)\\\\) where \n\n\\begin{split}\n  p^{\\mbox{NB}}_{\\params}(f|y) & = \\alpha_{f,y} \\\\\\\\\n  p^{\\mbox{NB}}_{\\params}(y) & = \\beta_{y}.\n\\end{split}\n\nThat is, \\\\(\\balpha\\\\) captures the per-class feature weights, and \\\\(\\bbeta\\\\) the class priors. \n\n\n#### Training the Naive Bayes Model\n\nThe NB model again can be trained using Maximum Likelihood estimation. This amounts to setting \n\n\\begin{split}\n  \\alpha_{f,y} & = \\frac{\\counts{\\train}{f,y}}{\\counts{\\train}{y}}\\\\\\\\\n  \\beta{y} & = \\frac{\\counts{\\train}{y}}{\\left| \\train \\right|}\n\\end{split}\n\n\n\n\n \n \n ",
      "extraFields" : { }
    }
  } ],
  "config" : { }
}
