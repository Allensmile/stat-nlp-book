{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %cd ..\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "Language models (LMs) calculate the probability to see a given sequence of words, as defined through a [tokenization](todo) algorithm, in a given language or sub-language/domain/genre. For example, an English language model may assign a higher probability to seeing the sequence \"How are you?\" than to \"Wassup' dawg?\", and for a hip-hop language model this proportion may be reversed. <span class=\"summary\">Language models (LMs) calculate the probability to see a given sequence of words.\n",
    "\n",
    "There are several use cases for such models: \n",
    "\n",
    "* To filter out bad translations in machine translation.\n",
    "* To rank speech recognition output. \n",
    "* In concept-to-text generation.\n",
    "\n",
    "More formally, a language model is a stochastic process that models the probability \\\\(\\prob(w_1,\\ldots,w_d)\\\\) of observing sequences of words \\\\(w_1,\\ldots,w_d\\\\). We can, without loss of generality, decompose the probability of such sequences into<span class=\"summary\">Without loss of generality</span>  \n",
    "\n",
    "$$\n",
    "\\prob(w_1,\\ldots,w_d) = \\prob(w_1) \\prod_{i = 2}^d \\prob(w_i|w_1,\\ldots,w_{i-1}).\n",
    "$$\n",
    "\n",
    "This means that a language model can be defined by how it models the conditional probablity $\\prob(w_i|w_1,\\ldots,w_{i-1})$ of seeing a word \\\\(w_i\\\\) after having seen the *history* of previous words $w_1,\\ldots,w_{i-1}$. We also have to model the prior probability $\\prob(w_1)$, but it is easy to reduce this prior to a conditional probability as well.\n",
    "\n",
    "In practice it is common to define language models based on *equivalence classes* of histories instead of having different conditional distributions for each possible history. This overcomes sparsity and efficiency problems when working with full histories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Language Models\n",
    "\n",
    "The most common type of equivalence class relies on *truncating* histories $w_1,\\ldots,w_{i-1}$ to length $n-1$:\n",
    "$$\n",
    "\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\prob(w_i|w_{i-n},\\ldots,w_{i-1}).\n",
    "$$\n",
    "\n",
    "That is, the probability of a word only depends on the last $n-1$ previous words. We will refer to such model as a *n-gram language model*.\n",
    "\n",
    "## A Uniform Baseline LM\n",
    "\n",
    "*Unigram* models are the simplest 1-gram language models. That is, they model the conditional probability of word using the prior probability of seeing that word:\n",
    "$$\n",
    "\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\prob(w_i).\n",
    "$$\n",
    "\n",
    "To setup datasets and as baseline for more complex language models, we first introduce the simplest instantituation of a unigram model: a *uniform* language model which assigns the same prior probability to each word. That is, given a *vocabulary* of words \\\\(\\vocab\\\\), the uniform LM is defined as:\n",
    "\n",
    "$$\n",
    "\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\frac{1}{|\\vocab|}.\n",
    "$$\n",
    "\n",
    "Let us \"train\" and test such a language model on the OHHLA corpus. First we need to load this corpus. Below we focus on a subset to make our code more responsive and to allow us to test models more quickly. Check the [loading from OHHLA](load_ohhla.ipynb) notebook to see how `load_albums` and `words` are defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[BAR] Can ' t even call this a blues song [/BAR] [BAR] It ' s been so long [/BAR] [BAR] Neither one of us was wrong or anything like that [/BAR] [BAR] It seems like\""
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statnlpbook.util as util\n",
    "util.execute_notebook('load_ohhla.ipynb')\n",
    "docs = load_albums(j_live)\n",
    "trainDocs, testDocs = docs[:len(docs)//2], docs[len(docs)//2:] \n",
    "train = words(trainDocs)\n",
    "test = words(testDocs)\n",
    "\" \".join(train[0:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a uniform language model. Language models in this book implement the `LanguageModel` [abstract base class](https://docs.python.org/3/library/abc.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import abc \n",
    "class LanguageModel(metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocab: the vocabulary underlying this language model. Should be a set of words.\n",
    "        order: history length (-1).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab, order):\n",
    "        self.vocab = vocab\n",
    "        self.order = order\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def probability(self, word,*history):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word: the word we need the probability of\n",
    "            history: words to condition on.\n",
    "        Returns:\n",
    "            the probability p(w|history)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important method we have to provide is `probability(word,history)` which returns the probability of a word given a history. Let us implement a uniform LM using this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003912363067292645"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class UniformLM(LanguageModel):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__(vocab, 1)\n",
    "    def probability(self, word,*history):\n",
    "        return 1.0 / len(self.vocab) if word in self.vocab else 0.0\n",
    "    \n",
    "vocab = set(train)\n",
    "baseline = UniformLM(vocab)\n",
    "baseline.probability(\"call\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "It is instructive and easy to sample language from a language model. In many, but not all, cases the more natural the generated language of an LM looks, the better this LM is.\n",
    "\n",
    "To sample from an LM one simply needs to iteratively sample from the LM conditional probability over words, and add newly sampled words to the next history. The only challenge in implementing this is to sample from a categorical distribution over words. Here we provide this functionality via `np.random.choice` from [numpy](http://www.numpy.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bill',\n",
       " 'gamblin',\n",
       " 'Spit',\n",
       " 'escape',\n",
       " 'throughout',\n",
       " 'Keith',\n",
       " 'ample',\n",
       " '30-year-old',\n",
       " 'sisters',\n",
       " 'Band']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample(lm, init, amount):\n",
    "    words = list(lm.vocab)\n",
    "    result = []\n",
    "    result += init\n",
    "    for _ in range(0, amount):\n",
    "        history = result[-(lm.order-1):]\n",
    "        probs = [lm.probability(word, *history) for word in words]\n",
    "        sampled = np.random.choice(words,p=probs)\n",
    "        result.append(sampled)\n",
    "    return result\n",
    "\n",
    "sample(baseline, [], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "How do we determine the quality of an (n-gram) LM? One way is through *extrinsic* evaluation: assess how much the LM improves performance on *downstream tasks* such as machine translation or speech recognition. Arguably this is the most important measure of LM quality, but it can be costly as re-training such systems may take days, and when we seek to develop general-purpose LMs we may have to evaluate performance on several tasks. This is problematic when one wants to iteratively improve LMs and test new models and parameters. It is hence useful to find *intrinsic* means of evaluation that assess the stand-alone quality of LMs with minimal overhead.\n",
    "\n",
    "One intrinsic way is to measure how well the LM plays the \"Shannon Game\": Predict what the next word in actual context should be, and win if your predictions match the words in an actual corpus. This can be formalized  using the notion of *perplexity* of the LM on a given dataset. Given a test sequence \\\\(w_1,\\ldots,w_T\\\\) of \\\\(T\\\\) words, we calculate the perplexity \\\\(\\perplexity\\\\) as follows:\n",
    "\n",
    "$$\n",
    "\\perplexity(w_1,\\ldots,w_T) = \\prob(w_1,\\ldots,w_T)^{-\\frac{1}{T}} = \\sqrt[T]{\\prod_i^T \\frac{1}{\\prob(w_i|w_{i-n},\\ldots,w_{i-1})}}\n",
    "$$\n",
    "\n",
    "We can implement a perplexity function based on the `LanguageModel` interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def perplexity(lm, data):\n",
    "    log_prob = 0.0\n",
    "    history_order = lm.order - 1\n",
    "    for i in range(history_order, len(data)):\n",
    "        history = data[i - history_order : i]\n",
    "        word = data[i]\n",
    "        p = lm.probability(word, *history)\n",
    "        log_prob += math.log(p) if p > 0.0 else float(\"-inf\")\n",
    "    return math.exp(-log_prob / (len(data) - history_order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the uniform model does on our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(baseline, test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Vocabularly Words\n",
    "The problem in the above example is that the baseline model assigns zero probability to words that are not in the vocabulary. Test sets will usually contain such words, and this leads to the above result of infinite perplexity. For example, the following three words do not appear in the training set vocabulary `vocab` and hence receive 0 probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('send', 0.0), ('corrections', 0.0), ('typist', 0.0)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(w,baseline.probability(w)) for w in test if w not in vocab][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Long Tail\n",
    "The fact that we regularly encounter new words in our corpus is a common phenomenon not specific to our corpus. Generally we will see a few words that appear repeatedly, and a long tail of words that appear only a few times. While each individual long-tail word is rare, the probability of seeing any long-tail word is quite high (the long tail covers a lot of the frequency mass).\n",
    "\n",
    "Let us observe this phenomenon for our data: we will rank the words according to their frequency, and plot this frequency against the rank. Let us first extracted the sorted counts and their ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "counts = collections.defaultdict(int)\n",
    "for word in train:\n",
    "    counts[word] += 1\n",
    "sorted_counts = sorted(counts.values(),reverse=True)\n",
    "ranks = range(1,len(sorted_counts)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the counts against their rank. Play around with the x and y scale and change them to `'log'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10aba9e48>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ad06160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xscale('linear')\n",
    "plt.yscale('linear')\n",
    "plt.plot(ranks, sorted_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In log-space such rank vs frequency graphs resemble linear functions. This observation is known as *Zipf's Law*, and can be formalized as follows. Let \\\\(r\\_w\\\\) be the rank of a word \\\\(w\\\\), and \\\\(f\\_w\\\\) its frequency, then we have:\n",
    "\n",
    "$$\n",
    "  f_w \\propto \\frac{1}{r_w}.\n",
    "$$\n",
    "\n",
    "## Inserting Out-of-Vocabularly Tokens\n",
    "The long tail of infrequent words is a problem for LMs because it means there will always be words with zero counts in your training set. There are various solutions to this problem. For example, when it comes to calculating the LM perplexity we could remove words that do not appear in the training set. This overcomes the problem of infinite perplexity but doesn't solve the actual issue: the LM assigns too low probability to unseen words. Moreover, the problem only gets worse when one considers n-gram models with larger \\\\(n\\\\), because these will encounter many unseen n-grams, which, when removed, will only leave small fractions of the original sentences.\n",
    "\n",
    "The principled solution to this problem is smoothing, and we will discuss it in more detail later. Before we get there we present a simple preprocessing step that generally simplifies the handling of unseen words, and gives rise to a simple smoothing heuristic. Namely, we replace unseen words in the test corpus with an out-of-vocabularly token, say `OOV`. This means that LMs can still work with a fixed vocabularly that consists of all training words, and the `OOV` token. Now we just need a way to estimate the probability of the `OOV` token to avoid the infinite perplexity problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[BAR]',\n",
       " '[OOV]',\n",
       " '[OOV]',\n",
       " 'to',\n",
       " 'the',\n",
       " '[OOV]',\n",
       " '[/BAR]',\n",
       " '[BAR]',\n",
       " '[/BAR]',\n",
       " '[BAR]']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OOV = '[OOV]'\n",
    "def replace_OOVs(vocab,data):\n",
    "    return [word if word in vocab else OOV for word in data]\n",
    "\n",
    "replace_OOVs(baseline.vocab, test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to (heuristically) estimate the `OOV` probability is to replace the first encounter of each word in the training set with the `OOV` token. Now we can estimate LMs as before, and will automatically get some estimate of the `OOV` probability. The underlying assumption of this heuristic is that the probability of unseen words is identical to the probability of encountering a new word. We illustrate the two operations of this method in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[OOV]', 'AA', '[OOV]', 'BB', 'AA']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inject_OOVs(data):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for word in data:\n",
    "        if word in seen:\n",
    "            result.append(word)\n",
    "        else:\n",
    "            result.append(OOV)\n",
    "            seen.add(word)\n",
    "    return result\n",
    "\n",
    "inject_OOVs([\"AA\",\"AA\",\"BB\",\"BB\",\"AA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply this to our training and test set, and create a new uniform model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "928.0000000011556"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_train = inject_OOVs(train)\n",
    "oov_vocab = set(oov_train)\n",
    "oov_test = replace_OOVs(oov_vocab, test)\n",
    "oov_baseline = UniformLM(oov_vocab)\n",
    "perplexity(oov_baseline,oov_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Language Models\n",
    "The uniform LM is obviously not good at modelling actual language. To improve upon this baseline, we can estimate the conditional n-gram distributions from the training data. To this end let us first introduce one parameter $\\param_{w,h}$ for each word $w$ and history $h$ of length $n - 1$, and define a parametrized language model $p_\\params$: \n",
    "\n",
    "$$\n",
    "\\prob_\\params(w|h) = \\param_{w,h}\n",
    "$$\n",
    "\n",
    "Training an n-gram LM amounts to estimating \\\\(\\params\\\\) from some training set \\\\(\\train=(w_1,\\ldots,w_n)\\\\).\n",
    "One way to do this is to choose the \\\\(\\params\\\\) that maximizes the log-likelihood of \\\\(\\train\\\\):\n",
    "$$\n",
    "\\params^* = \\argmax_\\params \\log p_\\params(\\train)\n",
    "$$\n",
    "\n",
    "As it turns out, this maximum-log-likelihood estimate (MLE) can calculated in closed form, simply by counting:\n",
    "$$\n",
    "\\param^*_{w,h} = \\frac{\\counts{\\train}{h,w}}{\\counts{\\train}{h}} \n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\counts{D}{e} = \\text{Count of } e \\text{ in }  D \n",
    "$$\n",
    "\n",
    "Many LM variants can be implemented simply by estimating the counts in the nominator and denominator differently. We therefore introduce an interface for such count-based LMs. This will help us later to implement LM variants by modifying the counts of a base-LM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CountLM(LanguageModel):\n",
    "    \"\"\"\n",
    "    A Language Model that uses counts of events and histories to calculate probabilities of words in context.\n",
    "    \"\"\"\n",
    "    @abc.abstractmethod\n",
    "    def counts(self, word_and_history):\n",
    "        pass\n",
    "    @abc.abstractmethod\n",
    "    def norm(self, history):\n",
    "        pass\n",
    "    \n",
    "    def probability(self, word, *history):\n",
    "        sub_history = tuple(history[-(self.order-1):]) if self.order > 1 else () \n",
    "        return self.counts((word,) + sub_history) / self.norm(sub_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use this to code up a generic NGram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramLM(CountLM):\n",
    "    def __init__(self, train, order):\n",
    "        \"\"\"\n",
    "        Create an NGram language model.\n",
    "        Args:\n",
    "            train: list of training tokens.\n",
    "            order: order of the LM.\n",
    "        \"\"\"\n",
    "        super().__init__(set(train), order)\n",
    "        self._counts = collections.defaultdict(float)\n",
    "        self._norm = collections.defaultdict(float)\n",
    "        for i in range(self.order, len(train)):\n",
    "            history = tuple(train[i - self.order + 1 : i])\n",
    "            word = train[i]\n",
    "            self._counts[(word,) + history] += 1.0\n",
    "            self._norm[history] += 1.0\n",
    "    def counts(self, word_and_history):\n",
    "        return self._counts[word_and_history]\n",
    "    def norm(self, history):\n",
    "        return self._norm[history]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train a unigram model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ce8ef60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unigram = NGramLM(oov_train,1)\n",
    "def plot_probabilities(lm, context = (), how_many = 10):    \n",
    "    probs = sorted([(word,lm.probability(word,*context)) for word in lm.vocab], key=lambda x:x[1], reverse=True)[:how_many]\n",
    "    plt.xticks(range(0,len(probs)),[word for word, _ in probs])\n",
    "    plt.bar(range(0,len(probs)),[prob for _,prob in probs],align='center')\n",
    "plot_probabilities(unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unigram LM has substantially reduced (and hence better) perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.11302463241343"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(unigram,oov_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also look at the language the unigram LM generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get', 'is', \"'\", 'was', 'It', 'on', 'with', 'man', 'that', 'in']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(unigram, [], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram LM\n",
    "\n",
    "The unigram model ignores any correlation between consecutive words in a sentence. The next best model to overcome this shortcoming is a bigram model. This model conditions the probability of the current word on the previous word. Let us construct such model from the training data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFbpJREFUeJzt3X/wZXV93/HnC1dQQDRQZrdddLEsNUMqJtiu24K6woxZ\nnMSlNWk3JmIdZehMNklLk4GZjN21M+0EYnEmWqM7MuYXLSS1wCaiXZJyg2iAL7+D7Le7KmxAkAYG\nEYnisrz7xzlfPXz3++N+93u/v84+HzN3OOd8Puec97l77+ue+znnfklVIUnql6OWugBJ0ugZ7pLU\nQ4a7JPWQ4S5JPWS4S1IPGe6S1ENDhXuSzUnGk+xNcukU7e9Ocl+Se5LcmeTcTtvDnbY7Rlm8JGlq\nme0+9yRHAXuB84DHgDFga1WNd/ocW1V/106/Ebiuqta3898A3lxVTy/MIUiSJhvmzH0DsK+q9lfV\nAeAaYEu3w0Swt44HnuzMZ8j9SJJGZJjQXQs80pl/tF32EkkuSLIHuBH41U5TATclGUty0XyKlSQN\nZ9WoNlRV1wPXJzkH+EPgDW3T2VX1eJKTaUJ+T1XdOqr9SpIONUy4fxN4XWf+lHbZlKrq1iSrkpxU\nVU9V1ePt8r9Nch3NMM8h4Z7EP3IjSXNUVZlq+TDDMmPA+iTrkhwNbAV2dTskOa0zfVa7w6eSHJvk\n+Hb5ccA7gQdmKHJOj+3bt895nYV4LJc6llMt1mEdK6GO5VTL4dQxk1nP3KvqYJJtwG6aD4OrqmpP\nkoub5toJvCfJhcAPgOeAf92uvhq4rj0rXwVcXVW7Z9unJGl+hhpzr6ov8qMx9Illn+5MXwFcMcV6\nDwE/Oc8aJUlztKJvUdy0adNSlwAsnzpg+dRiHS9lHS+1XOqA5VPLqOuY9UdMiyVJLZdaJGklSELN\n44KqJGmFMdwlqYcMd0nqIcNdknrIcJekHjLcJamHDHdJ6iHDXZJ6yHCXpB4y3CWphwx3Seohw12S\neshwl6QeMtwlqYcMd0nqIcNdknrIcJekHjLcJamHDHdJ6iHDXZJ6yHCXpB4aKtyTbE4ynmRvkkun\naH93kvuS3JPkziTnDruuJGn0UlUzd0iOAvYC5wGPAWPA1qoa7/Q5tqr+rp1+I3BdVa0fZt3ONmYu\nZIRWr17Ht7718GLtTpIWRBKqKlO1DXPmvgHYV1X7q+oAcA2wpdthIthbxwNPDrvuS9WiPJ54Yv8Q\nhy1JK9cw4b4WeKQz/2i77CWSXJBkD3Aj8KtzWVeSNFqrRrWhqroeuD7JW4E/BN4w963s6Exvah+S\nJIDBYMBgMBiq7zBj7huBHVW1uZ2/DKiqunyGdb5OMyRz+rDrNmPuizXsHmY7bkla7uY75j4GrE+y\nLsnRwFZg16QdnNaZPgugqp4aZl1J0ujNOixTVQeTbAN203wYXFVVe5Jc3DTXTuA9SS4EfgA8RxPi\n0667QMciSWrNOiyzWByWkaS5me+wjCRphTHcJamHDHdJ6iHDXZJ6yHCXpB4y3CWphwx3Seohw12S\neshwl6QeMtwlqYcMd0nqIcNdknrIcJekHjLcJamHDHdJ6iHDXZJ6yHCXpB4y3CWphwx3Seohw12S\neshwl6QeMtwnWbPmVJIsymPNmlOX+nAl9VSqaqlrACBJwWLVEqY77iQshzokaTZJqKpM1TbUmXuS\nzUnGk+xNcukU7e9Ncl/7uDXJmZ22h9vl9yS54/APQ5I0rFWzdUhyFPAJ4DzgMWAsyQ1VNd7p9g3g\nbVX1TJLNwE5gY9v2IrCpqp4ebemSpOkMc+a+AdhXVfur6gBwDbCl26GqbquqZ9rZ24C1neYMuR9J\n0ogME7prgUc684/y0vCe7EPAFzrzBdyUZCzJRXMvUZI0V7MOy8xFkncAHwDO6Sw+u6oeT3IyTcjv\nqapbp97Cjs70pvYhSQIYDAYMBoOh+s56t0ySjcCOqtrczl8GVFVdPqnfmcDngM1V9fVptrUdeLaq\nrpyizbtlJGkO5nu3zBiwPsm6JEcDW4Fdk3bwOppgf1832JMcm+T4dvo44J3AA4d3GJKkYc06LFNV\nB5NsA3bTfBhcVVV7klzcNNdO4MPAicAn05z6HqiqDcBq4LrmrJxVwNVVtXuhDkaS1PBHTIfWwXKo\nQ5JmM+8fMUmSVhbDXZJ6yHCXpB4y3CWphwx3Seohw12Seshwl6QeMtwlqYcMd0nqIcNdknrIcJek\nHjLcJamHDHdJ6iHDXZJ6yHCXpB4y3CWphwx3Seohw12Seshwl6QeMtwlqYcMd0nqIcNdknrIcJek\nHhoq3JNsTjKeZG+SS6dof2+S+9rHrUnOHHZdSdLopapm7pAcBewFzgMeA8aArVU13umzEdhTVc8k\n2QzsqKqNw6zb2UbBzLWMTpjuuJOwHOqQpNkkoaoyVdswZ+4bgH1Vtb+qDgDXAFu6Harqtqp6pp29\nDVg77LqSpNEbJtzXAo905h/lR+E9lQ8BXzjMdSVJI7BqlBtL8g7gA8A5h7eFHZ3pTe1DkgQwGAwY\nDAZD9R1mzH0jzRj65nb+MqCq6vJJ/c4EPgdsrqqvz2Xdts0xd0mag/mOuY8B65OsS3I0sBXYNWkH\nr6MJ9vdNBPuw60qSRm/WYZmqOphkG7Cb5sPgqqrak+Tiprl2Ah8GTgQ+mebU90BVbZhu3QU7GkkS\nMMSwzGJxWEaS5ma+wzKSpBXGcJekHjLcJamHDHdJ6iHDXZJ6yHCXpB4y3CWphwx3Seohw12Seshw\nl6QeMtwlqYcMd0nqIcNdknrIcJekHjLcJamHDHdJ6iHDXZJ6yHCXpB4y3CWphwx3Seohw12Seshw\nl6QeMtwlqYeGCvckm5OMJ9mb5NIp2t+Q5CtJvp/kkkltDye5L8k9Se4YVeGSpOmtmq1DkqOATwDn\nAY8BY0luqKrxTrengF8BLphiEy8Cm6rq6RHUK0kawjBn7huAfVW1v6oOANcAW7odqurJqroLeGGK\n9TPkfiRJIzJM6K4FHunMP9ouG1YBNyUZS3LRXIqTJB2eWYdlRuDsqno8yck0Ib+nqm6duuuOzvSm\n9iFJAhgMBgwGg6H6pqpm7pBsBHZU1eZ2/jKgquryKfpuB56tqiun2da07UmqOclfDGG6407CcqhD\nkmaThKrKVG3DDMuMAeuTrEtyNLAV2DXT/jo7PjbJ8e30ccA7gQeGrlySdFhmHZapqoNJtgG7aT4M\nrqqqPUkubpprZ5LVwJ3Aq4AXk/wacAZwMnBdc1bOKuDqqtq9UAcjSWrMOiyzWByWkaS5me+wjCRp\nhTHcJamHDHdJ6iHDXZJ6yHCXpB4y3CWphwx3Seohw12Seshwl6QeMtwlqYcMd0nqIcNdknrIcJek\nHjLcJamHDHdJ6iHDXZJ6yHCXpB4y3CWphwx3Seohw12Seshwl6QeMtwlqYcMd0nqoaHCPcnmJONJ\n9ia5dIr2NyT5SpLvJ7lkLutKkkYvVTVzh+QoYC9wHvAYMAZsrarxTp+/B6wDLgCerqorh123s42C\nmWsZnTDdcSdhOdQhSbNJQlVlqrZhztw3APuqan9VHQCuAbZ0O1TVk1V1F/DCXNeVJI3eMOG+Fnik\nM/9ou2wY81lXknSYVi11AS+1ozO9qX1IkgAGgwGDwWCovsOE+zeB13XmT2mXDWOO6+4YcrOSdOTZ\ntGkTmzZt+uH8Rz7ykWn7DjMsMwasT7IuydHAVmDXDP27g/tzXVeSNAKznrlX1cEk24DdNB8GV1XV\nniQXN821M8lq4E7gVcCLSX4NOKOqvjvVugt2NJIkYIhbIReLt0JK0tzM91ZISdIKY7hLUg8Z7pLU\nQ4a7JPWQ4S5JPWS4S1IPGe6S1EOGuyT1kOEuST1kuEtSDxnuktRDhrsk9ZDhLkk9ZLhLUg8Z7pLU\nQ4a7JPWQ4S5JPWS4S1IPGe6S1EOGuyT1kOEuST1kuEtSDxnuktRDQ4V7ks1JxpPsTXLpNH1+J8m+\nJPcm+anO8oeT3JfkniR3jKpwSdL0Vs3WIclRwCeA84DHgLEkN1TVeKfP+cBpVXV6krcAvwtsbJtf\nBDZV1dMjr16SNKVhztw3APuqan9VHQCuAbZM6rMF+AOAqrodeHWS1W1bhtyPJGlEhgndtcAjnflH\n22Uz9flmp08BNyUZS3LR4RYqSRrerMMyI3B2VT2e5GSakN9TVbdO3XVHZ3pT+5AkAQwGAwaDwVB9\nU1Uzd0g2AjuqanM7fxlQVXV5p8+ngJur6tp2fhx4e1U9MWlb24Fnq+rKKfZTzUn+YgjTHXcSlkMd\nkjSbJFRVpmobZlhmDFifZF2So4GtwK5JfXYBF7Y72wh8u6qeSHJskuPb5ccB7wQeOMzjkCQNadZh\nmao6mGQbsJvmw+CqqtqT5OKmuXZW1Y1J3pXka8BzwAfa1VcD1zVn5awCrq6q3QtzKJKkCbMOyywW\nh2UkaW7mOywjSVphDHdJ6iHDXZJ6yHCXpB4y3CWphwx3Seohw32ZWrPmVJIsymPNmlOXfR2S5sb7\n3A+tA+tYfnVIOpT3uUvSEcZwl6QeMtwlqYcMd60Ii3lhd6aLu8ulDmk2XlA9tA6s40ivY/palksd\nEnhBVZKOOIa7tEL5GwTNxGGZQ+vAOo70OqavZbnUsfi1ODy0HDksI2nBLJdvEEdiHTPxzP3QOrCO\nI72O6WtZLnUsfi3WsYzr8Mxdko4Uhrsk9ZDhLkk9ZLhLUg8NFe5JNicZT7I3yaXT9PmdJPuS3Jvk\nJ+eyriRptGYN9yRHAZ8Afhr4CeAXkvz4pD7nA6dV1enAxcCnhl13fgaj29S8DJa6gI7BUhfQGix1\nAa3BUhfQGix1Aa3BUhfQGix1AR2DpS6gNRjp1oY5c98A7Kuq/VV1ALgG2DKpzxbgDwCq6nbg1UlW\nD7nuPAxGt6l5GSx1AR2DpS6gNVjqAlqDpS6gNVjqAlqDpS6gNVjqAjoGS11AazDSrQ0T7muBRzrz\nj7bLhukzzLqSpBFbqAuqM/90SpK0oGb9hWqSjcCOqtrczl8GVFVd3unzKeDmqrq2nR8H3g68frZ1\nO9tYHj+VlaQVZLpfqK4aYt0xYH2SdcDjwFbgFyb12QX8MnBt+2Hw7ap6IsmTQ6w7Y4GSpLmbNdyr\n6mCSbcBummGcq6pqT5KLm+baWVU3JnlXkq8BzwEfmGndBTsaSRKwjP5wmCRpdPyF6gqT5NbDWGfL\naH9fsDwl2Z7kkqWuY6VJ8v4kaxZxf3+W5ITF2t9cLfT7Jcm6JH+9UNufsKLDPclDI97euiTfS3J3\nO39KkuvbX9fuS/KxJKs6/c9JcnuSPUkeTHJRu/xtSb4yadsvS/KtJGuSXJHk8cMJoqo65zAO7QKa\nH5FJU/k3LOItylX1M1X1ncXa32FYjPfLwg+ZVNWKfQDfGPH21gH3d+ZvBy5spwN8BriinV8D7Afe\n1M6fCNwJnN/23Q+8trOtnwb+vDP/H4FLDqPGZ2nuRPrTzrKPd+r8LeCrwL3AFcA/A54Cvg7cDbx+\nns/RrwPb2umPAX/RTr8D+CPgkzQX4f8a2N5Z77eABybqGuG/2W8C/xe4BfjvwCXAm4C/avf1OeDV\nbd+b2zpuB8aBs0f8+rkQuA+4B/h94GeA24C7aK47ndz22w5c1dbzNeBXFuC98eH2GGd6Xl4DvKd9\nTe1pXx/HjLiO6zqvhw+1yx5q3y/rgAeBne1r44uj3v8sz8c/BL7Q1veXwD8a9ftlmlrWtc/3H7XH\n/8fAK4CzaH7JNNbWtXpe+1mIJ3KxHsDtC/Ck399OnwsMJrW/Cvjb9h/iP9Hc5tltPxe4pZ3+KPAb\nnbbPAh/szG/n8ML9O8DbgF2dZR9vg+VEYLyz/ITOvv/liJ6jtwDXttO3tOH1MpoPq4uA17RtR7Xh\n9Y+nq2sEtZxFE6bHtP82+4D/0C47p+3zEeDKdvpm4Lfb6fOBm0b42jmjDY8fa+dfQ/uh0s5/sLPv\n7cCtNDc0nAQ8CbxshLX8kzaYXg4cD+xtw2ym5+WnRrX/SbVMvB5eQRPwJwLf4Efh/gPgjW2fa4H3\nLkAN0z0ff07zZ1Og+TX9xInKyN4v09SzDngR2NjOf4bmpOnLwEntsn9FcwPKYe9nRQ/LVNVbFnDz\nP0FzxtXd37PA3wDrp2qnOXM/o53+H7S3fSY5GngXzdnSfM10y+gzwPeSfCbJvwC+N4L9TXYX8OYk\nrwKepzkT/KfAW4EvAVuT3EVz9npG+1iout4KXFdVz7f/NjcAx9GE6sS1id+n+TCc8L86x7FuRHVA\n88H+J1X1NEBVfRt4bZL/neR+mjdv96v+56vqhap6CngCWD3CWs4GbqiqA1X1XZpblY9n5udloW5F\n/ndJ7qU5CTgFOH1S+0NVNTH+fBdw6gLUMNXz8UrgnwN/kuQe4NOM9t9gNn9TVbe101fzo7+/dVNb\nz28C/2A+OxjmPne91FBvgqq6K8lxSU6nCbjb2jf8KLxAc7Y84RXtPg8m2QCcB/w8sK2dHpmqeiHJ\nwzTjtF8G7qcZkjkN+D7NmfObq+o7ST4LvGIx6moN82/zfPvfgyz86//jwEer6vNJ3k5zxj65DmjO\n4hayliX5DUl7zOcCb6mq55PcTPta7eg+DwenaF+Q0mi+WT5dVWctwv6mMnnM/Vngq1V19qh2sKLP\n3BfYgzRf536ovcL/Wppx0kPa2/mvduYnzt63ttOjUDTj+WckeXmS19AGZZJjab4Gf5Hma+eZ7TrP\nAqO8O+FLNGeit9AML/xbmjP1E4DvAs+2fzju/Fnqmq9bgAuSHNN+k/hZmt9ZPJ1k4k3yPprx1KmM\nMvT+D/DzSU4EaP97AvBY2/7+Ee5rNl8GfrZ9Xo6nGfv/LtM/L6N+fUx4NU2APt/efbKxXd593hfj\ng2eq5+M54KEkP/fDQpKFer9MZV2SiZGH99J8Az65/REoSVYlOWPatYdguE+jqv4CeGWSX4Lmbhea\ncfTPVtX3gf8GvD/Jm9r2k2gu1nX/tMI1wC/RnNneMLrS6ps0F2EeaPdxd9t2AvBnSe6jCb5/36nj\nN5LcleT1I6jhSzQXlP+qqv4fzTDLLVV1P83FuomLRRNDANPVNS9VdQ/NOO39wOeBO2g+/N4PfLQd\nDngTzfUROPRsaWR3LFTVg8B/Bv6y/Vr9UWAH8D+TjNFcq5l29VHV0dZyJ83Qw300z8v9NENj0z0v\nvwd8KsndSY4ZYSlfBF6e5KvAfwEm7iDrHu+C3zUyw/Pxi8AH2/8HxQPAu9tVRv1+mco48MtJHqS5\nPvNx4OeAy9t/n3toLu4eNn/E1NH+mYQ/raoz2/m1wO8CP05zhnEj8OvV/PlikpwDXElzMQ/gY1W1\nc9I27wb2VNUvTlq+HXi2qq6cQ30nAXdW1UK94NQTSY6rqueSvJLmA/Wiqrp3qetaKkfi8+GY+6F+\n+DWxPUN+93Qd24tTG2ba2KjG9JL8fZrbpH57FNtT7+1sv9YfA/xe34NsCEfc8+GZe0eSU2i+Oj65\nkBdaklxB80OJ/1pVn16o/Ug6chnuktRDXlCVpB4y3CWphwx3Seohw12Seshwl6Qe+v8p52Bu8fD+\nwgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b789128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigram = NGramLM(oov_train,2)\n",
    "probs = sorted([(word,bigram.probability(word, \"I\")) for word in bigram.vocab], key=lambda x:x[1], reverse=True)[:10]\n",
    "plot_probabilities(bigram, ('I',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a more peaked distribution conditioned on \"I\" than in the case of the unigram model. Let us see how the bigram LM generates language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[BAR] She said nigga please don ' s love of a voice said Shit I [OOV] ' t play [/BAR] [BAR] [OOV] [OOV] [OOV] now [OOV] your own flow so damn\""
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(sample(bigram, ['[BAR]'], 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the bigram model improve perplexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(bigram,oov_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the bigram model has the problem we tried to avoid using the OOV preprocessing method above. The problem is that there are contexts in which the OOV word (and other words) hasn't been seen, and hence it receives 0 probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.probability(\"[OOV]\",\"money\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing\n",
    "\n",
    "The general problem is that maximum likelhood estimates will always underestimate the true probability of some words, and in turn overestimate the (context-dependent) probabilities of other words. To overcome this issue we aim to _smooth_ the probabilities and move mass from seen events to unseen events.\n",
    "\n",
    "### Laplace Smoothing\n",
    "\n",
    "The easiest way to overcome the problem of zero probabilities is to simply add pseudo counts to each event in the dataset (in a Bayesian setting this amounts to a maximum posteriori estimate under a dirichlet prior on parameters).\n",
    "\n",
    "$$\n",
    "\\param^{\\alpha}_{w,h} = \\frac{\\counts{\\train}{h,w} + \\alpha}{\\counts{\\train}{h} + \\alpha \\lvert V \\rvert } \n",
    "$$\n",
    "\n",
    "Let us implement this in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010660980810234541"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LaplaceLM(CountLM):\n",
    "    def __init__(self, base_lm, alpha):\n",
    "        super().__init__(base_lm.vocab, base_lm.order)\n",
    "        self.base_lm = base_lm\n",
    "        self.alpha = alpha\n",
    "    def counts(self, word_and_history):\n",
    "        return self.base_lm.counts(word_and_history) + self.alpha\n",
    "    def norm(self, history):\n",
    "        return self.base_lm.norm(history) + self.alpha * len(self.base_lm.vocab)\n",
    "\n",
    "laplace_bigram = LaplaceLM(bigram, 0.1) \n",
    "laplace_bigram.probability(\"[OOV]\",\"money\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give a better perplexity value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.51634219903197"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(laplace_bigram,oov_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted counts\n",
    "It is often useful to think of smoothing algorithms as un-smoothed Maximum-Likelhood estimators that work with *adjusted* n-gram counts in the numerator, and fixed history counts in the denominator. This allows us to see how counts from high-frequency words are reduced, and counts of unseen words increased. If these changes are too big, the smoothing method is likely not very effective.\n",
    "\n",
    "Let us reformulate the laplace LM using adjusted counts. Note that we since we have histories with count 0, we do need to increase the original denominator by a small \\\\(\\epsilon\\\\) to avoid division by zero. \n",
    "$$\n",
    "\\begin{split}\n",
    "\\counts{\\train,\\alpha}{h,w} &= \\param^{\\alpha}_{w,h} \\cdot (\\counts{\\train}{h} +  \\epsilon)\\\\\\\\\n",
    "\\counts{\\train,\\alpha}{h} &= \\counts{\\train}{h} + \\epsilon\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585.0, 564.5934362811013)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AdjustedLaplaceLM(CountLM):\n",
    "    def __init__(self, base_lm, alpha):\n",
    "        super().__init__(base_lm.vocab, base_lm.order)\n",
    "        self.base_lm = base_lm\n",
    "        self.alpha = alpha\n",
    "        self.eps = 0.000001\n",
    "    def counts(self, word_and_history):\n",
    "        history = word_and_history[1:]\n",
    "        word = word_and_history[0]\n",
    "        return 0.0 if word not in self.vocab else \\\n",
    "               (self.base_lm.counts(word_and_history) + self.alpha) / \\\n",
    "               (self.base_lm.norm(history) + self.alpha * len(self.base_lm.vocab)) * \\\n",
    "               (self.base_lm.norm(history) + self.eps)\n",
    "    def norm(self, history):\n",
    "        return self.base_lm.norm(history) + self.eps\n",
    "\n",
    "adjusted_laplace_bigram = AdjustedLaplaceLM(bigram, 0.1)\n",
    "bigram.counts((OOV,OOV)), adjusted_laplace_bigram.counts((OOV,OOV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that for high frequency words the absolute counts are altered quite substantially. This is unfortunate because for high frequency words we would expect the counts to be relatively accurate. Can we test more generally wether our adjusted counts are sensible?\n",
    "\n",
    "One option is to compare the adjusted counts to average counts in a held-out set. For example, for words of count 0 in the training set, how does their average count in the held-out set compare to their adjusted count in the smoothed model? To test this we need some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_product(lists):\n",
    "    \"\"\"\n",
    "    Returns a generator over all tuples in the cross product of the lists in `lists`.\n",
    "    Args:\n",
    "        lists: a list of lists\n",
    "    Returns:\n",
    "        generator that generates all tuples in the cross product.\n",
    "    \"\"\"\n",
    "    if len(lists) == 0:\n",
    "        yield ()\n",
    "    else:\n",
    "        for prev_tuple in cross_product(lists[1:]):\n",
    "            for head in lists[0]:\n",
    "                yield (head,) + prev_tuple\n",
    "\n",
    "def avg_counts(train_lm, test_lm, vocab):\n",
    "    \"\"\"\n",
    "    Calculate a dictionary from counts in the training-LM to counts in the test-LM. \n",
    "    \"\"\"\n",
    "    avg_test_counts = collections.defaultdict(float)\n",
    "    norm = collections.defaultdict(float)\n",
    "    for ngram in cross_product([list(train_lm.vocab)] * train_lm.order):\n",
    "        train_count = train_lm.counts(ngram)\n",
    "        test_count = test_lm.counts(ngram)\n",
    "        avg_test_counts[train_count] += test_count\n",
    "        norm[train_count] += 1.0\n",
    "    for c in avg_test_counts.keys():\n",
    "        avg_test_counts[c] /= norm[c]\n",
    "    return avg_test_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate a table of training counts, test counts, and smoothed counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0.0041     0.0096\n",
      "1     0.4125     0.1986\n",
      "2     1.0836     0.4627\n",
      "3     1.7626     0.6324\n",
      "4     2.9123     0.9495\n",
      "5     3.6724     1.0867\n"
     ]
    }
   ],
   "source": [
    "test_bigram = NGramLM(oov_test, 2)\n",
    "joint_vocab = set(oov_test + oov_train)\n",
    "avg_test_counts = avg_counts(bigram, test_bigram, joint_vocab)\n",
    "avg_laplace_counts = avg_counts(bigram, AdjustedLaplaceLM(bigram, 0.1), joint_vocab)\n",
    "for count in range(0, 6):\n",
    "    print(\"{} {:10.4f} {:10.4f}\".format(count, avg_test_counts[count], avg_laplace_counts[count]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "For a given context the smoothing methods discussed above shift mass uniformly across the words that haven't been seen in this context. This makes sense when the words are not in the vocabularly. However, when words are in the vocabularly but just have not been seen in the given context, we can do better because we can leverage statistics about the word from other contexts. In particular, we can *back-off* to the statistics of \\\\(n-1\\\\) grams. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0010548523206751054, 0.0010548523206751054)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_laplace_bigram.probability('skies','skies'), adjusted_laplace_bigram.probability('[/BAR]','skies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple technique to use the \\\\(n-1\\\\) gram statistics is interpolation. Here we  compose the probability of a word as the weighted sum of the probability of an \\\\(n\\\\)-gram model \\\\(p'\\\\) and a back-off \\\\(n-1\\\\) model \\\\(p''\\\\): \n",
    "\n",
    "$$\n",
    "\\prob_{\\alpha}(w_i|w_{i-n},\\ldots,w_{i-1}) = \\alpha \\cdot \\prob'(w_i|w_{i-n},\\ldots,w_{i-1}) + (1 - \\alpha) \\cdot \\prob''(w_i|w_{i-n+1},\\ldots,w_{i-1})\n",
    "$$\n",
    "\n",
    "A Python implementation of this model can be seen below. We also show how a more likely unigram now has a higher probability in a context it hasn't seen in before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0001662701118815446, 0.09764798462230231)"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class InterpolatedLM(LanguageModel):\n",
    "    def __init__(self, main, backoff, alpha):\n",
    "        super().__init__(main.vocab, main.order)\n",
    "        self.main = main\n",
    "        self.backoff = backoff\n",
    "        self.alpha = alpha\n",
    "    def probability(self, word, *history):\n",
    "        return self.alpha * self.main.probability(word,*history) + \\\n",
    "               (1.0 - self.alpha) * self.backoff.probability(word,*history)\n",
    "\n",
    "interpolated = InterpolatedLM(adjusted_laplace_bigram,unigram,0.01)\n",
    "interpolated.probability('skies','skies'), interpolated.probability('[/BAR]','skies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now find a good $\\alpha$ parameter to optimise for perplexity. Notice that in practice this should be done using a development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = np.arange(0,1.1,0.1)\n",
    "perplexities = [perplexity(InterpolatedLM(adjusted_laplace_bigram,unigram,alpha),oov_test) for alpha in alphas]\n",
    "plt.plot(alphas,perplexities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backoff \n",
    "Instead of combining probabilities for all words given a context, it makes sense to back-off only when no counts for a given event are available and rely on available counts where possible. \n",
    "\n",
    "<div class=\"newslide\"></div>\n",
    "A particularly simple, if not to say stupid, backoff method is [Stupid Backoff](http://www.aclweb.org/anthology/D07-1090.pdf). Let \\\\(w\\\\) be a word and \\\\(h_{n}\\\\) be an n-gram of length \\\\(n\\\\):  \n",
    "\n",
    "$$\n",
    "\\prob_{\\mbox{Stupid}}(w|h_n) = \n",
    "\\begin{cases}\n",
    "\\frac{\\counts{\\train}{h_n,w}}{\\counts{\\train}{h_n}}  &= \\mbox{if }\\counts{\\train}{h_n,w} > 0 \\\\\\\\\n",
    "\\prob_{\\mbox{Stupid}}(w|h_{n-1}) & \\mbox{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class StupidBackoff(LanguageModel):\n",
    "    def __init__(self, main, backoff, alpha):\n",
    "        super().__init__(main.vocab, main.order)\n",
    "        self.main = main\n",
    "        self.backoff = backoff\n",
    "        self.alpha = alpha\n",
    "    def probability(self, word, *history):\n",
    "        return self.main.probability(word,*history) \\\n",
    "          if self.main.counts((word,)+tuple(history)) > 0 \\\n",
    "          else self.alpha * self.backoff.probability(word,*history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the Stupid LM is very effective when it comes to *extrinsic* evaluations, but it doesn't represent a valid probability distribution: when you sum over the probabilities of all words given a history, the result may be larger than 1. This is the case because the main n-gram model probabilities for all non-zero count words already sum to 1. The fact that the probabilities sum to more than 1 makes perplexity values meaningless. The code below illustrates the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0711443177349667"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# val stupid = StupidBackoff(bigram,unigram, 0.1)\n",
    "# stupid.vocab.toSeq.map(w => stupid.probability(w, \"the\")).sum \n",
    "stupid = StupidBackoff(bigram, unigram, 0.1)\n",
    "sum([stupid.probability(word, 'the') for word in stupid.vocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are several \"proper backoff models\" that do not have this problem, e.g. the Katz-Backoff method. We refer to other material below for a deeper discussion of these.\n",
    "\n",
    "### Background Reading\n",
    "\n",
    "* Jurafsky & Martin, Speech and Language Processing: Chapter 4, N-Grams.\n",
    "* Bill MacCartney, Stanford NLP Lunch Tutorial: [Smoothing](http://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
