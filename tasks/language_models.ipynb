{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %cd ..\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "Language models (LMs) calculate the probability to see a given sequence of words, as defined through a [tokenization](todo) algorithm, in a given language or sub-language/domain/genre. For example, an English language model may assign a higher probability to seeing the sequence \"How are you?\" than to \"Wassup' dawg?\", and for a hip-hop language model this proportion may be reversed. <span class=\"summary\">Language models (LMs) calculate the probability to see a given sequence of words.\n",
    "\n",
    "There are several use cases for such models: \n",
    "\n",
    "* To filter out bad translations in machine translation.\n",
    "* To rank speech recognition output. \n",
    "* In concept-to-text generation.\n",
    "\n",
    "More formally, a language model is a stochastic process that models the probability \\\\(\\prob(w_1,\\ldots,w_d)\\\\) of observing sequences of words \\\\(w_1,\\ldots,w_d\\\\). We can, without loss of generality, decompose the probability of such sequences into<span class=\"summary\">Without loss of generality</span>  \n",
    "\n",
    "$$\n",
    "\\prob(w_1,\\ldots,w_d) = \\prob(w_1) \\prod_{i = 2}^d \\prob(w_i|w_1,\\ldots,w_{i-1}).\n",
    "$$\n",
    "\n",
    "This means that a language model can be defined by how it models the conditional probablity $\\prob(w_i|w_1,\\ldots,w_{i-1})$ of seeing a word \\\\(w_i\\\\) after having seen the *history* of previous words $w_1,\\ldots,w_{i-1}$. We also have to model the prior probability $\\prob(w_1)$, but it is easy to reduce this prior to a conditional probability as well.\n",
    "\n",
    "In practice it is common to define language models based on *equivalence classes* of histories instead of having different conditional distributions for each possible history. This overcomes sparsity and efficiency problems when working with full histories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Language Models\n",
    "\n",
    "The most common type of equivalence class relies on *truncating* histories $w_1,\\ldots,w_{i-1}$ to length $n-1$:\n",
    "$$\n",
    "\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\prob(w_i|w_{i-n},\\ldots,w_{i-1}).\n",
    "$$\n",
    "\n",
    "That is, the probability of a word only depends on the last $n-1$ previous words. We will refer to such model as a *n-gram language model*.\n",
    "\n",
    "## A Uniform Baseline LM\n",
    "\n",
    "*Unigram* models are the simplest 1-gram language models. That is, they model the conditional probability of word using the prior probability of seeing that word:\n",
    "$$\n",
    "\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\prob(w_i).\n",
    "$$\n",
    "\n",
    "To setup datasets and as baseline for more complex language models, we first introduce the simplest instantituation of a unigram model: a *uniform* language model which assigns the same prior probability to each word. That is, given a *vocabulary* of words \\\\(\\vocab\\\\), the uniform LM is defined as:\n",
    "\n",
    "$$\n",
    "\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\frac{1}{|\\vocab|}.\n",
    "$$\n",
    "\n",
    "Let us \"train\" and test such a language model on the OHHLA corpus. First we need to load this corpus. Below we focus on a subset to make our code more responsive and to allow us to test models more quickly. Check the [loading from OHHLA](load_ohhla.ipynb) notebook to see how `load_albums` and `words` are defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[BAR] Can ' t even call this a blues song [/BAR] [BAR] It ' s been so long [/BAR] [BAR] Neither one of us was wrong or anything like that [/BAR] [BAR] It seems like\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statnlpbook.util as util\n",
    "util.execute_notebook('load_ohhla.ipynb')\n",
    "docs = load_albums(j_live)\n",
    "trainDocs, testDocs = docs[:len(docs)//2], docs[len(docs)//2:] \n",
    "train = words(trainDocs)\n",
    "test = words(testDocs)\n",
    "\" \".join(train[0:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a uniform language model. Language models in this book implement the `LanguageModel` [abstract base class](https://docs.python.org/3/library/abc.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import abc \n",
    "class LanguageModel(metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocab: the vocabulary underlying this language model. Should be a set of words.\n",
    "        order: history length (-1).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab, order):\n",
    "        self.vocab = vocab\n",
    "        self.order = order\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def probability(self, word,*history):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word: the word we need the probability of\n",
    "            history: words to condition on.\n",
    "        Returns:\n",
    "            the probability p(w|history)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important method we have to provide is `probability(word,history)` which returns the probability of a word given a history. Let us implement a uniform LM using this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003912363067292645"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class UniformLM(LanguageModel):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__(vocab, 1)\n",
    "    def probability(self, word,*history):\n",
    "        return 1.0 / len(self.vocab) if word in self.vocab else 0.0\n",
    "    \n",
    "vocab = set(train)\n",
    "baseline = UniformLM(vocab)\n",
    "baseline.probability(\"call\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "It is instructive and easy to sample language from a language model. In many, but not all, cases the more natural the generated language of an LM looks, the better this LM is.\n",
    "\n",
    "To sample from an LM one simply needs to iteratively sample from the LM conditional probability over words, and add newly sampled words to the next history. The only challenge in implementing this is to sample from a categorical distribution over words. Here we provide this functionality via `np.random.choice` from [numpy](http://www.numpy.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['guns',\n",
       " 'will',\n",
       " 'wait',\n",
       " 'thoughts',\n",
       " 'survive',\n",
       " 'cause',\n",
       " 'nowadays',\n",
       " 'there',\n",
       " 'rise',\n",
       " 'sexy']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample(lm, init, amount):\n",
    "    words = list(lm.vocab)\n",
    "    result = []\n",
    "    result += init\n",
    "    for _ in range(0, amount):\n",
    "        history = result[-(lm.order-1):]\n",
    "        probs = [lm.probability(word, *history) for word in words]\n",
    "        sampled = np.random.choice(words,p=probs)\n",
    "        result.append(sampled)\n",
    "    return result\n",
    "\n",
    "sample(baseline, [], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "How do we determine the quality of an (n-gram) LM? One way is through *extrinsic* evaluation: assess how much the LM improves performance on *downstream tasks* such as machine translation or speech recognition. Arguably this is the most important measure of LM quality, but it can be costly as re-training such systems may take days, and when we seek to develop general-purpose LMs we may have to evaluate performance on several tasks. This is problematic when one wants to iteratively improve LMs and test new models and parameters. It is hence useful to find *intrinsic* means of evaluation that assess the stand-alone quality of LMs with minimal overhead.\n",
    "\n",
    "One intrinsic way is to measure how well the LM plays the \"Shannon Game\": Predict what the next word in actual context should be, and win if your predictions match the words in an actual corpus. This can be formalized  using the notion of *perplexity* of the LM on a given dataset. Given a test sequence \\\\(w_1,\\ldots,w_T\\\\) of \\\\(T\\\\) words, we calculate the perplexity \\\\(\\perplexity\\\\) as follows:\n",
    "\n",
    "$$\n",
    "\\perplexity(w_1,\\ldots,w_T) = \\prob(w_1,\\ldots,w_T)^{-\\frac{1}{T}} = \\sqrt[T]{\\prod_i^T \\frac{1}{\\prob(w_i|w_{i-n},\\ldots,w_{i-1})}}\n",
    "$$\n",
    "\n",
    "We can implement a perplexity function based on the `LanguageModel` interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def perplexity(lm, data):\n",
    "    log_prob = 0.0\n",
    "    history_order = lm.order - 1\n",
    "    for i in range(history_order, len(data)):\n",
    "        history = data[i - history_order : i]\n",
    "        word = data[i]\n",
    "        p = lm.probability(word, *history)\n",
    "        log_prob += math.log(p) if p > 0.0 else float(\"-inf\")\n",
    "    return math.exp(-log_prob / (len(data) - history_order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the uniform model does on our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(baseline, test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Vocabularly Words\n",
    "The problem in the above example is that the baseline model assigns zero probability to words that are not in the vocabulary. Test sets will usually contain such words, and this leads to the above result of infinite perplexity. For example, the following three words do not appear in the training set vocabulary `vocab` and hence receive 0 probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('send', 0.0), ('corrections', 0.0), ('typist', 0.0)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(w,baseline.probability(w)) for w in test if w not in vocab][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Long Tail\n",
    "The fact that we regularly encounter new words in our corpus is a common phenomenon not specific to our corpus. Generally we will see a few words that appear repeatedly, and a long tail of words that appear only a few times. While each individual long-tail word is rare, the probability of seeing any long-tail word is quite high (the long tail covers a lot of the frequency mass).\n",
    "\n",
    "Let us observe this phenomenon for our data: we will rank the words according to their frequency, and plot this frequency against the rank. Let us first extracted the sorted counts and their ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "counts = collections.defaultdict(int)\n",
    "for word in train:\n",
    "    counts[word] += 1\n",
    "sorted_counts = sorted(counts.values(),reverse=True)\n",
    "ranks = range(1,len(sorted_counts)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the counts against their rank. Play around with the x and y scale and change them to `'log'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10aba9e48>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ad06160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xscale('linear')\n",
    "plt.yscale('linear')\n",
    "plt.plot(ranks, sorted_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In log-space such rank vs frequency graphs resemble linear functions. This observation is known as *Zipf's Law*, and can be formalized as follows. Let \\\\(r\\_w\\\\) be the rank of a word \\\\(w\\\\), and \\\\(f\\_w\\\\) its frequency, then we have:\n",
    "\n",
    "$$\n",
    "  f_w \\propto \\frac{1}{r_w}.\n",
    "$$\n",
    "\n",
    "## Inserting Out-of-Vocabularly Tokens\n",
    "The long tail of infrequent words is a problem for LMs because it means there will always be words with zero counts in your training set. There are various solutions to this problem. For example, when it comes to calculating the LM perplexity we could remove words that do not appear in the training set. This overcomes the problem of infinite perplexity but doesn't solve the actual issue: the LM assigns too low probability to unseen words. Moreover, the problem only gets worse when one considers n-gram models with larger \\\\(n\\\\), because these will encounter many unseen n-grams, which, when removed, will only leave small fractions of the original sentences.\n",
    "\n",
    "The principled solution to this problem is smoothing, and we will discuss it in more detail later. Before we get there we present a simple preprocessing step that generally simplifies the handling of unseen words, and gives rise to a simple smoothing heuristic. Namely, we replace unseen words in the test corpus with an out-of-vocabularly token, say `OOV`. This means that LMs can still work with a fixed vocabularly that consists of all training words, and the `OOV` token. Now we just need a way to estimate the probability of the `OOV` token to avoid the infinite perplexity problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[BAR]',\n",
       " '[OOV]',\n",
       " '[OOV]',\n",
       " 'to',\n",
       " 'the',\n",
       " '[OOV]',\n",
       " '[/BAR]',\n",
       " '[BAR]',\n",
       " '[/BAR]',\n",
       " '[BAR]']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OOV = '[OOV]'\n",
    "def replace_OOVs(vocab,data):\n",
    "    return [word if word in vocab else OOV for word in data]\n",
    "\n",
    "replace_OOVs(baseline.vocab, test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to (heuristically) estimate the `OOV` probability is to replace the first encounter of each word in the training set with the `OOV` token. Now we can estimate LMs as before, and will automatically get some estimate of the `OOV` probability. The underlying assumption of this heuristic is that the probability of unseen words is identical to the probability of encountering a new word. We illustrate the two operations of this method in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[OOV]', 'AA', '[OOV]', 'BB', 'AA']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inject_OOVs(data):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for word in data:\n",
    "        if word in seen:\n",
    "            result.append(word)\n",
    "        else:\n",
    "            result.append(OOV)\n",
    "            seen.add(word)\n",
    "    return result\n",
    "\n",
    "inject_OOVs([\"AA\",\"AA\",\"BB\",\"BB\",\"AA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply this to our training and test set, and create a new uniform model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "928.0000000011556"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_train = inject_OOVs(train)\n",
    "oov_vocab = set(oov_train)\n",
    "oov_test = replace_OOVs(oov_vocab, test)\n",
    "oov_baseline = UniformLM(oov_vocab)\n",
    "perplexity(oov_baseline,oov_test) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
