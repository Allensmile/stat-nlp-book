{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# %cd .. \n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\Xs}{\\mathcal{X}}\n",
    "\\newcommand{\\Ys}{\\mathcal{Y}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\repr}{\\mathbf{f}}\n",
    "\\newcommand{\\repry}{\\mathbf{g}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\DeclareMathOperator{\\argmin}{argmin}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "\\newcommand{\\indi}{\\mathbb{I}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import statnlpbook.util as util\n",
    "util.execute_notebook('structured_prediction.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Structured Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "No emerging unified _theory of NLP_, most textbooks and courses explain NLP as \n",
    "\n",
    "> a collection of problems, techniques, ideas, frameworks, etc. that really are not tied together in any reasonable way other than the fact that they have to do with NLP.\n",
    ">\n",
    ">  -- <cite>[Hal Daume](http://nlpers.blogspot.co.uk/2012/12/teaching-intro-grad-nlp.html)</cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "but there is a reoccuring pattern ... the\n",
    "## Structured Prediction Recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Problem Signature \n",
    "\n",
    "* Given given some input structure \\\\(\\x \\in \\Xs \\\\), such as a word, sentence, or document ...  \n",
    "* predict an **output structure** \\\\(\\y \\in \\Ys \\\\), such as a class label, a sentence or syntactic tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Approach\n",
    "\n",
    " * Define a parametrized _model_ \\\\(s_\\params(\\x,\\y)\\\\) that measures the _match_ of a given \\\\(\\x\\\\) and \\\\(\\y\\\\) using _representations_ $\\repr(\\x)$ and $\\repry(\\y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * _Learn_ the parameters \\\\(\\params\\\\) from the training data \\\\(\\train\\\\) (a _continuous optimization problem_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * Given an input \\\\(\\x\\\\) find the highest-scoring output structure $$ \\y^* = \\argmax_{\\y\\in\\Ys} s(\\x,\\y) $$ (a _discrete optimization problem_).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Good NLPers** combine **three skills** in accordance with this recipe: \n",
    "\n",
    "* modelling,\n",
    "* continuous optimization and\n",
    "* discrete optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "* Difficult to show meaningful example without going into depth (as we will later)\n",
    "* Instead consider a toy example that uses same ingredients and steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Task\n",
    "\"Machine translation\" from Enlish into Japanese sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I want coffe', 'コヒがほし'), ('where is the restroom?', 'コヒがほし')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('As he crossed toward the pharmacy at the corner he involuntarily turned his head because of a burst of light that had ricocheted from his temple',\n",
       "  'コヒがほし')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Too difficult! Let's make simplified\n",
    "### Assumptions\n",
    "* There are only 4 target Japanese sentences we care about.\n",
    "* The lengths of the source English and target Japanese sentence are sufficient representations of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our \n",
    "### Output Space\n",
    "is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['コヒがほし', 'コヒがほし', 'コヒがほし']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representation\n",
    "* $\\repr(\\x)=|\\x|$ \n",
    "* $\\repry(\\y)=|\\y|$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Model\n",
    "$$\n",
    "s_\\param(\\x,\\y) = \\param |\\repr(\\x) - \\repry(\\y)|\n",
    "$$\n",
    "\n",
    "Note: if $\\param>0$ bigger difference in length is rewarded, else it is penalised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us inspect this model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s(-1., \"Blah\", \"Blub Blub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How to estimate $\\param \\in \\{-1,1\\}$? Let us define a \n",
    "### Loss Function\n",
    "$$\n",
    "l(\\param)=\\sum_{(\\x,\\y) \\in \\train} \\indi(\\y=\\y'_{\\param})\n",
    "$$\n",
    "where $\\y'_{\\param} \\in \\Ys$ is highest scoring translation\n",
    "$$\\y'_{\\param}=\\argmax_\\y s_\\param(\\x,\\y).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I want coffe', 'コヒがほし'),\n",
       " ('where is the restroom?', 'This is not Japanese at all')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(1.0, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Learning\n",
    "is as simple as choosing the parameter with lowest loss:\n",
    "\n",
    "$$\n",
    "\\param^* = \\argmin_{\\param \\in \\{-1,1\\}} l(\\param) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_star = 1.0 if loss(1.0, train) < loss(-1.0, train) else -1.0\n",
    "theta_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Prediction\n",
    "same thing, just in $\\Ys$:\n",
    "\n",
    "$$\\y'_{\\param}=\\argmax_\\y s_\\param(\\x,\\y).$$\n",
    "\n",
    "Seen before? Yes, training often involves prediction in inner loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What the hell, Sebastian! Fix this example'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(1, test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In Practice\n",
    "1. Feature representations and scoring functions are **more elaborate**\n",
    "   1. involve several **non-linear** transformations of both input and output. \n",
    "\n",
    "1. Parameter space usually **multi-dimensional** (millions of dimensions). \n",
    "   1. **Impossible to search exhaustively**.\n",
    "   2. **Numeric optimisation algorithms** (often SGD).\n",
    "\n",
    "1. Output space often exponentional sized (e.g. *all* Jap. sentences)\n",
    "   1. **Impossible to search exhaustively**.\n",
    "   2. **Discrete optimisation algorithms** (Dynamic Programming, Greedy, integer linear programming)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
