{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# %cd .. \n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "Language models (LMs) calculate the probability to see a given sequence of words, as defined through a [tokenization](todo) algorithm, in a given language or sub-language/domain/genre. For example, an English language model may assign a higher probability to seeing the sequence \"How are you?\" than to \"Wassup' dawg?\", and for a hip-hop language model this proportion may be reversed. <span class=\"summary\">Language models (LMs) calculate the probability to see a given sequence of words.\n",
    "\n",
    "There are several use cases for such models: \n",
    "\n",
    "* To filter out bad translations in machine translation.\n",
    "* To rank speech recognition output. \n",
    "* In concept-to-text generation.\n",
    "\n",
    "More formally, a language model is a stochastic process that models the probability \\\\(\\prob(w_1,\\ldots,w_d)\\\\) of observing sequences of words \\\\(w_1,\\ldots,w_d\\\\). We can, without loss of generality, decompose the probability of such sequences into  \n",
    "\n",
    "$$\n",
    "\\prob(w_1,\\ldots,w_d) = \\prob(w_1) \\prod_{i = 2}^d \\prob(w_i|w_1,\\ldots,w_{i-1}).\n",
    "$$\n",
    "\n",
    "This means that a language model can be defined by how it models the conditional probablity $\\prob(w_i|w_1,\\ldots,w_{i-1})$ of seeing a word \\\\(w_i\\\\) after having seen the *history* of previous words $w_1,\\ldots,w_{i-1}$. We also have to model the prior probability $\\prob(w_1)$, but it is easy to reduce this prior to a conditional probability as well.\n",
    "\n",
    "In practice it is common to define language models based on *equivalence classes* of histories instead of having different conditional distributions for each possible history. This overcomes sparsity and efficiency problems when working with full histories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Language Models\n",
    "\n",
    "The most common type of equivalence class relies on *truncating* histories $w_1,\\ldots,w_{i-1}$ to length $n-1$:\n",
    "$$\n",
    "\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\prob(w_i|w_{i-n},\\ldots,w_{i-1}).\n",
    "$$\n",
    "\n",
    "That is, the probability of a word only depends on the last $n-1$ previous words. We will refer to such model as a *n-gram language model*.\n",
    "\n",
    "## A Uniform Baseline LM\n",
    "\n",
    "*Unigram* models are the simplest 1-gram language models. That is, they model the conditional probability of word using the prior probability of seeing that word:\n",
    "$$\n",
    "\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\prob(w_i).\n",
    "$$\n",
    "\n",
    "To setup datasets and as baseline for more complex language models, we first introduce the simplest instantituation of a unigram model: a *uniform* language model which assigns the same prior probability to each word. That is, given a *vocabulary* of words \\\\(\\vocab\\\\), the uniform LM is defined as:\n",
    "\n",
    "$$\n",
    "\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\frac{1}{|\\vocab|}.\n",
    "$$\n",
    "\n",
    "Let us \"train\" and test such a language model on the OHHLA corpus. First we need to load this corpus. Below we focus on a subset to make our code more responsive and to allow us to test models more quickly. Check the [loading from OHHLA](load_ohhla.ipynb) notebook to see how `load_albums` and `words` are defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[BAR] Can ' t even call this a blues song [/BAR] [BAR] It ' s been so long [/BAR] [BAR] Neither one of us was wrong or anything like that [/BAR] [BAR] It seems like\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statnlpbook.util as util\n",
    "util.execute_notebook('load_ohhla.ipynb')\n",
    "docs = load_albums(j_live)\n",
    "trainDocs, testDocs = docs[:len(docs)//2], docs[len(docs)//2:] \n",
    "train = words(trainDocs)\n",
    "test = words(testDocs)\n",
    "\" \".join(train[0:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a uniform language model. Language models in this book implement the `LanguageModel` [abstract base class](https://docs.python.org/3/library/abc.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import abc \n",
    "class LanguageModel(metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocab: the vocabulary underlying this language model. Should be a set of words.\n",
    "        order: history length (-1).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab, order):\n",
    "        self.vocab = vocab\n",
    "        self.order = order\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def probability(self, word,*history):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word: the word we need the probability of\n",
    "            history: words to condition on.\n",
    "        Returns:\n",
    "            the probability p(w|history)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important method we have to provide is `probability(word,history)` which returns the probability of a word given a history. Let us implement a uniform LM using this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003912363067292645"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class UniformLM(LanguageModel):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__(vocab, 1)\n",
    "    def probability(self, word,*history):\n",
    "        return 1.0 / len(self.vocab) if word in self.vocab else 0.0\n",
    "    \n",
    "vocab = set(train)\n",
    "baseline = UniformLM(vocab)\n",
    "baseline.probability(\"call\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "It is instructive and easy to sample language from a language model. In many, but not all, cases the more natural the generated language of an LM looks, the better this LM is.\n",
    "\n",
    "To sample from an LM one simply needs to iteratively sample from the LM conditional probability over words, and add newly sampled words to the next history. The only challenge in implementing this is to sample from a categorical distribution over words. Here we provide this functionality via `np.random.choice` from [numpy](http://www.numpy.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fun',\n",
       " 'PlayStation',\n",
       " 'caps',\n",
       " 'fruit',\n",
       " 'hate',\n",
       " 'pulled',\n",
       " 'About',\n",
       " 'victim',\n",
       " 'played',\n",
       " 'floor']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample(lm, init, amount):\n",
    "    words = list(lm.vocab)\n",
    "    result = []\n",
    "    result += init\n",
    "    for _ in range(0, amount):\n",
    "        history = result[-(lm.order-1):]\n",
    "        probs = [lm.probability(word, *history) for word in words]\n",
    "        sampled = np.random.choice(words,p=probs)\n",
    "        result.append(sampled)\n",
    "    return result\n",
    "\n",
    "sample(baseline, [], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "How do we determine the quality of an (n-gram) LM? One way is through *extrinsic* evaluation: assess how much the LM improves performance on *downstream tasks* such as machine translation or speech recognition. Arguably this is the most important measure of LM quality, but it can be costly as re-training such systems may take days, and when we seek to develop general-purpose LMs we may have to evaluate performance on several tasks. This is problematic when one wants to iteratively improve LMs and test new models and parameters. It is hence useful to find *intrinsic* means of evaluation that assess the stand-alone quality of LMs with minimal overhead.\n",
    "\n",
    "One intrinsic way is to measure how well the LM plays the \"Shannon Game\": Predict what the next word in actual context should be, and win if your predictions match the words in an actual corpus. This can be formalized  using the notion of *perplexity* of the LM on a given dataset. Given a test sequence \\\\(w_1,\\ldots,w_T\\\\) of \\\\(T\\\\) words, we calculate the perplexity \\\\(\\perplexity\\\\) as follows:\n",
    "\n",
    "$$\n",
    "\\perplexity(w_1,\\ldots,w_T) = \\prob(w_1,\\ldots,w_T)^{-\\frac{1}{T}} = \\sqrt[T]{\\prod_i^T \\frac{1}{\\prob(w_i|w_{i-n},\\ldots,w_{i-1})}}\n",
    "$$\n",
    "\n",
    "We can implement a perplexity function based on the `LanguageModel` interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def perplexity(lm, data):\n",
    "    log_prob = 0.0\n",
    "    history_order = lm.order - 1\n",
    "    for i in range(history_order, len(data)):\n",
    "        history = data[i - history_order : i]\n",
    "        word = data[i]\n",
    "        p = lm.probability(word, *history)\n",
    "        log_prob += math.log(p) if p > 0.0 else float(\"-inf\")\n",
    "    return math.exp(-log_prob / (len(data) - history_order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the uniform model does on our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(baseline, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Vocabularly Words\n",
    "The problem in the above example is that the baseline model assigns zero probability to words that are not in the vocabulary. Test sets will usually contain such words, and this leads to the above result of infinite perplexity. For example, the following three words do not appear in the training set vocabulary `vocab` and hence receive 0 probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('send', 0.0), ('corrections', 0.0), ('typist', 0.0)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(w,baseline.probability(w)) for w in test if w not in vocab][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Long Tail\n",
    "The fact that we regularly encounter new words in our corpus is a common phenomenon not specific to our corpus. Generally we will see a few words that appear repeatedly, and a long tail of words that appear only a few times. While each individual long-tail word is rare, the probability of seeing any long-tail word is quite high (the long tail covers a lot of the frequency mass).\n",
    "\n",
    "Let us observe this phenomenon for our data: we will rank the words according to their frequency, and plot this frequency against the rank. Let us first extracted the sorted counts and their ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "counts = collections.defaultdict(int)\n",
    "for word in train:\n",
    "    counts[word] += 1\n",
    "sorted_counts = sorted(counts.values(),reverse=True)\n",
    "ranks = range(1,len(sorted_counts)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the counts against their rank. Play around with the x and y scale and change them to `'log'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "<style>\n",
       "\n",
       "</style>\n",
       "\n",
       "<div id=\"fig_el5466445855727843659457194\"></div>\n",
       "<script>\n",
       "function mpld3_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(mpld3) !== \"undefined\" && mpld3._mpld3IsLoaded){\n",
       "   // already loaded: just create the figure\n",
       "   !function(mpld3){\n",
       "       \n",
       "       mpld3.draw_figure(\"fig_el5466445855727843659457194\", {\"width\": 480.0, \"plugins\": [{\"type\": \"reset\"}, {\"button\": true, \"enabled\": false, \"type\": \"zoom\"}, {\"button\": true, \"enabled\": false, \"type\": \"boxzoom\"}], \"id\": \"el546644585572784\", \"axes\": [{\"lines\": [{\"coordinates\": \"data\", \"xindex\": 0, \"dasharray\": \"10,0\", \"color\": \"#0000FF\", \"id\": \"el546644586961272\", \"alpha\": 1, \"zorder\": 2, \"data\": \"data01\", \"yindex\": 1, \"linewidth\": 1.0}], \"images\": [], \"xdomain\": [0.0, 3000.0], \"ylim\": [0.0, 1400.0], \"axesbg\": \"#FFFFFF\", \"yscale\": \"linear\", \"id\": \"el546644587386976\", \"zoomable\": true, \"xlim\": [0.0, 3000.0], \"collections\": [], \"ydomain\": [0.0, 1400.0], \"axesbgalpha\": null, \"markers\": [], \"sharex\": [], \"paths\": [], \"xscale\": \"linear\", \"axes\": [{\"grid\": {\"gridOn\": false}, \"tickformat\": null, \"position\": \"bottom\", \"tickvalues\": null, \"nticks\": 7, \"scale\": \"linear\", \"fontsize\": 10.0}, {\"grid\": {\"gridOn\": false}, \"tickformat\": null, \"position\": \"left\", \"tickvalues\": null, \"nticks\": 8, \"scale\": \"linear\", \"fontsize\": 10.0}], \"texts\": [], \"bbox\": [0.125, 0.125, 0.775, 0.775], \"sharey\": []}], \"data\": {\"data01\": [[1.0, 1255.0], [2.0, 1255.0], [3.0, 509.0], [4.0, 390.0], [5.0, 265.0], [6.0, 243.0], [7.0, 208.0], [8.0, 187.0], [9.0, 146.0], [10.0, 142.0], [11.0, 136.0], [12.0, 113.0], [13.0, 109.0], [14.0, 103.0], [15.0, 98.0], [16.0, 94.0], [17.0, 88.0], [18.0, 82.0], [19.0, 71.0], [20.0, 67.0], [21.0, 65.0], [22.0, 61.0], [23.0, 60.0], [24.0, 58.0], [25.0, 58.0], [26.0, 57.0], [27.0, 56.0], [28.0, 55.0], [29.0, 55.0], [30.0, 53.0], [31.0, 50.0], [32.0, 47.0], [33.0, 45.0], [34.0, 45.0], [35.0, 45.0], [36.0, 44.0], [37.0, 42.0], [38.0, 42.0], [39.0, 41.0], [40.0, 40.0], [41.0, 40.0], [42.0, 40.0], [43.0, 40.0], [44.0, 36.0], [45.0, 34.0], [46.0, 34.0], [47.0, 32.0], [48.0, 32.0], [49.0, 30.0], [50.0, 29.0], [51.0, 29.0], [52.0, 28.0], [53.0, 27.0], [54.0, 26.0], [55.0, 26.0], [56.0, 26.0], [57.0, 25.0], [58.0, 25.0], [59.0, 25.0], [60.0, 25.0], [61.0, 24.0], [62.0, 24.0], [63.0, 23.0], [64.0, 22.0], [65.0, 22.0], [66.0, 21.0], [67.0, 21.0], [68.0, 21.0], [69.0, 20.0], [70.0, 19.0], [71.0, 19.0], [72.0, 19.0], [73.0, 18.0], [74.0, 18.0], [75.0, 18.0], [76.0, 18.0], [77.0, 18.0], [78.0, 17.0], [79.0, 17.0], [80.0, 17.0], [81.0, 17.0], [82.0, 17.0], [83.0, 17.0], [84.0, 16.0], [85.0, 16.0], [86.0, 16.0], [87.0, 16.0], [88.0, 16.0], [89.0, 16.0], [90.0, 16.0], [91.0, 16.0], [92.0, 16.0], [93.0, 16.0], [94.0, 15.0], [95.0, 15.0], [96.0, 15.0], [97.0, 15.0], [98.0, 14.0], [99.0, 14.0], [100.0, 14.0], [101.0, 14.0], [102.0, 14.0], [103.0, 14.0], [104.0, 14.0], [105.0, 13.0], [106.0, 13.0], [107.0, 13.0], [108.0, 13.0], [109.0, 13.0], [110.0, 13.0], [111.0, 13.0], [112.0, 13.0], [113.0, 13.0], [114.0, 12.0], [115.0, 12.0], [116.0, 12.0], [117.0, 12.0], [118.0, 12.0], [119.0, 11.0], [120.0, 11.0], [121.0, 11.0], [122.0, 11.0], [123.0, 11.0], [124.0, 11.0], [125.0, 11.0], [126.0, 11.0], [127.0, 11.0], [128.0, 11.0], [129.0, 11.0], [130.0, 11.0], [131.0, 11.0], [132.0, 11.0], [133.0, 11.0], [134.0, 11.0], [135.0, 11.0], [136.0, 11.0], [137.0, 10.0], [138.0, 10.0], [139.0, 10.0], [140.0, 10.0], [141.0, 10.0], [142.0, 10.0], [143.0, 10.0], [144.0, 10.0], [145.0, 10.0], [146.0, 10.0], [147.0, 10.0], [148.0, 10.0], [149.0, 10.0], [150.0, 10.0], [151.0, 10.0], [152.0, 9.0], [153.0, 9.0], [154.0, 9.0], [155.0, 9.0], [156.0, 9.0], [157.0, 9.0], [158.0, 9.0], [159.0, 9.0], [160.0, 9.0], [161.0, 9.0], [162.0, 9.0], [163.0, 9.0], [164.0, 9.0], [165.0, 9.0], [166.0, 9.0], [167.0, 9.0], [168.0, 9.0], [169.0, 9.0], [170.0, 8.0], [171.0, 8.0], [172.0, 8.0], [173.0, 8.0], [174.0, 8.0], [175.0, 8.0], [176.0, 8.0], [177.0, 8.0], [178.0, 8.0], [179.0, 8.0], [180.0, 8.0], [181.0, 8.0], [182.0, 8.0], [183.0, 8.0], [184.0, 8.0], [185.0, 8.0], [186.0, 8.0], [187.0, 8.0], [188.0, 8.0], [189.0, 7.0], [190.0, 7.0], [191.0, 7.0], [192.0, 7.0], [193.0, 7.0], [194.0, 7.0], [195.0, 7.0], [196.0, 7.0], [197.0, 7.0], [198.0, 7.0], [199.0, 7.0], [200.0, 7.0], [201.0, 7.0], [202.0, 7.0], [203.0, 7.0], [204.0, 7.0], [205.0, 7.0], [206.0, 7.0], [207.0, 7.0], [208.0, 7.0], [209.0, 7.0], [210.0, 7.0], [211.0, 7.0], [212.0, 7.0], [213.0, 7.0], [214.0, 7.0], [215.0, 7.0], [216.0, 7.0], [217.0, 7.0], [218.0, 7.0], [219.0, 7.0], [220.0, 6.0], [221.0, 6.0], [222.0, 6.0], [223.0, 6.0], [224.0, 6.0], [225.0, 6.0], [226.0, 6.0], [227.0, 6.0], [228.0, 6.0], [229.0, 6.0], [230.0, 6.0], [231.0, 6.0], [232.0, 6.0], [233.0, 6.0], [234.0, 6.0], [235.0, 6.0], [236.0, 6.0], [237.0, 6.0], [238.0, 6.0], [239.0, 6.0], [240.0, 6.0], [241.0, 6.0], [242.0, 6.0], [243.0, 6.0], [244.0, 6.0], [245.0, 6.0], [246.0, 6.0], [247.0, 6.0], [248.0, 6.0], [249.0, 6.0], [250.0, 6.0], [251.0, 6.0], [252.0, 6.0], [253.0, 6.0], [254.0, 6.0], [255.0, 6.0], [256.0, 6.0], [257.0, 6.0], [258.0, 5.0], [259.0, 5.0], [260.0, 5.0], [261.0, 5.0], [262.0, 5.0], [263.0, 5.0], [264.0, 5.0], [265.0, 5.0], [266.0, 5.0], [267.0, 5.0], [268.0, 5.0], [269.0, 5.0], [270.0, 5.0], [271.0, 5.0], [272.0, 5.0], [273.0, 5.0], [274.0, 5.0], [275.0, 5.0], [276.0, 5.0], [277.0, 5.0], [278.0, 5.0], [279.0, 5.0], [280.0, 5.0], [281.0, 5.0], [282.0, 5.0], [283.0, 5.0], [284.0, 5.0], [285.0, 5.0], [286.0, 5.0], [287.0, 5.0], [288.0, 5.0], [289.0, 5.0], [290.0, 5.0], [291.0, 5.0], [292.0, 5.0], [293.0, 5.0], [294.0, 5.0], [295.0, 5.0], [296.0, 5.0], [297.0, 5.0], [298.0, 5.0], [299.0, 5.0], [300.0, 5.0], [301.0, 5.0], [302.0, 5.0], [303.0, 5.0], [304.0, 5.0], [305.0, 5.0], [306.0, 5.0], [307.0, 5.0], [308.0, 5.0], [309.0, 5.0], [310.0, 5.0], [311.0, 5.0], [312.0, 5.0], [313.0, 5.0], [314.0, 5.0], [315.0, 5.0], [316.0, 5.0], [317.0, 5.0], [318.0, 4.0], [319.0, 4.0], [320.0, 4.0], [321.0, 4.0], [322.0, 4.0], [323.0, 4.0], [324.0, 4.0], [325.0, 4.0], [326.0, 4.0], [327.0, 4.0], [328.0, 4.0], [329.0, 4.0], [330.0, 4.0], [331.0, 4.0], [332.0, 4.0], [333.0, 4.0], [334.0, 4.0], [335.0, 4.0], [336.0, 4.0], [337.0, 4.0], [338.0, 4.0], [339.0, 4.0], [340.0, 4.0], [341.0, 4.0], [342.0, 4.0], [343.0, 4.0], [344.0, 4.0], [345.0, 4.0], [346.0, 4.0], [347.0, 4.0], [348.0, 4.0], [349.0, 4.0], [350.0, 4.0], [351.0, 4.0], [352.0, 4.0], [353.0, 4.0], [354.0, 4.0], [355.0, 4.0], [356.0, 4.0], [357.0, 4.0], [358.0, 4.0], [359.0, 4.0], [360.0, 4.0], [361.0, 4.0], [362.0, 4.0], [363.0, 4.0], [364.0, 4.0], [365.0, 4.0], [366.0, 4.0], [367.0, 4.0], [368.0, 4.0], [369.0, 4.0], [370.0, 4.0], [371.0, 4.0], [372.0, 4.0], [373.0, 4.0], [374.0, 4.0], [375.0, 4.0], [376.0, 4.0], [377.0, 4.0], [378.0, 4.0], [379.0, 4.0], [380.0, 4.0], [381.0, 4.0], [382.0, 4.0], [383.0, 4.0], [384.0, 4.0], [385.0, 4.0], [386.0, 4.0], [387.0, 4.0], [388.0, 4.0], [389.0, 4.0], [390.0, 4.0], [391.0, 4.0], [392.0, 4.0], [393.0, 4.0], [394.0, 4.0], [395.0, 4.0], [396.0, 4.0], [397.0, 4.0], [398.0, 4.0], [399.0, 3.0], [400.0, 3.0], [401.0, 3.0], [402.0, 3.0], [403.0, 3.0], [404.0, 3.0], [405.0, 3.0], [406.0, 3.0], [407.0, 3.0], [408.0, 3.0], [409.0, 3.0], [410.0, 3.0], [411.0, 3.0], [412.0, 3.0], [413.0, 3.0], [414.0, 3.0], [415.0, 3.0], [416.0, 3.0], [417.0, 3.0], [418.0, 3.0], [419.0, 3.0], [420.0, 3.0], [421.0, 3.0], [422.0, 3.0], [423.0, 3.0], [424.0, 3.0], [425.0, 3.0], [426.0, 3.0], [427.0, 3.0], [428.0, 3.0], [429.0, 3.0], [430.0, 3.0], [431.0, 3.0], [432.0, 3.0], [433.0, 3.0], [434.0, 3.0], [435.0, 3.0], [436.0, 3.0], [437.0, 3.0], [438.0, 3.0], [439.0, 3.0], [440.0, 3.0], [441.0, 3.0], [442.0, 3.0], [443.0, 3.0], [444.0, 3.0], [445.0, 3.0], [446.0, 3.0], [447.0, 3.0], [448.0, 3.0], [449.0, 3.0], [450.0, 3.0], [451.0, 3.0], [452.0, 3.0], [453.0, 3.0], [454.0, 3.0], [455.0, 3.0], [456.0, 3.0], [457.0, 3.0], [458.0, 3.0], [459.0, 3.0], [460.0, 3.0], [461.0, 3.0], [462.0, 3.0], [463.0, 3.0], [464.0, 3.0], [465.0, 3.0], [466.0, 3.0], [467.0, 3.0], [468.0, 3.0], [469.0, 3.0], [470.0, 3.0], [471.0, 3.0], [472.0, 3.0], [473.0, 3.0], [474.0, 3.0], [475.0, 3.0], [476.0, 3.0], [477.0, 3.0], [478.0, 3.0], [479.0, 3.0], [480.0, 3.0], [481.0, 3.0], [482.0, 3.0], [483.0, 3.0], [484.0, 3.0], [485.0, 3.0], [486.0, 3.0], [487.0, 3.0], [488.0, 3.0], [489.0, 3.0], [490.0, 3.0], [491.0, 3.0], [492.0, 3.0], [493.0, 3.0], [494.0, 3.0], [495.0, 3.0], [496.0, 3.0], [497.0, 3.0], [498.0, 3.0], [499.0, 3.0], [500.0, 3.0], [501.0, 3.0], [502.0, 3.0], [503.0, 3.0], [504.0, 3.0], [505.0, 3.0], [506.0, 3.0], [507.0, 3.0], [508.0, 3.0], [509.0, 3.0], [510.0, 3.0], [511.0, 3.0], [512.0, 3.0], [513.0, 3.0], [514.0, 3.0], [515.0, 3.0], [516.0, 3.0], [517.0, 3.0], [518.0, 3.0], [519.0, 3.0], [520.0, 3.0], [521.0, 3.0], [522.0, 3.0], [523.0, 3.0], [524.0, 3.0], [525.0, 3.0], [526.0, 3.0], [527.0, 3.0], [528.0, 3.0], [529.0, 3.0], [530.0, 3.0], [531.0, 3.0], [532.0, 3.0], [533.0, 3.0], [534.0, 3.0], [535.0, 3.0], [536.0, 3.0], [537.0, 3.0], [538.0, 3.0], [539.0, 3.0], [540.0, 3.0], [541.0, 3.0], [542.0, 3.0], [543.0, 3.0], [544.0, 3.0], [545.0, 3.0], [546.0, 3.0], [547.0, 3.0], [548.0, 3.0], [549.0, 3.0], [550.0, 3.0], [551.0, 3.0], [552.0, 3.0], [553.0, 3.0], [554.0, 3.0], [555.0, 3.0], [556.0, 3.0], [557.0, 3.0], [558.0, 3.0], [559.0, 3.0], [560.0, 3.0], [561.0, 3.0], [562.0, 3.0], [563.0, 3.0], [564.0, 3.0], [565.0, 3.0], [566.0, 2.0], [567.0, 2.0], [568.0, 2.0], [569.0, 2.0], [570.0, 2.0], [571.0, 2.0], [572.0, 2.0], [573.0, 2.0], [574.0, 2.0], [575.0, 2.0], [576.0, 2.0], [577.0, 2.0], [578.0, 2.0], [579.0, 2.0], [580.0, 2.0], [581.0, 2.0], [582.0, 2.0], [583.0, 2.0], [584.0, 2.0], [585.0, 2.0], [586.0, 2.0], [587.0, 2.0], [588.0, 2.0], [589.0, 2.0], [590.0, 2.0], [591.0, 2.0], [592.0, 2.0], [593.0, 2.0], [594.0, 2.0], [595.0, 2.0], [596.0, 2.0], [597.0, 2.0], [598.0, 2.0], [599.0, 2.0], [600.0, 2.0], [601.0, 2.0], [602.0, 2.0], [603.0, 2.0], [604.0, 2.0], [605.0, 2.0], [606.0, 2.0], [607.0, 2.0], [608.0, 2.0], [609.0, 2.0], [610.0, 2.0], [611.0, 2.0], [612.0, 2.0], [613.0, 2.0], [614.0, 2.0], [615.0, 2.0], [616.0, 2.0], [617.0, 2.0], [618.0, 2.0], [619.0, 2.0], [620.0, 2.0], [621.0, 2.0], [622.0, 2.0], [623.0, 2.0], [624.0, 2.0], [625.0, 2.0], [626.0, 2.0], [627.0, 2.0], [628.0, 2.0], [629.0, 2.0], [630.0, 2.0], [631.0, 2.0], [632.0, 2.0], [633.0, 2.0], [634.0, 2.0], [635.0, 2.0], [636.0, 2.0], [637.0, 2.0], [638.0, 2.0], [639.0, 2.0], [640.0, 2.0], [641.0, 2.0], [642.0, 2.0], [643.0, 2.0], [644.0, 2.0], [645.0, 2.0], [646.0, 2.0], [647.0, 2.0], [648.0, 2.0], [649.0, 2.0], [650.0, 2.0], [651.0, 2.0], [652.0, 2.0], [653.0, 2.0], [654.0, 2.0], [655.0, 2.0], [656.0, 2.0], [657.0, 2.0], [658.0, 2.0], [659.0, 2.0], [660.0, 2.0], [661.0, 2.0], [662.0, 2.0], [663.0, 2.0], [664.0, 2.0], [665.0, 2.0], [666.0, 2.0], [667.0, 2.0], [668.0, 2.0], [669.0, 2.0], [670.0, 2.0], [671.0, 2.0], [672.0, 2.0], [673.0, 2.0], [674.0, 2.0], [675.0, 2.0], [676.0, 2.0], [677.0, 2.0], [678.0, 2.0], [679.0, 2.0], [680.0, 2.0], [681.0, 2.0], [682.0, 2.0], [683.0, 2.0], [684.0, 2.0], [685.0, 2.0], [686.0, 2.0], [687.0, 2.0], [688.0, 2.0], [689.0, 2.0], [690.0, 2.0], [691.0, 2.0], [692.0, 2.0], [693.0, 2.0], [694.0, 2.0], [695.0, 2.0], [696.0, 2.0], [697.0, 2.0], [698.0, 2.0], [699.0, 2.0], [700.0, 2.0], [701.0, 2.0], [702.0, 2.0], [703.0, 2.0], [704.0, 2.0], [705.0, 2.0], [706.0, 2.0], [707.0, 2.0], [708.0, 2.0], [709.0, 2.0], [710.0, 2.0], [711.0, 2.0], [712.0, 2.0], [713.0, 2.0], [714.0, 2.0], [715.0, 2.0], [716.0, 2.0], [717.0, 2.0], [718.0, 2.0], [719.0, 2.0], [720.0, 2.0], [721.0, 2.0], [722.0, 2.0], [723.0, 2.0], [724.0, 2.0], [725.0, 2.0], [726.0, 2.0], [727.0, 2.0], [728.0, 2.0], [729.0, 2.0], [730.0, 2.0], [731.0, 2.0], [732.0, 2.0], [733.0, 2.0], [734.0, 2.0], [735.0, 2.0], [736.0, 2.0], [737.0, 2.0], [738.0, 2.0], [739.0, 2.0], [740.0, 2.0], [741.0, 2.0], [742.0, 2.0], [743.0, 2.0], [744.0, 2.0], [745.0, 2.0], [746.0, 2.0], [747.0, 2.0], [748.0, 2.0], [749.0, 2.0], [750.0, 2.0], [751.0, 2.0], [752.0, 2.0], [753.0, 2.0], [754.0, 2.0], [755.0, 2.0], [756.0, 2.0], [757.0, 2.0], [758.0, 2.0], [759.0, 2.0], [760.0, 2.0], [761.0, 2.0], [762.0, 2.0], [763.0, 2.0], [764.0, 2.0], [765.0, 2.0], [766.0, 2.0], [767.0, 2.0], [768.0, 2.0], [769.0, 2.0], [770.0, 2.0], [771.0, 2.0], [772.0, 2.0], [773.0, 2.0], [774.0, 2.0], [775.0, 2.0], [776.0, 2.0], [777.0, 2.0], [778.0, 2.0], [779.0, 2.0], [780.0, 2.0], [781.0, 2.0], [782.0, 2.0], [783.0, 2.0], [784.0, 2.0], [785.0, 2.0], [786.0, 2.0], [787.0, 2.0], [788.0, 2.0], [789.0, 2.0], [790.0, 2.0], [791.0, 2.0], [792.0, 2.0], [793.0, 2.0], [794.0, 2.0], [795.0, 2.0], [796.0, 2.0], [797.0, 2.0], [798.0, 2.0], [799.0, 2.0], [800.0, 2.0], [801.0, 2.0], [802.0, 2.0], [803.0, 2.0], [804.0, 2.0], [805.0, 2.0], [806.0, 2.0], [807.0, 2.0], [808.0, 2.0], [809.0, 2.0], [810.0, 2.0], [811.0, 2.0], [812.0, 2.0], [813.0, 2.0], [814.0, 2.0], [815.0, 2.0], [816.0, 2.0], [817.0, 2.0], [818.0, 2.0], [819.0, 2.0], [820.0, 2.0], [821.0, 2.0], [822.0, 2.0], [823.0, 2.0], [824.0, 2.0], [825.0, 2.0], [826.0, 2.0], [827.0, 2.0], [828.0, 2.0], [829.0, 2.0], [830.0, 2.0], [831.0, 2.0], [832.0, 2.0], [833.0, 2.0], [834.0, 2.0], [835.0, 2.0], [836.0, 2.0], [837.0, 2.0], [838.0, 2.0], [839.0, 2.0], [840.0, 2.0], [841.0, 2.0], [842.0, 2.0], [843.0, 2.0], [844.0, 2.0], [845.0, 2.0], [846.0, 2.0], [847.0, 2.0], [848.0, 2.0], [849.0, 2.0], [850.0, 2.0], [851.0, 2.0], [852.0, 2.0], [853.0, 2.0], [854.0, 2.0], [855.0, 2.0], [856.0, 2.0], [857.0, 2.0], [858.0, 2.0], [859.0, 2.0], [860.0, 2.0], [861.0, 2.0], [862.0, 2.0], [863.0, 2.0], [864.0, 2.0], [865.0, 2.0], [866.0, 2.0], [867.0, 2.0], [868.0, 2.0], [869.0, 2.0], [870.0, 2.0], [871.0, 2.0], [872.0, 2.0], [873.0, 2.0], [874.0, 2.0], [875.0, 2.0], [876.0, 2.0], [877.0, 2.0], [878.0, 2.0], [879.0, 2.0], [880.0, 2.0], [881.0, 2.0], [882.0, 2.0], [883.0, 2.0], [884.0, 2.0], [885.0, 2.0], [886.0, 2.0], [887.0, 2.0], [888.0, 2.0], [889.0, 2.0], [890.0, 2.0], [891.0, 2.0], [892.0, 2.0], [893.0, 2.0], [894.0, 2.0], [895.0, 2.0], [896.0, 2.0], [897.0, 2.0], [898.0, 2.0], [899.0, 2.0], [900.0, 2.0], [901.0, 2.0], [902.0, 2.0], [903.0, 2.0], [904.0, 2.0], [905.0, 2.0], [906.0, 2.0], [907.0, 2.0], [908.0, 2.0], [909.0, 2.0], [910.0, 2.0], [911.0, 2.0], [912.0, 2.0], [913.0, 2.0], [914.0, 2.0], [915.0, 2.0], [916.0, 2.0], [917.0, 2.0], [918.0, 2.0], [919.0, 2.0], [920.0, 2.0], [921.0, 2.0], [922.0, 2.0], [923.0, 2.0], [924.0, 2.0], [925.0, 2.0], [926.0, 2.0], [927.0, 2.0], [928.0, 1.0], [929.0, 1.0], [930.0, 1.0], [931.0, 1.0], [932.0, 1.0], [933.0, 1.0], [934.0, 1.0], [935.0, 1.0], [936.0, 1.0], [937.0, 1.0], [938.0, 1.0], [939.0, 1.0], [940.0, 1.0], [941.0, 1.0], [942.0, 1.0], [943.0, 1.0], [944.0, 1.0], [945.0, 1.0], [946.0, 1.0], [947.0, 1.0], [948.0, 1.0], [949.0, 1.0], [950.0, 1.0], [951.0, 1.0], [952.0, 1.0], [953.0, 1.0], [954.0, 1.0], [955.0, 1.0], [956.0, 1.0], [957.0, 1.0], [958.0, 1.0], [959.0, 1.0], [960.0, 1.0], [961.0, 1.0], [962.0, 1.0], [963.0, 1.0], [964.0, 1.0], [965.0, 1.0], [966.0, 1.0], [967.0, 1.0], [968.0, 1.0], [969.0, 1.0], [970.0, 1.0], [971.0, 1.0], [972.0, 1.0], [973.0, 1.0], [974.0, 1.0], [975.0, 1.0], [976.0, 1.0], [977.0, 1.0], [978.0, 1.0], [979.0, 1.0], [980.0, 1.0], [981.0, 1.0], [982.0, 1.0], [983.0, 1.0], [984.0, 1.0], [985.0, 1.0], [986.0, 1.0], [987.0, 1.0], [988.0, 1.0], [989.0, 1.0], [990.0, 1.0], [991.0, 1.0], [992.0, 1.0], [993.0, 1.0], [994.0, 1.0], [995.0, 1.0], [996.0, 1.0], [997.0, 1.0], [998.0, 1.0], [999.0, 1.0], [1000.0, 1.0], [1001.0, 1.0], [1002.0, 1.0], [1003.0, 1.0], [1004.0, 1.0], [1005.0, 1.0], [1006.0, 1.0], [1007.0, 1.0], [1008.0, 1.0], [1009.0, 1.0], [1010.0, 1.0], [1011.0, 1.0], [1012.0, 1.0], [1013.0, 1.0], [1014.0, 1.0], [1015.0, 1.0], [1016.0, 1.0], [1017.0, 1.0], [1018.0, 1.0], [1019.0, 1.0], [1020.0, 1.0], [1021.0, 1.0], [1022.0, 1.0], [1023.0, 1.0], [1024.0, 1.0], [1025.0, 1.0], [1026.0, 1.0], [1027.0, 1.0], [1028.0, 1.0], [1029.0, 1.0], [1030.0, 1.0], [1031.0, 1.0], [1032.0, 1.0], [1033.0, 1.0], [1034.0, 1.0], [1035.0, 1.0], [1036.0, 1.0], [1037.0, 1.0], [1038.0, 1.0], [1039.0, 1.0], [1040.0, 1.0], [1041.0, 1.0], [1042.0, 1.0], [1043.0, 1.0], [1044.0, 1.0], [1045.0, 1.0], [1046.0, 1.0], [1047.0, 1.0], [1048.0, 1.0], [1049.0, 1.0], [1050.0, 1.0], [1051.0, 1.0], [1052.0, 1.0], [1053.0, 1.0], [1054.0, 1.0], [1055.0, 1.0], [1056.0, 1.0], [1057.0, 1.0], [1058.0, 1.0], [1059.0, 1.0], [1060.0, 1.0], [1061.0, 1.0], [1062.0, 1.0], [1063.0, 1.0], [1064.0, 1.0], [1065.0, 1.0], [1066.0, 1.0], [1067.0, 1.0], [1068.0, 1.0], [1069.0, 1.0], [1070.0, 1.0], [1071.0, 1.0], [1072.0, 1.0], [1073.0, 1.0], [1074.0, 1.0], [1075.0, 1.0], [1076.0, 1.0], [1077.0, 1.0], [1078.0, 1.0], [1079.0, 1.0], [1080.0, 1.0], [1081.0, 1.0], [1082.0, 1.0], [1083.0, 1.0], [1084.0, 1.0], [1085.0, 1.0], [1086.0, 1.0], [1087.0, 1.0], [1088.0, 1.0], [1089.0, 1.0], [1090.0, 1.0], [1091.0, 1.0], [1092.0, 1.0], [1093.0, 1.0], [1094.0, 1.0], [1095.0, 1.0], [1096.0, 1.0], [1097.0, 1.0], [1098.0, 1.0], [1099.0, 1.0], [1100.0, 1.0], [1101.0, 1.0], [1102.0, 1.0], [1103.0, 1.0], [1104.0, 1.0], [1105.0, 1.0], [1106.0, 1.0], [1107.0, 1.0], [1108.0, 1.0], [1109.0, 1.0], [1110.0, 1.0], [1111.0, 1.0], [1112.0, 1.0], [1113.0, 1.0], [1114.0, 1.0], [1115.0, 1.0], [1116.0, 1.0], [1117.0, 1.0], [1118.0, 1.0], [1119.0, 1.0], [1120.0, 1.0], [1121.0, 1.0], [1122.0, 1.0], [1123.0, 1.0], [1124.0, 1.0], [1125.0, 1.0], [1126.0, 1.0], [1127.0, 1.0], [1128.0, 1.0], [1129.0, 1.0], [1130.0, 1.0], [1131.0, 1.0], [1132.0, 1.0], [1133.0, 1.0], [1134.0, 1.0], [1135.0, 1.0], [1136.0, 1.0], [1137.0, 1.0], [1138.0, 1.0], [1139.0, 1.0], [1140.0, 1.0], [1141.0, 1.0], [1142.0, 1.0], [1143.0, 1.0], [1144.0, 1.0], [1145.0, 1.0], [1146.0, 1.0], [1147.0, 1.0], [1148.0, 1.0], [1149.0, 1.0], [1150.0, 1.0], [1151.0, 1.0], [1152.0, 1.0], [1153.0, 1.0], [1154.0, 1.0], [1155.0, 1.0], [1156.0, 1.0], [1157.0, 1.0], [1158.0, 1.0], [1159.0, 1.0], [1160.0, 1.0], [1161.0, 1.0], [1162.0, 1.0], [1163.0, 1.0], [1164.0, 1.0], [1165.0, 1.0], [1166.0, 1.0], [1167.0, 1.0], [1168.0, 1.0], [1169.0, 1.0], [1170.0, 1.0], [1171.0, 1.0], [1172.0, 1.0], [1173.0, 1.0], [1174.0, 1.0], [1175.0, 1.0], [1176.0, 1.0], [1177.0, 1.0], [1178.0, 1.0], [1179.0, 1.0], [1180.0, 1.0], [1181.0, 1.0], [1182.0, 1.0], [1183.0, 1.0], [1184.0, 1.0], [1185.0, 1.0], [1186.0, 1.0], [1187.0, 1.0], [1188.0, 1.0], [1189.0, 1.0], [1190.0, 1.0], [1191.0, 1.0], [1192.0, 1.0], [1193.0, 1.0], [1194.0, 1.0], [1195.0, 1.0], [1196.0, 1.0], [1197.0, 1.0], [1198.0, 1.0], [1199.0, 1.0], [1200.0, 1.0], [1201.0, 1.0], [1202.0, 1.0], [1203.0, 1.0], [1204.0, 1.0], [1205.0, 1.0], [1206.0, 1.0], [1207.0, 1.0], [1208.0, 1.0], [1209.0, 1.0], [1210.0, 1.0], [1211.0, 1.0], [1212.0, 1.0], [1213.0, 1.0], [1214.0, 1.0], [1215.0, 1.0], [1216.0, 1.0], [1217.0, 1.0], [1218.0, 1.0], [1219.0, 1.0], [1220.0, 1.0], [1221.0, 1.0], [1222.0, 1.0], [1223.0, 1.0], [1224.0, 1.0], [1225.0, 1.0], [1226.0, 1.0], [1227.0, 1.0], [1228.0, 1.0], [1229.0, 1.0], [1230.0, 1.0], [1231.0, 1.0], [1232.0, 1.0], [1233.0, 1.0], [1234.0, 1.0], [1235.0, 1.0], [1236.0, 1.0], [1237.0, 1.0], [1238.0, 1.0], [1239.0, 1.0], [1240.0, 1.0], [1241.0, 1.0], [1242.0, 1.0], [1243.0, 1.0], [1244.0, 1.0], [1245.0, 1.0], [1246.0, 1.0], [1247.0, 1.0], [1248.0, 1.0], [1249.0, 1.0], [1250.0, 1.0], [1251.0, 1.0], [1252.0, 1.0], [1253.0, 1.0], [1254.0, 1.0], [1255.0, 1.0], [1256.0, 1.0], [1257.0, 1.0], [1258.0, 1.0], [1259.0, 1.0], [1260.0, 1.0], [1261.0, 1.0], [1262.0, 1.0], [1263.0, 1.0], [1264.0, 1.0], [1265.0, 1.0], [1266.0, 1.0], [1267.0, 1.0], [1268.0, 1.0], [1269.0, 1.0], [1270.0, 1.0], [1271.0, 1.0], [1272.0, 1.0], [1273.0, 1.0], [1274.0, 1.0], [1275.0, 1.0], [1276.0, 1.0], [1277.0, 1.0], [1278.0, 1.0], [1279.0, 1.0], [1280.0, 1.0], [1281.0, 1.0], [1282.0, 1.0], [1283.0, 1.0], [1284.0, 1.0], [1285.0, 1.0], [1286.0, 1.0], [1287.0, 1.0], [1288.0, 1.0], [1289.0, 1.0], [1290.0, 1.0], [1291.0, 1.0], [1292.0, 1.0], [1293.0, 1.0], [1294.0, 1.0], [1295.0, 1.0], [1296.0, 1.0], [1297.0, 1.0], [1298.0, 1.0], [1299.0, 1.0], [1300.0, 1.0], [1301.0, 1.0], [1302.0, 1.0], [1303.0, 1.0], [1304.0, 1.0], [1305.0, 1.0], [1306.0, 1.0], [1307.0, 1.0], [1308.0, 1.0], [1309.0, 1.0], [1310.0, 1.0], [1311.0, 1.0], [1312.0, 1.0], [1313.0, 1.0], [1314.0, 1.0], [1315.0, 1.0], [1316.0, 1.0], [1317.0, 1.0], [1318.0, 1.0], [1319.0, 1.0], [1320.0, 1.0], [1321.0, 1.0], [1322.0, 1.0], [1323.0, 1.0], [1324.0, 1.0], [1325.0, 1.0], [1326.0, 1.0], [1327.0, 1.0], [1328.0, 1.0], [1329.0, 1.0], [1330.0, 1.0], [1331.0, 1.0], [1332.0, 1.0], [1333.0, 1.0], [1334.0, 1.0], [1335.0, 1.0], [1336.0, 1.0], [1337.0, 1.0], [1338.0, 1.0], [1339.0, 1.0], [1340.0, 1.0], [1341.0, 1.0], [1342.0, 1.0], [1343.0, 1.0], [1344.0, 1.0], [1345.0, 1.0], [1346.0, 1.0], [1347.0, 1.0], [1348.0, 1.0], [1349.0, 1.0], [1350.0, 1.0], [1351.0, 1.0], [1352.0, 1.0], [1353.0, 1.0], [1354.0, 1.0], [1355.0, 1.0], [1356.0, 1.0], [1357.0, 1.0], [1358.0, 1.0], [1359.0, 1.0], [1360.0, 1.0], [1361.0, 1.0], [1362.0, 1.0], [1363.0, 1.0], [1364.0, 1.0], [1365.0, 1.0], [1366.0, 1.0], [1367.0, 1.0], [1368.0, 1.0], [1369.0, 1.0], [1370.0, 1.0], [1371.0, 1.0], [1372.0, 1.0], [1373.0, 1.0], [1374.0, 1.0], [1375.0, 1.0], [1376.0, 1.0], [1377.0, 1.0], [1378.0, 1.0], [1379.0, 1.0], [1380.0, 1.0], [1381.0, 1.0], [1382.0, 1.0], [1383.0, 1.0], [1384.0, 1.0], [1385.0, 1.0], [1386.0, 1.0], [1387.0, 1.0], [1388.0, 1.0], [1389.0, 1.0], [1390.0, 1.0], [1391.0, 1.0], [1392.0, 1.0], [1393.0, 1.0], [1394.0, 1.0], [1395.0, 1.0], [1396.0, 1.0], [1397.0, 1.0], [1398.0, 1.0], [1399.0, 1.0], [1400.0, 1.0], [1401.0, 1.0], [1402.0, 1.0], [1403.0, 1.0], [1404.0, 1.0], [1405.0, 1.0], [1406.0, 1.0], [1407.0, 1.0], [1408.0, 1.0], [1409.0, 1.0], [1410.0, 1.0], [1411.0, 1.0], [1412.0, 1.0], [1413.0, 1.0], [1414.0, 1.0], [1415.0, 1.0], [1416.0, 1.0], [1417.0, 1.0], [1418.0, 1.0], [1419.0, 1.0], [1420.0, 1.0], [1421.0, 1.0], [1422.0, 1.0], [1423.0, 1.0], [1424.0, 1.0], [1425.0, 1.0], [1426.0, 1.0], [1427.0, 1.0], [1428.0, 1.0], [1429.0, 1.0], [1430.0, 1.0], [1431.0, 1.0], [1432.0, 1.0], [1433.0, 1.0], [1434.0, 1.0], [1435.0, 1.0], [1436.0, 1.0], [1437.0, 1.0], [1438.0, 1.0], [1439.0, 1.0], [1440.0, 1.0], [1441.0, 1.0], [1442.0, 1.0], [1443.0, 1.0], [1444.0, 1.0], [1445.0, 1.0], [1446.0, 1.0], [1447.0, 1.0], [1448.0, 1.0], [1449.0, 1.0], [1450.0, 1.0], [1451.0, 1.0], [1452.0, 1.0], [1453.0, 1.0], [1454.0, 1.0], [1455.0, 1.0], [1456.0, 1.0], [1457.0, 1.0], [1458.0, 1.0], [1459.0, 1.0], [1460.0, 1.0], [1461.0, 1.0], [1462.0, 1.0], [1463.0, 1.0], [1464.0, 1.0], [1465.0, 1.0], [1466.0, 1.0], [1467.0, 1.0], [1468.0, 1.0], [1469.0, 1.0], [1470.0, 1.0], [1471.0, 1.0], [1472.0, 1.0], [1473.0, 1.0], [1474.0, 1.0], [1475.0, 1.0], [1476.0, 1.0], [1477.0, 1.0], [1478.0, 1.0], [1479.0, 1.0], [1480.0, 1.0], [1481.0, 1.0], [1482.0, 1.0], [1483.0, 1.0], [1484.0, 1.0], [1485.0, 1.0], [1486.0, 1.0], [1487.0, 1.0], [1488.0, 1.0], [1489.0, 1.0], [1490.0, 1.0], [1491.0, 1.0], [1492.0, 1.0], [1493.0, 1.0], [1494.0, 1.0], [1495.0, 1.0], [1496.0, 1.0], [1497.0, 1.0], [1498.0, 1.0], [1499.0, 1.0], [1500.0, 1.0], [1501.0, 1.0], [1502.0, 1.0], [1503.0, 1.0], [1504.0, 1.0], [1505.0, 1.0], [1506.0, 1.0], [1507.0, 1.0], [1508.0, 1.0], [1509.0, 1.0], [1510.0, 1.0], [1511.0, 1.0], [1512.0, 1.0], [1513.0, 1.0], [1514.0, 1.0], [1515.0, 1.0], [1516.0, 1.0], [1517.0, 1.0], [1518.0, 1.0], [1519.0, 1.0], [1520.0, 1.0], [1521.0, 1.0], [1522.0, 1.0], [1523.0, 1.0], [1524.0, 1.0], [1525.0, 1.0], [1526.0, 1.0], [1527.0, 1.0], [1528.0, 1.0], [1529.0, 1.0], [1530.0, 1.0], [1531.0, 1.0], [1532.0, 1.0], [1533.0, 1.0], [1534.0, 1.0], [1535.0, 1.0], [1536.0, 1.0], [1537.0, 1.0], [1538.0, 1.0], [1539.0, 1.0], [1540.0, 1.0], [1541.0, 1.0], [1542.0, 1.0], [1543.0, 1.0], [1544.0, 1.0], [1545.0, 1.0], [1546.0, 1.0], [1547.0, 1.0], [1548.0, 1.0], [1549.0, 1.0], [1550.0, 1.0], [1551.0, 1.0], [1552.0, 1.0], [1553.0, 1.0], [1554.0, 1.0], [1555.0, 1.0], [1556.0, 1.0], [1557.0, 1.0], [1558.0, 1.0], [1559.0, 1.0], [1560.0, 1.0], [1561.0, 1.0], [1562.0, 1.0], [1563.0, 1.0], [1564.0, 1.0], [1565.0, 1.0], [1566.0, 1.0], [1567.0, 1.0], [1568.0, 1.0], [1569.0, 1.0], [1570.0, 1.0], [1571.0, 1.0], [1572.0, 1.0], [1573.0, 1.0], [1574.0, 1.0], [1575.0, 1.0], [1576.0, 1.0], [1577.0, 1.0], [1578.0, 1.0], [1579.0, 1.0], [1580.0, 1.0], [1581.0, 1.0], [1582.0, 1.0], [1583.0, 1.0], [1584.0, 1.0], [1585.0, 1.0], [1586.0, 1.0], [1587.0, 1.0], [1588.0, 1.0], [1589.0, 1.0], [1590.0, 1.0], [1591.0, 1.0], [1592.0, 1.0], [1593.0, 1.0], [1594.0, 1.0], [1595.0, 1.0], [1596.0, 1.0], [1597.0, 1.0], [1598.0, 1.0], [1599.0, 1.0], [1600.0, 1.0], [1601.0, 1.0], [1602.0, 1.0], [1603.0, 1.0], [1604.0, 1.0], [1605.0, 1.0], [1606.0, 1.0], [1607.0, 1.0], [1608.0, 1.0], [1609.0, 1.0], [1610.0, 1.0], [1611.0, 1.0], [1612.0, 1.0], [1613.0, 1.0], [1614.0, 1.0], [1615.0, 1.0], [1616.0, 1.0], [1617.0, 1.0], [1618.0, 1.0], [1619.0, 1.0], [1620.0, 1.0], [1621.0, 1.0], [1622.0, 1.0], [1623.0, 1.0], [1624.0, 1.0], [1625.0, 1.0], [1626.0, 1.0], [1627.0, 1.0], [1628.0, 1.0], [1629.0, 1.0], [1630.0, 1.0], [1631.0, 1.0], [1632.0, 1.0], [1633.0, 1.0], [1634.0, 1.0], [1635.0, 1.0], [1636.0, 1.0], [1637.0, 1.0], [1638.0, 1.0], [1639.0, 1.0], [1640.0, 1.0], [1641.0, 1.0], [1642.0, 1.0], [1643.0, 1.0], [1644.0, 1.0], [1645.0, 1.0], [1646.0, 1.0], [1647.0, 1.0], [1648.0, 1.0], [1649.0, 1.0], [1650.0, 1.0], [1651.0, 1.0], [1652.0, 1.0], [1653.0, 1.0], [1654.0, 1.0], [1655.0, 1.0], [1656.0, 1.0], [1657.0, 1.0], [1658.0, 1.0], [1659.0, 1.0], [1660.0, 1.0], [1661.0, 1.0], [1662.0, 1.0], [1663.0, 1.0], [1664.0, 1.0], [1665.0, 1.0], [1666.0, 1.0], [1667.0, 1.0], [1668.0, 1.0], [1669.0, 1.0], [1670.0, 1.0], [1671.0, 1.0], [1672.0, 1.0], [1673.0, 1.0], [1674.0, 1.0], [1675.0, 1.0], [1676.0, 1.0], [1677.0, 1.0], [1678.0, 1.0], [1679.0, 1.0], [1680.0, 1.0], [1681.0, 1.0], [1682.0, 1.0], [1683.0, 1.0], [1684.0, 1.0], [1685.0, 1.0], [1686.0, 1.0], [1687.0, 1.0], [1688.0, 1.0], [1689.0, 1.0], [1690.0, 1.0], [1691.0, 1.0], [1692.0, 1.0], [1693.0, 1.0], [1694.0, 1.0], [1695.0, 1.0], [1696.0, 1.0], [1697.0, 1.0], [1698.0, 1.0], [1699.0, 1.0], [1700.0, 1.0], [1701.0, 1.0], [1702.0, 1.0], [1703.0, 1.0], [1704.0, 1.0], [1705.0, 1.0], [1706.0, 1.0], [1707.0, 1.0], [1708.0, 1.0], [1709.0, 1.0], [1710.0, 1.0], [1711.0, 1.0], [1712.0, 1.0], [1713.0, 1.0], [1714.0, 1.0], [1715.0, 1.0], [1716.0, 1.0], [1717.0, 1.0], [1718.0, 1.0], [1719.0, 1.0], [1720.0, 1.0], [1721.0, 1.0], [1722.0, 1.0], [1723.0, 1.0], [1724.0, 1.0], [1725.0, 1.0], [1726.0, 1.0], [1727.0, 1.0], [1728.0, 1.0], [1729.0, 1.0], [1730.0, 1.0], [1731.0, 1.0], [1732.0, 1.0], [1733.0, 1.0], [1734.0, 1.0], [1735.0, 1.0], [1736.0, 1.0], [1737.0, 1.0], [1738.0, 1.0], [1739.0, 1.0], [1740.0, 1.0], [1741.0, 1.0], [1742.0, 1.0], [1743.0, 1.0], [1744.0, 1.0], [1745.0, 1.0], [1746.0, 1.0], [1747.0, 1.0], [1748.0, 1.0], [1749.0, 1.0], [1750.0, 1.0], [1751.0, 1.0], [1752.0, 1.0], [1753.0, 1.0], [1754.0, 1.0], [1755.0, 1.0], [1756.0, 1.0], [1757.0, 1.0], [1758.0, 1.0], [1759.0, 1.0], [1760.0, 1.0], [1761.0, 1.0], [1762.0, 1.0], [1763.0, 1.0], [1764.0, 1.0], [1765.0, 1.0], [1766.0, 1.0], [1767.0, 1.0], [1768.0, 1.0], [1769.0, 1.0], [1770.0, 1.0], [1771.0, 1.0], [1772.0, 1.0], [1773.0, 1.0], [1774.0, 1.0], [1775.0, 1.0], [1776.0, 1.0], [1777.0, 1.0], [1778.0, 1.0], [1779.0, 1.0], [1780.0, 1.0], [1781.0, 1.0], [1782.0, 1.0], [1783.0, 1.0], [1784.0, 1.0], [1785.0, 1.0], [1786.0, 1.0], [1787.0, 1.0], [1788.0, 1.0], [1789.0, 1.0], [1790.0, 1.0], [1791.0, 1.0], [1792.0, 1.0], [1793.0, 1.0], [1794.0, 1.0], [1795.0, 1.0], [1796.0, 1.0], [1797.0, 1.0], [1798.0, 1.0], [1799.0, 1.0], [1800.0, 1.0], [1801.0, 1.0], [1802.0, 1.0], [1803.0, 1.0], [1804.0, 1.0], [1805.0, 1.0], [1806.0, 1.0], [1807.0, 1.0], [1808.0, 1.0], [1809.0, 1.0], [1810.0, 1.0], [1811.0, 1.0], [1812.0, 1.0], [1813.0, 1.0], [1814.0, 1.0], [1815.0, 1.0], [1816.0, 1.0], [1817.0, 1.0], [1818.0, 1.0], [1819.0, 1.0], [1820.0, 1.0], [1821.0, 1.0], [1822.0, 1.0], [1823.0, 1.0], [1824.0, 1.0], [1825.0, 1.0], [1826.0, 1.0], [1827.0, 1.0], [1828.0, 1.0], [1829.0, 1.0], [1830.0, 1.0], [1831.0, 1.0], [1832.0, 1.0], [1833.0, 1.0], [1834.0, 1.0], [1835.0, 1.0], [1836.0, 1.0], [1837.0, 1.0], [1838.0, 1.0], [1839.0, 1.0], [1840.0, 1.0], [1841.0, 1.0], [1842.0, 1.0], [1843.0, 1.0], [1844.0, 1.0], [1845.0, 1.0], [1846.0, 1.0], [1847.0, 1.0], [1848.0, 1.0], [1849.0, 1.0], [1850.0, 1.0], [1851.0, 1.0], [1852.0, 1.0], [1853.0, 1.0], [1854.0, 1.0], [1855.0, 1.0], [1856.0, 1.0], [1857.0, 1.0], [1858.0, 1.0], [1859.0, 1.0], [1860.0, 1.0], [1861.0, 1.0], [1862.0, 1.0], [1863.0, 1.0], [1864.0, 1.0], [1865.0, 1.0], [1866.0, 1.0], [1867.0, 1.0], [1868.0, 1.0], [1869.0, 1.0], [1870.0, 1.0], [1871.0, 1.0], [1872.0, 1.0], [1873.0, 1.0], [1874.0, 1.0], [1875.0, 1.0], [1876.0, 1.0], [1877.0, 1.0], [1878.0, 1.0], [1879.0, 1.0], [1880.0, 1.0], [1881.0, 1.0], [1882.0, 1.0], [1883.0, 1.0], [1884.0, 1.0], [1885.0, 1.0], [1886.0, 1.0], [1887.0, 1.0], [1888.0, 1.0], [1889.0, 1.0], [1890.0, 1.0], [1891.0, 1.0], [1892.0, 1.0], [1893.0, 1.0], [1894.0, 1.0], [1895.0, 1.0], [1896.0, 1.0], [1897.0, 1.0], [1898.0, 1.0], [1899.0, 1.0], [1900.0, 1.0], [1901.0, 1.0], [1902.0, 1.0], [1903.0, 1.0], [1904.0, 1.0], [1905.0, 1.0], [1906.0, 1.0], [1907.0, 1.0], [1908.0, 1.0], [1909.0, 1.0], [1910.0, 1.0], [1911.0, 1.0], [1912.0, 1.0], [1913.0, 1.0], [1914.0, 1.0], [1915.0, 1.0], [1916.0, 1.0], [1917.0, 1.0], [1918.0, 1.0], [1919.0, 1.0], [1920.0, 1.0], [1921.0, 1.0], [1922.0, 1.0], [1923.0, 1.0], [1924.0, 1.0], [1925.0, 1.0], [1926.0, 1.0], [1927.0, 1.0], [1928.0, 1.0], [1929.0, 1.0], [1930.0, 1.0], [1931.0, 1.0], [1932.0, 1.0], [1933.0, 1.0], [1934.0, 1.0], [1935.0, 1.0], [1936.0, 1.0], [1937.0, 1.0], [1938.0, 1.0], [1939.0, 1.0], [1940.0, 1.0], [1941.0, 1.0], [1942.0, 1.0], [1943.0, 1.0], [1944.0, 1.0], [1945.0, 1.0], [1946.0, 1.0], [1947.0, 1.0], [1948.0, 1.0], [1949.0, 1.0], [1950.0, 1.0], [1951.0, 1.0], [1952.0, 1.0], [1953.0, 1.0], [1954.0, 1.0], [1955.0, 1.0], [1956.0, 1.0], [1957.0, 1.0], [1958.0, 1.0], [1959.0, 1.0], [1960.0, 1.0], [1961.0, 1.0], [1962.0, 1.0], [1963.0, 1.0], [1964.0, 1.0], [1965.0, 1.0], [1966.0, 1.0], [1967.0, 1.0], [1968.0, 1.0], [1969.0, 1.0], [1970.0, 1.0], [1971.0, 1.0], [1972.0, 1.0], [1973.0, 1.0], [1974.0, 1.0], [1975.0, 1.0], [1976.0, 1.0], [1977.0, 1.0], [1978.0, 1.0], [1979.0, 1.0], [1980.0, 1.0], [1981.0, 1.0], [1982.0, 1.0], [1983.0, 1.0], [1984.0, 1.0], [1985.0, 1.0], [1986.0, 1.0], [1987.0, 1.0], [1988.0, 1.0], [1989.0, 1.0], [1990.0, 1.0], [1991.0, 1.0], [1992.0, 1.0], [1993.0, 1.0], [1994.0, 1.0], [1995.0, 1.0], [1996.0, 1.0], [1997.0, 1.0], [1998.0, 1.0], [1999.0, 1.0], [2000.0, 1.0], [2001.0, 1.0], [2002.0, 1.0], [2003.0, 1.0], [2004.0, 1.0], [2005.0, 1.0], [2006.0, 1.0], [2007.0, 1.0], [2008.0, 1.0], [2009.0, 1.0], [2010.0, 1.0], [2011.0, 1.0], [2012.0, 1.0], [2013.0, 1.0], [2014.0, 1.0], [2015.0, 1.0], [2016.0, 1.0], [2017.0, 1.0], [2018.0, 1.0], [2019.0, 1.0], [2020.0, 1.0], [2021.0, 1.0], [2022.0, 1.0], [2023.0, 1.0], [2024.0, 1.0], [2025.0, 1.0], [2026.0, 1.0], [2027.0, 1.0], [2028.0, 1.0], [2029.0, 1.0], [2030.0, 1.0], [2031.0, 1.0], [2032.0, 1.0], [2033.0, 1.0], [2034.0, 1.0], [2035.0, 1.0], [2036.0, 1.0], [2037.0, 1.0], [2038.0, 1.0], [2039.0, 1.0], [2040.0, 1.0], [2041.0, 1.0], [2042.0, 1.0], [2043.0, 1.0], [2044.0, 1.0], [2045.0, 1.0], [2046.0, 1.0], [2047.0, 1.0], [2048.0, 1.0], [2049.0, 1.0], [2050.0, 1.0], [2051.0, 1.0], [2052.0, 1.0], [2053.0, 1.0], [2054.0, 1.0], [2055.0, 1.0], [2056.0, 1.0], [2057.0, 1.0], [2058.0, 1.0], [2059.0, 1.0], [2060.0, 1.0], [2061.0, 1.0], [2062.0, 1.0], [2063.0, 1.0], [2064.0, 1.0], [2065.0, 1.0], [2066.0, 1.0], [2067.0, 1.0], [2068.0, 1.0], [2069.0, 1.0], [2070.0, 1.0], [2071.0, 1.0], [2072.0, 1.0], [2073.0, 1.0], [2074.0, 1.0], [2075.0, 1.0], [2076.0, 1.0], [2077.0, 1.0], [2078.0, 1.0], [2079.0, 1.0], [2080.0, 1.0], [2081.0, 1.0], [2082.0, 1.0], [2083.0, 1.0], [2084.0, 1.0], [2085.0, 1.0], [2086.0, 1.0], [2087.0, 1.0], [2088.0, 1.0], [2089.0, 1.0], [2090.0, 1.0], [2091.0, 1.0], [2092.0, 1.0], [2093.0, 1.0], [2094.0, 1.0], [2095.0, 1.0], [2096.0, 1.0], [2097.0, 1.0], [2098.0, 1.0], [2099.0, 1.0], [2100.0, 1.0], [2101.0, 1.0], [2102.0, 1.0], [2103.0, 1.0], [2104.0, 1.0], [2105.0, 1.0], [2106.0, 1.0], [2107.0, 1.0], [2108.0, 1.0], [2109.0, 1.0], [2110.0, 1.0], [2111.0, 1.0], [2112.0, 1.0], [2113.0, 1.0], [2114.0, 1.0], [2115.0, 1.0], [2116.0, 1.0], [2117.0, 1.0], [2118.0, 1.0], [2119.0, 1.0], [2120.0, 1.0], [2121.0, 1.0], [2122.0, 1.0], [2123.0, 1.0], [2124.0, 1.0], [2125.0, 1.0], [2126.0, 1.0], [2127.0, 1.0], [2128.0, 1.0], [2129.0, 1.0], [2130.0, 1.0], [2131.0, 1.0], [2132.0, 1.0], [2133.0, 1.0], [2134.0, 1.0], [2135.0, 1.0], [2136.0, 1.0], [2137.0, 1.0], [2138.0, 1.0], [2139.0, 1.0], [2140.0, 1.0], [2141.0, 1.0], [2142.0, 1.0], [2143.0, 1.0], [2144.0, 1.0], [2145.0, 1.0], [2146.0, 1.0], [2147.0, 1.0], [2148.0, 1.0], [2149.0, 1.0], [2150.0, 1.0], [2151.0, 1.0], [2152.0, 1.0], [2153.0, 1.0], [2154.0, 1.0], [2155.0, 1.0], [2156.0, 1.0], [2157.0, 1.0], [2158.0, 1.0], [2159.0, 1.0], [2160.0, 1.0], [2161.0, 1.0], [2162.0, 1.0], [2163.0, 1.0], [2164.0, 1.0], [2165.0, 1.0], [2166.0, 1.0], [2167.0, 1.0], [2168.0, 1.0], [2169.0, 1.0], [2170.0, 1.0], [2171.0, 1.0], [2172.0, 1.0], [2173.0, 1.0], [2174.0, 1.0], [2175.0, 1.0], [2176.0, 1.0], [2177.0, 1.0], [2178.0, 1.0], [2179.0, 1.0], [2180.0, 1.0], [2181.0, 1.0], [2182.0, 1.0], [2183.0, 1.0], [2184.0, 1.0], [2185.0, 1.0], [2186.0, 1.0], [2187.0, 1.0], [2188.0, 1.0], [2189.0, 1.0], [2190.0, 1.0], [2191.0, 1.0], [2192.0, 1.0], [2193.0, 1.0], [2194.0, 1.0], [2195.0, 1.0], [2196.0, 1.0], [2197.0, 1.0], [2198.0, 1.0], [2199.0, 1.0], [2200.0, 1.0], [2201.0, 1.0], [2202.0, 1.0], [2203.0, 1.0], [2204.0, 1.0], [2205.0, 1.0], [2206.0, 1.0], [2207.0, 1.0], [2208.0, 1.0], [2209.0, 1.0], [2210.0, 1.0], [2211.0, 1.0], [2212.0, 1.0], [2213.0, 1.0], [2214.0, 1.0], [2215.0, 1.0], [2216.0, 1.0], [2217.0, 1.0], [2218.0, 1.0], [2219.0, 1.0], [2220.0, 1.0], [2221.0, 1.0], [2222.0, 1.0], [2223.0, 1.0], [2224.0, 1.0], [2225.0, 1.0], [2226.0, 1.0], [2227.0, 1.0], [2228.0, 1.0], [2229.0, 1.0], [2230.0, 1.0], [2231.0, 1.0], [2232.0, 1.0], [2233.0, 1.0], [2234.0, 1.0], [2235.0, 1.0], [2236.0, 1.0], [2237.0, 1.0], [2238.0, 1.0], [2239.0, 1.0], [2240.0, 1.0], [2241.0, 1.0], [2242.0, 1.0], [2243.0, 1.0], [2244.0, 1.0], [2245.0, 1.0], [2246.0, 1.0], [2247.0, 1.0], [2248.0, 1.0], [2249.0, 1.0], [2250.0, 1.0], [2251.0, 1.0], [2252.0, 1.0], [2253.0, 1.0], [2254.0, 1.0], [2255.0, 1.0], [2256.0, 1.0], [2257.0, 1.0], [2258.0, 1.0], [2259.0, 1.0], [2260.0, 1.0], [2261.0, 1.0], [2262.0, 1.0], [2263.0, 1.0], [2264.0, 1.0], [2265.0, 1.0], [2266.0, 1.0], [2267.0, 1.0], [2268.0, 1.0], [2269.0, 1.0], [2270.0, 1.0], [2271.0, 1.0], [2272.0, 1.0], [2273.0, 1.0], [2274.0, 1.0], [2275.0, 1.0], [2276.0, 1.0], [2277.0, 1.0], [2278.0, 1.0], [2279.0, 1.0], [2280.0, 1.0], [2281.0, 1.0], [2282.0, 1.0], [2283.0, 1.0], [2284.0, 1.0], [2285.0, 1.0], [2286.0, 1.0], [2287.0, 1.0], [2288.0, 1.0], [2289.0, 1.0], [2290.0, 1.0], [2291.0, 1.0], [2292.0, 1.0], [2293.0, 1.0], [2294.0, 1.0], [2295.0, 1.0], [2296.0, 1.0], [2297.0, 1.0], [2298.0, 1.0], [2299.0, 1.0], [2300.0, 1.0], [2301.0, 1.0], [2302.0, 1.0], [2303.0, 1.0], [2304.0, 1.0], [2305.0, 1.0], [2306.0, 1.0], [2307.0, 1.0], [2308.0, 1.0], [2309.0, 1.0], [2310.0, 1.0], [2311.0, 1.0], [2312.0, 1.0], [2313.0, 1.0], [2314.0, 1.0], [2315.0, 1.0], [2316.0, 1.0], [2317.0, 1.0], [2318.0, 1.0], [2319.0, 1.0], [2320.0, 1.0], [2321.0, 1.0], [2322.0, 1.0], [2323.0, 1.0], [2324.0, 1.0], [2325.0, 1.0], [2326.0, 1.0], [2327.0, 1.0], [2328.0, 1.0], [2329.0, 1.0], [2330.0, 1.0], [2331.0, 1.0], [2332.0, 1.0], [2333.0, 1.0], [2334.0, 1.0], [2335.0, 1.0], [2336.0, 1.0], [2337.0, 1.0], [2338.0, 1.0], [2339.0, 1.0], [2340.0, 1.0], [2341.0, 1.0], [2342.0, 1.0], [2343.0, 1.0], [2344.0, 1.0], [2345.0, 1.0], [2346.0, 1.0], [2347.0, 1.0], [2348.0, 1.0], [2349.0, 1.0], [2350.0, 1.0], [2351.0, 1.0], [2352.0, 1.0], [2353.0, 1.0], [2354.0, 1.0], [2355.0, 1.0], [2356.0, 1.0], [2357.0, 1.0], [2358.0, 1.0], [2359.0, 1.0], [2360.0, 1.0], [2361.0, 1.0], [2362.0, 1.0], [2363.0, 1.0], [2364.0, 1.0], [2365.0, 1.0], [2366.0, 1.0], [2367.0, 1.0], [2368.0, 1.0], [2369.0, 1.0], [2370.0, 1.0], [2371.0, 1.0], [2372.0, 1.0], [2373.0, 1.0], [2374.0, 1.0], [2375.0, 1.0], [2376.0, 1.0], [2377.0, 1.0], [2378.0, 1.0], [2379.0, 1.0], [2380.0, 1.0], [2381.0, 1.0], [2382.0, 1.0], [2383.0, 1.0], [2384.0, 1.0], [2385.0, 1.0], [2386.0, 1.0], [2387.0, 1.0], [2388.0, 1.0], [2389.0, 1.0], [2390.0, 1.0], [2391.0, 1.0], [2392.0, 1.0], [2393.0, 1.0], [2394.0, 1.0], [2395.0, 1.0], [2396.0, 1.0], [2397.0, 1.0], [2398.0, 1.0], [2399.0, 1.0], [2400.0, 1.0], [2401.0, 1.0], [2402.0, 1.0], [2403.0, 1.0], [2404.0, 1.0], [2405.0, 1.0], [2406.0, 1.0], [2407.0, 1.0], [2408.0, 1.0], [2409.0, 1.0], [2410.0, 1.0], [2411.0, 1.0], [2412.0, 1.0], [2413.0, 1.0], [2414.0, 1.0], [2415.0, 1.0], [2416.0, 1.0], [2417.0, 1.0], [2418.0, 1.0], [2419.0, 1.0], [2420.0, 1.0], [2421.0, 1.0], [2422.0, 1.0], [2423.0, 1.0], [2424.0, 1.0], [2425.0, 1.0], [2426.0, 1.0], [2427.0, 1.0], [2428.0, 1.0], [2429.0, 1.0], [2430.0, 1.0], [2431.0, 1.0], [2432.0, 1.0], [2433.0, 1.0], [2434.0, 1.0], [2435.0, 1.0], [2436.0, 1.0], [2437.0, 1.0], [2438.0, 1.0], [2439.0, 1.0], [2440.0, 1.0], [2441.0, 1.0], [2442.0, 1.0], [2443.0, 1.0], [2444.0, 1.0], [2445.0, 1.0], [2446.0, 1.0], [2447.0, 1.0], [2448.0, 1.0], [2449.0, 1.0], [2450.0, 1.0], [2451.0, 1.0], [2452.0, 1.0], [2453.0, 1.0], [2454.0, 1.0], [2455.0, 1.0], [2456.0, 1.0], [2457.0, 1.0], [2458.0, 1.0], [2459.0, 1.0], [2460.0, 1.0], [2461.0, 1.0], [2462.0, 1.0], [2463.0, 1.0], [2464.0, 1.0], [2465.0, 1.0], [2466.0, 1.0], [2467.0, 1.0], [2468.0, 1.0], [2469.0, 1.0], [2470.0, 1.0], [2471.0, 1.0], [2472.0, 1.0], [2473.0, 1.0], [2474.0, 1.0], [2475.0, 1.0], [2476.0, 1.0], [2477.0, 1.0], [2478.0, 1.0], [2479.0, 1.0], [2480.0, 1.0], [2481.0, 1.0], [2482.0, 1.0], [2483.0, 1.0], [2484.0, 1.0], [2485.0, 1.0], [2486.0, 1.0], [2487.0, 1.0], [2488.0, 1.0], [2489.0, 1.0], [2490.0, 1.0], [2491.0, 1.0], [2492.0, 1.0], [2493.0, 1.0], [2494.0, 1.0], [2495.0, 1.0], [2496.0, 1.0], [2497.0, 1.0], [2498.0, 1.0], [2499.0, 1.0], [2500.0, 1.0], [2501.0, 1.0], [2502.0, 1.0], [2503.0, 1.0], [2504.0, 1.0], [2505.0, 1.0], [2506.0, 1.0], [2507.0, 1.0], [2508.0, 1.0], [2509.0, 1.0], [2510.0, 1.0], [2511.0, 1.0], [2512.0, 1.0], [2513.0, 1.0], [2514.0, 1.0], [2515.0, 1.0], [2516.0, 1.0], [2517.0, 1.0], [2518.0, 1.0], [2519.0, 1.0], [2520.0, 1.0], [2521.0, 1.0], [2522.0, 1.0], [2523.0, 1.0], [2524.0, 1.0], [2525.0, 1.0], [2526.0, 1.0], [2527.0, 1.0], [2528.0, 1.0], [2529.0, 1.0], [2530.0, 1.0], [2531.0, 1.0], [2532.0, 1.0], [2533.0, 1.0], [2534.0, 1.0], [2535.0, 1.0], [2536.0, 1.0], [2537.0, 1.0], [2538.0, 1.0], [2539.0, 1.0], [2540.0, 1.0], [2541.0, 1.0], [2542.0, 1.0], [2543.0, 1.0], [2544.0, 1.0], [2545.0, 1.0], [2546.0, 1.0], [2547.0, 1.0], [2548.0, 1.0], [2549.0, 1.0], [2550.0, 1.0], [2551.0, 1.0], [2552.0, 1.0], [2553.0, 1.0], [2554.0, 1.0], [2555.0, 1.0], [2556.0, 1.0]]}, \"height\": 320.0});\n",
       "   }(mpld3);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/mpld3\n",
       "   require.config({paths: {d3: \"https://mpld3.github.io/js/d3.v3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      mpld3_load_lib(\"https://mpld3.github.io/js/mpld3.v0.2.js\", function(){\n",
       "         \n",
       "         mpld3.draw_figure(\"fig_el5466445855727843659457194\", {\"width\": 480.0, \"plugins\": [{\"type\": \"reset\"}, {\"button\": true, \"enabled\": false, \"type\": \"zoom\"}, {\"button\": true, \"enabled\": false, \"type\": \"boxzoom\"}], \"id\": \"el546644585572784\", \"axes\": [{\"lines\": [{\"coordinates\": \"data\", \"xindex\": 0, \"dasharray\": \"10,0\", \"color\": \"#0000FF\", \"id\": \"el546644586961272\", \"alpha\": 1, \"zorder\": 2, \"data\": \"data01\", \"yindex\": 1, \"linewidth\": 1.0}], \"images\": [], \"xdomain\": [0.0, 3000.0], \"ylim\": [0.0, 1400.0], \"axesbg\": \"#FFFFFF\", \"yscale\": \"linear\", \"id\": \"el546644587386976\", \"zoomable\": true, \"xlim\": [0.0, 3000.0], \"collections\": [], \"ydomain\": [0.0, 1400.0], \"axesbgalpha\": null, \"markers\": [], \"sharex\": [], \"paths\": [], \"xscale\": \"linear\", \"axes\": [{\"grid\": {\"gridOn\": false}, \"tickformat\": null, \"position\": \"bottom\", \"tickvalues\": null, \"nticks\": 7, \"scale\": \"linear\", \"fontsize\": 10.0}, {\"grid\": {\"gridOn\": false}, \"tickformat\": null, \"position\": \"left\", \"tickvalues\": null, \"nticks\": 8, \"scale\": \"linear\", \"fontsize\": 10.0}], \"texts\": [], \"bbox\": [0.125, 0.125, 0.775, 0.775], \"sharey\": []}], \"data\": {\"data01\": [[1.0, 1255.0], [2.0, 1255.0], [3.0, 509.0], [4.0, 390.0], [5.0, 265.0], [6.0, 243.0], [7.0, 208.0], [8.0, 187.0], [9.0, 146.0], [10.0, 142.0], [11.0, 136.0], [12.0, 113.0], [13.0, 109.0], [14.0, 103.0], [15.0, 98.0], [16.0, 94.0], [17.0, 88.0], [18.0, 82.0], [19.0, 71.0], [20.0, 67.0], [21.0, 65.0], [22.0, 61.0], [23.0, 60.0], [24.0, 58.0], [25.0, 58.0], [26.0, 57.0], [27.0, 56.0], [28.0, 55.0], [29.0, 55.0], [30.0, 53.0], [31.0, 50.0], [32.0, 47.0], [33.0, 45.0], [34.0, 45.0], [35.0, 45.0], [36.0, 44.0], [37.0, 42.0], [38.0, 42.0], [39.0, 41.0], [40.0, 40.0], [41.0, 40.0], [42.0, 40.0], [43.0, 40.0], [44.0, 36.0], [45.0, 34.0], [46.0, 34.0], [47.0, 32.0], [48.0, 32.0], [49.0, 30.0], [50.0, 29.0], [51.0, 29.0], [52.0, 28.0], [53.0, 27.0], [54.0, 26.0], [55.0, 26.0], [56.0, 26.0], [57.0, 25.0], [58.0, 25.0], [59.0, 25.0], [60.0, 25.0], [61.0, 24.0], [62.0, 24.0], [63.0, 23.0], [64.0, 22.0], [65.0, 22.0], [66.0, 21.0], [67.0, 21.0], [68.0, 21.0], [69.0, 20.0], [70.0, 19.0], [71.0, 19.0], [72.0, 19.0], [73.0, 18.0], [74.0, 18.0], [75.0, 18.0], [76.0, 18.0], [77.0, 18.0], [78.0, 17.0], [79.0, 17.0], [80.0, 17.0], [81.0, 17.0], [82.0, 17.0], [83.0, 17.0], [84.0, 16.0], [85.0, 16.0], [86.0, 16.0], [87.0, 16.0], [88.0, 16.0], [89.0, 16.0], [90.0, 16.0], [91.0, 16.0], [92.0, 16.0], [93.0, 16.0], [94.0, 15.0], [95.0, 15.0], [96.0, 15.0], [97.0, 15.0], [98.0, 14.0], [99.0, 14.0], [100.0, 14.0], [101.0, 14.0], [102.0, 14.0], [103.0, 14.0], [104.0, 14.0], [105.0, 13.0], [106.0, 13.0], [107.0, 13.0], [108.0, 13.0], [109.0, 13.0], [110.0, 13.0], [111.0, 13.0], [112.0, 13.0], [113.0, 13.0], [114.0, 12.0], [115.0, 12.0], [116.0, 12.0], [117.0, 12.0], [118.0, 12.0], [119.0, 11.0], [120.0, 11.0], [121.0, 11.0], [122.0, 11.0], [123.0, 11.0], [124.0, 11.0], [125.0, 11.0], [126.0, 11.0], [127.0, 11.0], [128.0, 11.0], [129.0, 11.0], [130.0, 11.0], [131.0, 11.0], [132.0, 11.0], [133.0, 11.0], [134.0, 11.0], [135.0, 11.0], [136.0, 11.0], [137.0, 10.0], [138.0, 10.0], [139.0, 10.0], [140.0, 10.0], [141.0, 10.0], [142.0, 10.0], [143.0, 10.0], [144.0, 10.0], [145.0, 10.0], [146.0, 10.0], [147.0, 10.0], [148.0, 10.0], [149.0, 10.0], [150.0, 10.0], [151.0, 10.0], [152.0, 9.0], [153.0, 9.0], [154.0, 9.0], [155.0, 9.0], [156.0, 9.0], [157.0, 9.0], [158.0, 9.0], [159.0, 9.0], [160.0, 9.0], [161.0, 9.0], [162.0, 9.0], [163.0, 9.0], [164.0, 9.0], [165.0, 9.0], [166.0, 9.0], [167.0, 9.0], [168.0, 9.0], [169.0, 9.0], [170.0, 8.0], [171.0, 8.0], [172.0, 8.0], [173.0, 8.0], [174.0, 8.0], [175.0, 8.0], [176.0, 8.0], [177.0, 8.0], [178.0, 8.0], [179.0, 8.0], [180.0, 8.0], [181.0, 8.0], [182.0, 8.0], [183.0, 8.0], [184.0, 8.0], [185.0, 8.0], [186.0, 8.0], [187.0, 8.0], [188.0, 8.0], [189.0, 7.0], [190.0, 7.0], [191.0, 7.0], [192.0, 7.0], [193.0, 7.0], [194.0, 7.0], [195.0, 7.0], [196.0, 7.0], [197.0, 7.0], [198.0, 7.0], [199.0, 7.0], [200.0, 7.0], [201.0, 7.0], [202.0, 7.0], [203.0, 7.0], [204.0, 7.0], [205.0, 7.0], [206.0, 7.0], [207.0, 7.0], [208.0, 7.0], [209.0, 7.0], [210.0, 7.0], [211.0, 7.0], [212.0, 7.0], [213.0, 7.0], [214.0, 7.0], [215.0, 7.0], [216.0, 7.0], [217.0, 7.0], [218.0, 7.0], [219.0, 7.0], [220.0, 6.0], [221.0, 6.0], [222.0, 6.0], [223.0, 6.0], [224.0, 6.0], [225.0, 6.0], [226.0, 6.0], [227.0, 6.0], [228.0, 6.0], [229.0, 6.0], [230.0, 6.0], [231.0, 6.0], [232.0, 6.0], [233.0, 6.0], [234.0, 6.0], [235.0, 6.0], [236.0, 6.0], [237.0, 6.0], [238.0, 6.0], [239.0, 6.0], [240.0, 6.0], [241.0, 6.0], [242.0, 6.0], [243.0, 6.0], [244.0, 6.0], [245.0, 6.0], [246.0, 6.0], [247.0, 6.0], [248.0, 6.0], [249.0, 6.0], [250.0, 6.0], [251.0, 6.0], [252.0, 6.0], [253.0, 6.0], [254.0, 6.0], [255.0, 6.0], [256.0, 6.0], [257.0, 6.0], [258.0, 5.0], [259.0, 5.0], [260.0, 5.0], [261.0, 5.0], [262.0, 5.0], [263.0, 5.0], [264.0, 5.0], [265.0, 5.0], [266.0, 5.0], [267.0, 5.0], [268.0, 5.0], [269.0, 5.0], [270.0, 5.0], [271.0, 5.0], [272.0, 5.0], [273.0, 5.0], [274.0, 5.0], [275.0, 5.0], [276.0, 5.0], [277.0, 5.0], [278.0, 5.0], [279.0, 5.0], [280.0, 5.0], [281.0, 5.0], [282.0, 5.0], [283.0, 5.0], [284.0, 5.0], [285.0, 5.0], [286.0, 5.0], [287.0, 5.0], [288.0, 5.0], [289.0, 5.0], [290.0, 5.0], [291.0, 5.0], [292.0, 5.0], [293.0, 5.0], [294.0, 5.0], [295.0, 5.0], [296.0, 5.0], [297.0, 5.0], [298.0, 5.0], [299.0, 5.0], [300.0, 5.0], [301.0, 5.0], [302.0, 5.0], [303.0, 5.0], [304.0, 5.0], [305.0, 5.0], [306.0, 5.0], [307.0, 5.0], [308.0, 5.0], [309.0, 5.0], [310.0, 5.0], [311.0, 5.0], [312.0, 5.0], [313.0, 5.0], [314.0, 5.0], [315.0, 5.0], [316.0, 5.0], [317.0, 5.0], [318.0, 4.0], [319.0, 4.0], [320.0, 4.0], [321.0, 4.0], [322.0, 4.0], [323.0, 4.0], [324.0, 4.0], [325.0, 4.0], [326.0, 4.0], [327.0, 4.0], [328.0, 4.0], [329.0, 4.0], [330.0, 4.0], [331.0, 4.0], [332.0, 4.0], [333.0, 4.0], [334.0, 4.0], [335.0, 4.0], [336.0, 4.0], [337.0, 4.0], [338.0, 4.0], [339.0, 4.0], [340.0, 4.0], [341.0, 4.0], [342.0, 4.0], [343.0, 4.0], [344.0, 4.0], [345.0, 4.0], [346.0, 4.0], [347.0, 4.0], [348.0, 4.0], [349.0, 4.0], [350.0, 4.0], [351.0, 4.0], [352.0, 4.0], [353.0, 4.0], [354.0, 4.0], [355.0, 4.0], [356.0, 4.0], [357.0, 4.0], [358.0, 4.0], [359.0, 4.0], [360.0, 4.0], [361.0, 4.0], [362.0, 4.0], [363.0, 4.0], [364.0, 4.0], [365.0, 4.0], [366.0, 4.0], [367.0, 4.0], [368.0, 4.0], [369.0, 4.0], [370.0, 4.0], [371.0, 4.0], [372.0, 4.0], [373.0, 4.0], [374.0, 4.0], [375.0, 4.0], [376.0, 4.0], [377.0, 4.0], [378.0, 4.0], [379.0, 4.0], [380.0, 4.0], [381.0, 4.0], [382.0, 4.0], [383.0, 4.0], [384.0, 4.0], [385.0, 4.0], [386.0, 4.0], [387.0, 4.0], [388.0, 4.0], [389.0, 4.0], [390.0, 4.0], [391.0, 4.0], [392.0, 4.0], [393.0, 4.0], [394.0, 4.0], [395.0, 4.0], [396.0, 4.0], [397.0, 4.0], [398.0, 4.0], [399.0, 3.0], [400.0, 3.0], [401.0, 3.0], [402.0, 3.0], [403.0, 3.0], [404.0, 3.0], [405.0, 3.0], [406.0, 3.0], [407.0, 3.0], [408.0, 3.0], [409.0, 3.0], [410.0, 3.0], [411.0, 3.0], [412.0, 3.0], [413.0, 3.0], [414.0, 3.0], [415.0, 3.0], [416.0, 3.0], [417.0, 3.0], [418.0, 3.0], [419.0, 3.0], [420.0, 3.0], [421.0, 3.0], [422.0, 3.0], [423.0, 3.0], [424.0, 3.0], [425.0, 3.0], [426.0, 3.0], [427.0, 3.0], [428.0, 3.0], [429.0, 3.0], [430.0, 3.0], [431.0, 3.0], [432.0, 3.0], [433.0, 3.0], [434.0, 3.0], [435.0, 3.0], [436.0, 3.0], [437.0, 3.0], [438.0, 3.0], [439.0, 3.0], [440.0, 3.0], [441.0, 3.0], [442.0, 3.0], [443.0, 3.0], [444.0, 3.0], [445.0, 3.0], [446.0, 3.0], [447.0, 3.0], [448.0, 3.0], [449.0, 3.0], [450.0, 3.0], [451.0, 3.0], [452.0, 3.0], [453.0, 3.0], [454.0, 3.0], [455.0, 3.0], [456.0, 3.0], [457.0, 3.0], [458.0, 3.0], [459.0, 3.0], [460.0, 3.0], [461.0, 3.0], [462.0, 3.0], [463.0, 3.0], [464.0, 3.0], [465.0, 3.0], [466.0, 3.0], [467.0, 3.0], [468.0, 3.0], [469.0, 3.0], [470.0, 3.0], [471.0, 3.0], [472.0, 3.0], [473.0, 3.0], [474.0, 3.0], [475.0, 3.0], [476.0, 3.0], [477.0, 3.0], [478.0, 3.0], [479.0, 3.0], [480.0, 3.0], [481.0, 3.0], [482.0, 3.0], [483.0, 3.0], [484.0, 3.0], [485.0, 3.0], [486.0, 3.0], [487.0, 3.0], [488.0, 3.0], [489.0, 3.0], [490.0, 3.0], [491.0, 3.0], [492.0, 3.0], [493.0, 3.0], [494.0, 3.0], [495.0, 3.0], [496.0, 3.0], [497.0, 3.0], [498.0, 3.0], [499.0, 3.0], [500.0, 3.0], [501.0, 3.0], [502.0, 3.0], [503.0, 3.0], [504.0, 3.0], [505.0, 3.0], [506.0, 3.0], [507.0, 3.0], [508.0, 3.0], [509.0, 3.0], [510.0, 3.0], [511.0, 3.0], [512.0, 3.0], [513.0, 3.0], [514.0, 3.0], [515.0, 3.0], [516.0, 3.0], [517.0, 3.0], [518.0, 3.0], [519.0, 3.0], [520.0, 3.0], [521.0, 3.0], [522.0, 3.0], [523.0, 3.0], [524.0, 3.0], [525.0, 3.0], [526.0, 3.0], [527.0, 3.0], [528.0, 3.0], [529.0, 3.0], [530.0, 3.0], [531.0, 3.0], [532.0, 3.0], [533.0, 3.0], [534.0, 3.0], [535.0, 3.0], [536.0, 3.0], [537.0, 3.0], [538.0, 3.0], [539.0, 3.0], [540.0, 3.0], [541.0, 3.0], [542.0, 3.0], [543.0, 3.0], [544.0, 3.0], [545.0, 3.0], [546.0, 3.0], [547.0, 3.0], [548.0, 3.0], [549.0, 3.0], [550.0, 3.0], [551.0, 3.0], [552.0, 3.0], [553.0, 3.0], [554.0, 3.0], [555.0, 3.0], [556.0, 3.0], [557.0, 3.0], [558.0, 3.0], [559.0, 3.0], [560.0, 3.0], [561.0, 3.0], [562.0, 3.0], [563.0, 3.0], [564.0, 3.0], [565.0, 3.0], [566.0, 2.0], [567.0, 2.0], [568.0, 2.0], [569.0, 2.0], [570.0, 2.0], [571.0, 2.0], [572.0, 2.0], [573.0, 2.0], [574.0, 2.0], [575.0, 2.0], [576.0, 2.0], [577.0, 2.0], [578.0, 2.0], [579.0, 2.0], [580.0, 2.0], [581.0, 2.0], [582.0, 2.0], [583.0, 2.0], [584.0, 2.0], [585.0, 2.0], [586.0, 2.0], [587.0, 2.0], [588.0, 2.0], [589.0, 2.0], [590.0, 2.0], [591.0, 2.0], [592.0, 2.0], [593.0, 2.0], [594.0, 2.0], [595.0, 2.0], [596.0, 2.0], [597.0, 2.0], [598.0, 2.0], [599.0, 2.0], [600.0, 2.0], [601.0, 2.0], [602.0, 2.0], [603.0, 2.0], [604.0, 2.0], [605.0, 2.0], [606.0, 2.0], [607.0, 2.0], [608.0, 2.0], [609.0, 2.0], [610.0, 2.0], [611.0, 2.0], [612.0, 2.0], [613.0, 2.0], [614.0, 2.0], [615.0, 2.0], [616.0, 2.0], [617.0, 2.0], [618.0, 2.0], [619.0, 2.0], [620.0, 2.0], [621.0, 2.0], [622.0, 2.0], [623.0, 2.0], [624.0, 2.0], [625.0, 2.0], [626.0, 2.0], [627.0, 2.0], [628.0, 2.0], [629.0, 2.0], [630.0, 2.0], [631.0, 2.0], [632.0, 2.0], [633.0, 2.0], [634.0, 2.0], [635.0, 2.0], [636.0, 2.0], [637.0, 2.0], [638.0, 2.0], [639.0, 2.0], [640.0, 2.0], [641.0, 2.0], [642.0, 2.0], [643.0, 2.0], [644.0, 2.0], [645.0, 2.0], [646.0, 2.0], [647.0, 2.0], [648.0, 2.0], [649.0, 2.0], [650.0, 2.0], [651.0, 2.0], [652.0, 2.0], [653.0, 2.0], [654.0, 2.0], [655.0, 2.0], [656.0, 2.0], [657.0, 2.0], [658.0, 2.0], [659.0, 2.0], [660.0, 2.0], [661.0, 2.0], [662.0, 2.0], [663.0, 2.0], [664.0, 2.0], [665.0, 2.0], [666.0, 2.0], [667.0, 2.0], [668.0, 2.0], [669.0, 2.0], [670.0, 2.0], [671.0, 2.0], [672.0, 2.0], [673.0, 2.0], [674.0, 2.0], [675.0, 2.0], [676.0, 2.0], [677.0, 2.0], [678.0, 2.0], [679.0, 2.0], [680.0, 2.0], [681.0, 2.0], [682.0, 2.0], [683.0, 2.0], [684.0, 2.0], [685.0, 2.0], [686.0, 2.0], [687.0, 2.0], [688.0, 2.0], [689.0, 2.0], [690.0, 2.0], [691.0, 2.0], [692.0, 2.0], [693.0, 2.0], [694.0, 2.0], [695.0, 2.0], [696.0, 2.0], [697.0, 2.0], [698.0, 2.0], [699.0, 2.0], [700.0, 2.0], [701.0, 2.0], [702.0, 2.0], [703.0, 2.0], [704.0, 2.0], [705.0, 2.0], [706.0, 2.0], [707.0, 2.0], [708.0, 2.0], [709.0, 2.0], [710.0, 2.0], [711.0, 2.0], [712.0, 2.0], [713.0, 2.0], [714.0, 2.0], [715.0, 2.0], [716.0, 2.0], [717.0, 2.0], [718.0, 2.0], [719.0, 2.0], [720.0, 2.0], [721.0, 2.0], [722.0, 2.0], [723.0, 2.0], [724.0, 2.0], [725.0, 2.0], [726.0, 2.0], [727.0, 2.0], [728.0, 2.0], [729.0, 2.0], [730.0, 2.0], [731.0, 2.0], [732.0, 2.0], [733.0, 2.0], [734.0, 2.0], [735.0, 2.0], [736.0, 2.0], [737.0, 2.0], [738.0, 2.0], [739.0, 2.0], [740.0, 2.0], [741.0, 2.0], [742.0, 2.0], [743.0, 2.0], [744.0, 2.0], [745.0, 2.0], [746.0, 2.0], [747.0, 2.0], [748.0, 2.0], [749.0, 2.0], [750.0, 2.0], [751.0, 2.0], [752.0, 2.0], [753.0, 2.0], [754.0, 2.0], [755.0, 2.0], [756.0, 2.0], [757.0, 2.0], [758.0, 2.0], [759.0, 2.0], [760.0, 2.0], [761.0, 2.0], [762.0, 2.0], [763.0, 2.0], [764.0, 2.0], [765.0, 2.0], [766.0, 2.0], [767.0, 2.0], [768.0, 2.0], [769.0, 2.0], [770.0, 2.0], [771.0, 2.0], [772.0, 2.0], [773.0, 2.0], [774.0, 2.0], [775.0, 2.0], [776.0, 2.0], [777.0, 2.0], [778.0, 2.0], [779.0, 2.0], [780.0, 2.0], [781.0, 2.0], [782.0, 2.0], [783.0, 2.0], [784.0, 2.0], [785.0, 2.0], [786.0, 2.0], [787.0, 2.0], [788.0, 2.0], [789.0, 2.0], [790.0, 2.0], [791.0, 2.0], [792.0, 2.0], [793.0, 2.0], [794.0, 2.0], [795.0, 2.0], [796.0, 2.0], [797.0, 2.0], [798.0, 2.0], [799.0, 2.0], [800.0, 2.0], [801.0, 2.0], [802.0, 2.0], [803.0, 2.0], [804.0, 2.0], [805.0, 2.0], [806.0, 2.0], [807.0, 2.0], [808.0, 2.0], [809.0, 2.0], [810.0, 2.0], [811.0, 2.0], [812.0, 2.0], [813.0, 2.0], [814.0, 2.0], [815.0, 2.0], [816.0, 2.0], [817.0, 2.0], [818.0, 2.0], [819.0, 2.0], [820.0, 2.0], [821.0, 2.0], [822.0, 2.0], [823.0, 2.0], [824.0, 2.0], [825.0, 2.0], [826.0, 2.0], [827.0, 2.0], [828.0, 2.0], [829.0, 2.0], [830.0, 2.0], [831.0, 2.0], [832.0, 2.0], [833.0, 2.0], [834.0, 2.0], [835.0, 2.0], [836.0, 2.0], [837.0, 2.0], [838.0, 2.0], [839.0, 2.0], [840.0, 2.0], [841.0, 2.0], [842.0, 2.0], [843.0, 2.0], [844.0, 2.0], [845.0, 2.0], [846.0, 2.0], [847.0, 2.0], [848.0, 2.0], [849.0, 2.0], [850.0, 2.0], [851.0, 2.0], [852.0, 2.0], [853.0, 2.0], [854.0, 2.0], [855.0, 2.0], [856.0, 2.0], [857.0, 2.0], [858.0, 2.0], [859.0, 2.0], [860.0, 2.0], [861.0, 2.0], [862.0, 2.0], [863.0, 2.0], [864.0, 2.0], [865.0, 2.0], [866.0, 2.0], [867.0, 2.0], [868.0, 2.0], [869.0, 2.0], [870.0, 2.0], [871.0, 2.0], [872.0, 2.0], [873.0, 2.0], [874.0, 2.0], [875.0, 2.0], [876.0, 2.0], [877.0, 2.0], [878.0, 2.0], [879.0, 2.0], [880.0, 2.0], [881.0, 2.0], [882.0, 2.0], [883.0, 2.0], [884.0, 2.0], [885.0, 2.0], [886.0, 2.0], [887.0, 2.0], [888.0, 2.0], [889.0, 2.0], [890.0, 2.0], [891.0, 2.0], [892.0, 2.0], [893.0, 2.0], [894.0, 2.0], [895.0, 2.0], [896.0, 2.0], [897.0, 2.0], [898.0, 2.0], [899.0, 2.0], [900.0, 2.0], [901.0, 2.0], [902.0, 2.0], [903.0, 2.0], [904.0, 2.0], [905.0, 2.0], [906.0, 2.0], [907.0, 2.0], [908.0, 2.0], [909.0, 2.0], [910.0, 2.0], [911.0, 2.0], [912.0, 2.0], [913.0, 2.0], [914.0, 2.0], [915.0, 2.0], [916.0, 2.0], [917.0, 2.0], [918.0, 2.0], [919.0, 2.0], [920.0, 2.0], [921.0, 2.0], [922.0, 2.0], [923.0, 2.0], [924.0, 2.0], [925.0, 2.0], [926.0, 2.0], [927.0, 2.0], [928.0, 1.0], [929.0, 1.0], [930.0, 1.0], [931.0, 1.0], [932.0, 1.0], [933.0, 1.0], [934.0, 1.0], [935.0, 1.0], [936.0, 1.0], [937.0, 1.0], [938.0, 1.0], [939.0, 1.0], [940.0, 1.0], [941.0, 1.0], [942.0, 1.0], [943.0, 1.0], [944.0, 1.0], [945.0, 1.0], [946.0, 1.0], [947.0, 1.0], [948.0, 1.0], [949.0, 1.0], [950.0, 1.0], [951.0, 1.0], [952.0, 1.0], [953.0, 1.0], [954.0, 1.0], [955.0, 1.0], [956.0, 1.0], [957.0, 1.0], [958.0, 1.0], [959.0, 1.0], [960.0, 1.0], [961.0, 1.0], [962.0, 1.0], [963.0, 1.0], [964.0, 1.0], [965.0, 1.0], [966.0, 1.0], [967.0, 1.0], [968.0, 1.0], [969.0, 1.0], [970.0, 1.0], [971.0, 1.0], [972.0, 1.0], [973.0, 1.0], [974.0, 1.0], [975.0, 1.0], [976.0, 1.0], [977.0, 1.0], [978.0, 1.0], [979.0, 1.0], [980.0, 1.0], [981.0, 1.0], [982.0, 1.0], [983.0, 1.0], [984.0, 1.0], [985.0, 1.0], [986.0, 1.0], [987.0, 1.0], [988.0, 1.0], [989.0, 1.0], [990.0, 1.0], [991.0, 1.0], [992.0, 1.0], [993.0, 1.0], [994.0, 1.0], [995.0, 1.0], [996.0, 1.0], [997.0, 1.0], [998.0, 1.0], [999.0, 1.0], [1000.0, 1.0], [1001.0, 1.0], [1002.0, 1.0], [1003.0, 1.0], [1004.0, 1.0], [1005.0, 1.0], [1006.0, 1.0], [1007.0, 1.0], [1008.0, 1.0], [1009.0, 1.0], [1010.0, 1.0], [1011.0, 1.0], [1012.0, 1.0], [1013.0, 1.0], [1014.0, 1.0], [1015.0, 1.0], [1016.0, 1.0], [1017.0, 1.0], [1018.0, 1.0], [1019.0, 1.0], [1020.0, 1.0], [1021.0, 1.0], [1022.0, 1.0], [1023.0, 1.0], [1024.0, 1.0], [1025.0, 1.0], [1026.0, 1.0], [1027.0, 1.0], [1028.0, 1.0], [1029.0, 1.0], [1030.0, 1.0], [1031.0, 1.0], [1032.0, 1.0], [1033.0, 1.0], [1034.0, 1.0], [1035.0, 1.0], [1036.0, 1.0], [1037.0, 1.0], [1038.0, 1.0], [1039.0, 1.0], [1040.0, 1.0], [1041.0, 1.0], [1042.0, 1.0], [1043.0, 1.0], [1044.0, 1.0], [1045.0, 1.0], [1046.0, 1.0], [1047.0, 1.0], [1048.0, 1.0], [1049.0, 1.0], [1050.0, 1.0], [1051.0, 1.0], [1052.0, 1.0], [1053.0, 1.0], [1054.0, 1.0], [1055.0, 1.0], [1056.0, 1.0], [1057.0, 1.0], [1058.0, 1.0], [1059.0, 1.0], [1060.0, 1.0], [1061.0, 1.0], [1062.0, 1.0], [1063.0, 1.0], [1064.0, 1.0], [1065.0, 1.0], [1066.0, 1.0], [1067.0, 1.0], [1068.0, 1.0], [1069.0, 1.0], [1070.0, 1.0], [1071.0, 1.0], [1072.0, 1.0], [1073.0, 1.0], [1074.0, 1.0], [1075.0, 1.0], [1076.0, 1.0], [1077.0, 1.0], [1078.0, 1.0], [1079.0, 1.0], [1080.0, 1.0], [1081.0, 1.0], [1082.0, 1.0], [1083.0, 1.0], [1084.0, 1.0], [1085.0, 1.0], [1086.0, 1.0], [1087.0, 1.0], [1088.0, 1.0], [1089.0, 1.0], [1090.0, 1.0], [1091.0, 1.0], [1092.0, 1.0], [1093.0, 1.0], [1094.0, 1.0], [1095.0, 1.0], [1096.0, 1.0], [1097.0, 1.0], [1098.0, 1.0], [1099.0, 1.0], [1100.0, 1.0], [1101.0, 1.0], [1102.0, 1.0], [1103.0, 1.0], [1104.0, 1.0], [1105.0, 1.0], [1106.0, 1.0], [1107.0, 1.0], [1108.0, 1.0], [1109.0, 1.0], [1110.0, 1.0], [1111.0, 1.0], [1112.0, 1.0], [1113.0, 1.0], [1114.0, 1.0], [1115.0, 1.0], [1116.0, 1.0], [1117.0, 1.0], [1118.0, 1.0], [1119.0, 1.0], [1120.0, 1.0], [1121.0, 1.0], [1122.0, 1.0], [1123.0, 1.0], [1124.0, 1.0], [1125.0, 1.0], [1126.0, 1.0], [1127.0, 1.0], [1128.0, 1.0], [1129.0, 1.0], [1130.0, 1.0], [1131.0, 1.0], [1132.0, 1.0], [1133.0, 1.0], [1134.0, 1.0], [1135.0, 1.0], [1136.0, 1.0], [1137.0, 1.0], [1138.0, 1.0], [1139.0, 1.0], [1140.0, 1.0], [1141.0, 1.0], [1142.0, 1.0], [1143.0, 1.0], [1144.0, 1.0], [1145.0, 1.0], [1146.0, 1.0], [1147.0, 1.0], [1148.0, 1.0], [1149.0, 1.0], [1150.0, 1.0], [1151.0, 1.0], [1152.0, 1.0], [1153.0, 1.0], [1154.0, 1.0], [1155.0, 1.0], [1156.0, 1.0], [1157.0, 1.0], [1158.0, 1.0], [1159.0, 1.0], [1160.0, 1.0], [1161.0, 1.0], [1162.0, 1.0], [1163.0, 1.0], [1164.0, 1.0], [1165.0, 1.0], [1166.0, 1.0], [1167.0, 1.0], [1168.0, 1.0], [1169.0, 1.0], [1170.0, 1.0], [1171.0, 1.0], [1172.0, 1.0], [1173.0, 1.0], [1174.0, 1.0], [1175.0, 1.0], [1176.0, 1.0], [1177.0, 1.0], [1178.0, 1.0], [1179.0, 1.0], [1180.0, 1.0], [1181.0, 1.0], [1182.0, 1.0], [1183.0, 1.0], [1184.0, 1.0], [1185.0, 1.0], [1186.0, 1.0], [1187.0, 1.0], [1188.0, 1.0], [1189.0, 1.0], [1190.0, 1.0], [1191.0, 1.0], [1192.0, 1.0], [1193.0, 1.0], [1194.0, 1.0], [1195.0, 1.0], [1196.0, 1.0], [1197.0, 1.0], [1198.0, 1.0], [1199.0, 1.0], [1200.0, 1.0], [1201.0, 1.0], [1202.0, 1.0], [1203.0, 1.0], [1204.0, 1.0], [1205.0, 1.0], [1206.0, 1.0], [1207.0, 1.0], [1208.0, 1.0], [1209.0, 1.0], [1210.0, 1.0], [1211.0, 1.0], [1212.0, 1.0], [1213.0, 1.0], [1214.0, 1.0], [1215.0, 1.0], [1216.0, 1.0], [1217.0, 1.0], [1218.0, 1.0], [1219.0, 1.0], [1220.0, 1.0], [1221.0, 1.0], [1222.0, 1.0], [1223.0, 1.0], [1224.0, 1.0], [1225.0, 1.0], [1226.0, 1.0], [1227.0, 1.0], [1228.0, 1.0], [1229.0, 1.0], [1230.0, 1.0], [1231.0, 1.0], [1232.0, 1.0], [1233.0, 1.0], [1234.0, 1.0], [1235.0, 1.0], [1236.0, 1.0], [1237.0, 1.0], [1238.0, 1.0], [1239.0, 1.0], [1240.0, 1.0], [1241.0, 1.0], [1242.0, 1.0], [1243.0, 1.0], [1244.0, 1.0], [1245.0, 1.0], [1246.0, 1.0], [1247.0, 1.0], [1248.0, 1.0], [1249.0, 1.0], [1250.0, 1.0], [1251.0, 1.0], [1252.0, 1.0], [1253.0, 1.0], [1254.0, 1.0], [1255.0, 1.0], [1256.0, 1.0], [1257.0, 1.0], [1258.0, 1.0], [1259.0, 1.0], [1260.0, 1.0], [1261.0, 1.0], [1262.0, 1.0], [1263.0, 1.0], [1264.0, 1.0], [1265.0, 1.0], [1266.0, 1.0], [1267.0, 1.0], [1268.0, 1.0], [1269.0, 1.0], [1270.0, 1.0], [1271.0, 1.0], [1272.0, 1.0], [1273.0, 1.0], [1274.0, 1.0], [1275.0, 1.0], [1276.0, 1.0], [1277.0, 1.0], [1278.0, 1.0], [1279.0, 1.0], [1280.0, 1.0], [1281.0, 1.0], [1282.0, 1.0], [1283.0, 1.0], [1284.0, 1.0], [1285.0, 1.0], [1286.0, 1.0], [1287.0, 1.0], [1288.0, 1.0], [1289.0, 1.0], [1290.0, 1.0], [1291.0, 1.0], [1292.0, 1.0], [1293.0, 1.0], [1294.0, 1.0], [1295.0, 1.0], [1296.0, 1.0], [1297.0, 1.0], [1298.0, 1.0], [1299.0, 1.0], [1300.0, 1.0], [1301.0, 1.0], [1302.0, 1.0], [1303.0, 1.0], [1304.0, 1.0], [1305.0, 1.0], [1306.0, 1.0], [1307.0, 1.0], [1308.0, 1.0], [1309.0, 1.0], [1310.0, 1.0], [1311.0, 1.0], [1312.0, 1.0], [1313.0, 1.0], [1314.0, 1.0], [1315.0, 1.0], [1316.0, 1.0], [1317.0, 1.0], [1318.0, 1.0], [1319.0, 1.0], [1320.0, 1.0], [1321.0, 1.0], [1322.0, 1.0], [1323.0, 1.0], [1324.0, 1.0], [1325.0, 1.0], [1326.0, 1.0], [1327.0, 1.0], [1328.0, 1.0], [1329.0, 1.0], [1330.0, 1.0], [1331.0, 1.0], [1332.0, 1.0], [1333.0, 1.0], [1334.0, 1.0], [1335.0, 1.0], [1336.0, 1.0], [1337.0, 1.0], [1338.0, 1.0], [1339.0, 1.0], [1340.0, 1.0], [1341.0, 1.0], [1342.0, 1.0], [1343.0, 1.0], [1344.0, 1.0], [1345.0, 1.0], [1346.0, 1.0], [1347.0, 1.0], [1348.0, 1.0], [1349.0, 1.0], [1350.0, 1.0], [1351.0, 1.0], [1352.0, 1.0], [1353.0, 1.0], [1354.0, 1.0], [1355.0, 1.0], [1356.0, 1.0], [1357.0, 1.0], [1358.0, 1.0], [1359.0, 1.0], [1360.0, 1.0], [1361.0, 1.0], [1362.0, 1.0], [1363.0, 1.0], [1364.0, 1.0], [1365.0, 1.0], [1366.0, 1.0], [1367.0, 1.0], [1368.0, 1.0], [1369.0, 1.0], [1370.0, 1.0], [1371.0, 1.0], [1372.0, 1.0], [1373.0, 1.0], [1374.0, 1.0], [1375.0, 1.0], [1376.0, 1.0], [1377.0, 1.0], [1378.0, 1.0], [1379.0, 1.0], [1380.0, 1.0], [1381.0, 1.0], [1382.0, 1.0], [1383.0, 1.0], [1384.0, 1.0], [1385.0, 1.0], [1386.0, 1.0], [1387.0, 1.0], [1388.0, 1.0], [1389.0, 1.0], [1390.0, 1.0], [1391.0, 1.0], [1392.0, 1.0], [1393.0, 1.0], [1394.0, 1.0], [1395.0, 1.0], [1396.0, 1.0], [1397.0, 1.0], [1398.0, 1.0], [1399.0, 1.0], [1400.0, 1.0], [1401.0, 1.0], [1402.0, 1.0], [1403.0, 1.0], [1404.0, 1.0], [1405.0, 1.0], [1406.0, 1.0], [1407.0, 1.0], [1408.0, 1.0], [1409.0, 1.0], [1410.0, 1.0], [1411.0, 1.0], [1412.0, 1.0], [1413.0, 1.0], [1414.0, 1.0], [1415.0, 1.0], [1416.0, 1.0], [1417.0, 1.0], [1418.0, 1.0], [1419.0, 1.0], [1420.0, 1.0], [1421.0, 1.0], [1422.0, 1.0], [1423.0, 1.0], [1424.0, 1.0], [1425.0, 1.0], [1426.0, 1.0], [1427.0, 1.0], [1428.0, 1.0], [1429.0, 1.0], [1430.0, 1.0], [1431.0, 1.0], [1432.0, 1.0], [1433.0, 1.0], [1434.0, 1.0], [1435.0, 1.0], [1436.0, 1.0], [1437.0, 1.0], [1438.0, 1.0], [1439.0, 1.0], [1440.0, 1.0], [1441.0, 1.0], [1442.0, 1.0], [1443.0, 1.0], [1444.0, 1.0], [1445.0, 1.0], [1446.0, 1.0], [1447.0, 1.0], [1448.0, 1.0], [1449.0, 1.0], [1450.0, 1.0], [1451.0, 1.0], [1452.0, 1.0], [1453.0, 1.0], [1454.0, 1.0], [1455.0, 1.0], [1456.0, 1.0], [1457.0, 1.0], [1458.0, 1.0], [1459.0, 1.0], [1460.0, 1.0], [1461.0, 1.0], [1462.0, 1.0], [1463.0, 1.0], [1464.0, 1.0], [1465.0, 1.0], [1466.0, 1.0], [1467.0, 1.0], [1468.0, 1.0], [1469.0, 1.0], [1470.0, 1.0], [1471.0, 1.0], [1472.0, 1.0], [1473.0, 1.0], [1474.0, 1.0], [1475.0, 1.0], [1476.0, 1.0], [1477.0, 1.0], [1478.0, 1.0], [1479.0, 1.0], [1480.0, 1.0], [1481.0, 1.0], [1482.0, 1.0], [1483.0, 1.0], [1484.0, 1.0], [1485.0, 1.0], [1486.0, 1.0], [1487.0, 1.0], [1488.0, 1.0], [1489.0, 1.0], [1490.0, 1.0], [1491.0, 1.0], [1492.0, 1.0], [1493.0, 1.0], [1494.0, 1.0], [1495.0, 1.0], [1496.0, 1.0], [1497.0, 1.0], [1498.0, 1.0], [1499.0, 1.0], [1500.0, 1.0], [1501.0, 1.0], [1502.0, 1.0], [1503.0, 1.0], [1504.0, 1.0], [1505.0, 1.0], [1506.0, 1.0], [1507.0, 1.0], [1508.0, 1.0], [1509.0, 1.0], [1510.0, 1.0], [1511.0, 1.0], [1512.0, 1.0], [1513.0, 1.0], [1514.0, 1.0], [1515.0, 1.0], [1516.0, 1.0], [1517.0, 1.0], [1518.0, 1.0], [1519.0, 1.0], [1520.0, 1.0], [1521.0, 1.0], [1522.0, 1.0], [1523.0, 1.0], [1524.0, 1.0], [1525.0, 1.0], [1526.0, 1.0], [1527.0, 1.0], [1528.0, 1.0], [1529.0, 1.0], [1530.0, 1.0], [1531.0, 1.0], [1532.0, 1.0], [1533.0, 1.0], [1534.0, 1.0], [1535.0, 1.0], [1536.0, 1.0], [1537.0, 1.0], [1538.0, 1.0], [1539.0, 1.0], [1540.0, 1.0], [1541.0, 1.0], [1542.0, 1.0], [1543.0, 1.0], [1544.0, 1.0], [1545.0, 1.0], [1546.0, 1.0], [1547.0, 1.0], [1548.0, 1.0], [1549.0, 1.0], [1550.0, 1.0], [1551.0, 1.0], [1552.0, 1.0], [1553.0, 1.0], [1554.0, 1.0], [1555.0, 1.0], [1556.0, 1.0], [1557.0, 1.0], [1558.0, 1.0], [1559.0, 1.0], [1560.0, 1.0], [1561.0, 1.0], [1562.0, 1.0], [1563.0, 1.0], [1564.0, 1.0], [1565.0, 1.0], [1566.0, 1.0], [1567.0, 1.0], [1568.0, 1.0], [1569.0, 1.0], [1570.0, 1.0], [1571.0, 1.0], [1572.0, 1.0], [1573.0, 1.0], [1574.0, 1.0], [1575.0, 1.0], [1576.0, 1.0], [1577.0, 1.0], [1578.0, 1.0], [1579.0, 1.0], [1580.0, 1.0], [1581.0, 1.0], [1582.0, 1.0], [1583.0, 1.0], [1584.0, 1.0], [1585.0, 1.0], [1586.0, 1.0], [1587.0, 1.0], [1588.0, 1.0], [1589.0, 1.0], [1590.0, 1.0], [1591.0, 1.0], [1592.0, 1.0], [1593.0, 1.0], [1594.0, 1.0], [1595.0, 1.0], [1596.0, 1.0], [1597.0, 1.0], [1598.0, 1.0], [1599.0, 1.0], [1600.0, 1.0], [1601.0, 1.0], [1602.0, 1.0], [1603.0, 1.0], [1604.0, 1.0], [1605.0, 1.0], [1606.0, 1.0], [1607.0, 1.0], [1608.0, 1.0], [1609.0, 1.0], [1610.0, 1.0], [1611.0, 1.0], [1612.0, 1.0], [1613.0, 1.0], [1614.0, 1.0], [1615.0, 1.0], [1616.0, 1.0], [1617.0, 1.0], [1618.0, 1.0], [1619.0, 1.0], [1620.0, 1.0], [1621.0, 1.0], [1622.0, 1.0], [1623.0, 1.0], [1624.0, 1.0], [1625.0, 1.0], [1626.0, 1.0], [1627.0, 1.0], [1628.0, 1.0], [1629.0, 1.0], [1630.0, 1.0], [1631.0, 1.0], [1632.0, 1.0], [1633.0, 1.0], [1634.0, 1.0], [1635.0, 1.0], [1636.0, 1.0], [1637.0, 1.0], [1638.0, 1.0], [1639.0, 1.0], [1640.0, 1.0], [1641.0, 1.0], [1642.0, 1.0], [1643.0, 1.0], [1644.0, 1.0], [1645.0, 1.0], [1646.0, 1.0], [1647.0, 1.0], [1648.0, 1.0], [1649.0, 1.0], [1650.0, 1.0], [1651.0, 1.0], [1652.0, 1.0], [1653.0, 1.0], [1654.0, 1.0], [1655.0, 1.0], [1656.0, 1.0], [1657.0, 1.0], [1658.0, 1.0], [1659.0, 1.0], [1660.0, 1.0], [1661.0, 1.0], [1662.0, 1.0], [1663.0, 1.0], [1664.0, 1.0], [1665.0, 1.0], [1666.0, 1.0], [1667.0, 1.0], [1668.0, 1.0], [1669.0, 1.0], [1670.0, 1.0], [1671.0, 1.0], [1672.0, 1.0], [1673.0, 1.0], [1674.0, 1.0], [1675.0, 1.0], [1676.0, 1.0], [1677.0, 1.0], [1678.0, 1.0], [1679.0, 1.0], [1680.0, 1.0], [1681.0, 1.0], [1682.0, 1.0], [1683.0, 1.0], [1684.0, 1.0], [1685.0, 1.0], [1686.0, 1.0], [1687.0, 1.0], [1688.0, 1.0], [1689.0, 1.0], [1690.0, 1.0], [1691.0, 1.0], [1692.0, 1.0], [1693.0, 1.0], [1694.0, 1.0], [1695.0, 1.0], [1696.0, 1.0], [1697.0, 1.0], [1698.0, 1.0], [1699.0, 1.0], [1700.0, 1.0], [1701.0, 1.0], [1702.0, 1.0], [1703.0, 1.0], [1704.0, 1.0], [1705.0, 1.0], [1706.0, 1.0], [1707.0, 1.0], [1708.0, 1.0], [1709.0, 1.0], [1710.0, 1.0], [1711.0, 1.0], [1712.0, 1.0], [1713.0, 1.0], [1714.0, 1.0], [1715.0, 1.0], [1716.0, 1.0], [1717.0, 1.0], [1718.0, 1.0], [1719.0, 1.0], [1720.0, 1.0], [1721.0, 1.0], [1722.0, 1.0], [1723.0, 1.0], [1724.0, 1.0], [1725.0, 1.0], [1726.0, 1.0], [1727.0, 1.0], [1728.0, 1.0], [1729.0, 1.0], [1730.0, 1.0], [1731.0, 1.0], [1732.0, 1.0], [1733.0, 1.0], [1734.0, 1.0], [1735.0, 1.0], [1736.0, 1.0], [1737.0, 1.0], [1738.0, 1.0], [1739.0, 1.0], [1740.0, 1.0], [1741.0, 1.0], [1742.0, 1.0], [1743.0, 1.0], [1744.0, 1.0], [1745.0, 1.0], [1746.0, 1.0], [1747.0, 1.0], [1748.0, 1.0], [1749.0, 1.0], [1750.0, 1.0], [1751.0, 1.0], [1752.0, 1.0], [1753.0, 1.0], [1754.0, 1.0], [1755.0, 1.0], [1756.0, 1.0], [1757.0, 1.0], [1758.0, 1.0], [1759.0, 1.0], [1760.0, 1.0], [1761.0, 1.0], [1762.0, 1.0], [1763.0, 1.0], [1764.0, 1.0], [1765.0, 1.0], [1766.0, 1.0], [1767.0, 1.0], [1768.0, 1.0], [1769.0, 1.0], [1770.0, 1.0], [1771.0, 1.0], [1772.0, 1.0], [1773.0, 1.0], [1774.0, 1.0], [1775.0, 1.0], [1776.0, 1.0], [1777.0, 1.0], [1778.0, 1.0], [1779.0, 1.0], [1780.0, 1.0], [1781.0, 1.0], [1782.0, 1.0], [1783.0, 1.0], [1784.0, 1.0], [1785.0, 1.0], [1786.0, 1.0], [1787.0, 1.0], [1788.0, 1.0], [1789.0, 1.0], [1790.0, 1.0], [1791.0, 1.0], [1792.0, 1.0], [1793.0, 1.0], [1794.0, 1.0], [1795.0, 1.0], [1796.0, 1.0], [1797.0, 1.0], [1798.0, 1.0], [1799.0, 1.0], [1800.0, 1.0], [1801.0, 1.0], [1802.0, 1.0], [1803.0, 1.0], [1804.0, 1.0], [1805.0, 1.0], [1806.0, 1.0], [1807.0, 1.0], [1808.0, 1.0], [1809.0, 1.0], [1810.0, 1.0], [1811.0, 1.0], [1812.0, 1.0], [1813.0, 1.0], [1814.0, 1.0], [1815.0, 1.0], [1816.0, 1.0], [1817.0, 1.0], [1818.0, 1.0], [1819.0, 1.0], [1820.0, 1.0], [1821.0, 1.0], [1822.0, 1.0], [1823.0, 1.0], [1824.0, 1.0], [1825.0, 1.0], [1826.0, 1.0], [1827.0, 1.0], [1828.0, 1.0], [1829.0, 1.0], [1830.0, 1.0], [1831.0, 1.0], [1832.0, 1.0], [1833.0, 1.0], [1834.0, 1.0], [1835.0, 1.0], [1836.0, 1.0], [1837.0, 1.0], [1838.0, 1.0], [1839.0, 1.0], [1840.0, 1.0], [1841.0, 1.0], [1842.0, 1.0], [1843.0, 1.0], [1844.0, 1.0], [1845.0, 1.0], [1846.0, 1.0], [1847.0, 1.0], [1848.0, 1.0], [1849.0, 1.0], [1850.0, 1.0], [1851.0, 1.0], [1852.0, 1.0], [1853.0, 1.0], [1854.0, 1.0], [1855.0, 1.0], [1856.0, 1.0], [1857.0, 1.0], [1858.0, 1.0], [1859.0, 1.0], [1860.0, 1.0], [1861.0, 1.0], [1862.0, 1.0], [1863.0, 1.0], [1864.0, 1.0], [1865.0, 1.0], [1866.0, 1.0], [1867.0, 1.0], [1868.0, 1.0], [1869.0, 1.0], [1870.0, 1.0], [1871.0, 1.0], [1872.0, 1.0], [1873.0, 1.0], [1874.0, 1.0], [1875.0, 1.0], [1876.0, 1.0], [1877.0, 1.0], [1878.0, 1.0], [1879.0, 1.0], [1880.0, 1.0], [1881.0, 1.0], [1882.0, 1.0], [1883.0, 1.0], [1884.0, 1.0], [1885.0, 1.0], [1886.0, 1.0], [1887.0, 1.0], [1888.0, 1.0], [1889.0, 1.0], [1890.0, 1.0], [1891.0, 1.0], [1892.0, 1.0], [1893.0, 1.0], [1894.0, 1.0], [1895.0, 1.0], [1896.0, 1.0], [1897.0, 1.0], [1898.0, 1.0], [1899.0, 1.0], [1900.0, 1.0], [1901.0, 1.0], [1902.0, 1.0], [1903.0, 1.0], [1904.0, 1.0], [1905.0, 1.0], [1906.0, 1.0], [1907.0, 1.0], [1908.0, 1.0], [1909.0, 1.0], [1910.0, 1.0], [1911.0, 1.0], [1912.0, 1.0], [1913.0, 1.0], [1914.0, 1.0], [1915.0, 1.0], [1916.0, 1.0], [1917.0, 1.0], [1918.0, 1.0], [1919.0, 1.0], [1920.0, 1.0], [1921.0, 1.0], [1922.0, 1.0], [1923.0, 1.0], [1924.0, 1.0], [1925.0, 1.0], [1926.0, 1.0], [1927.0, 1.0], [1928.0, 1.0], [1929.0, 1.0], [1930.0, 1.0], [1931.0, 1.0], [1932.0, 1.0], [1933.0, 1.0], [1934.0, 1.0], [1935.0, 1.0], [1936.0, 1.0], [1937.0, 1.0], [1938.0, 1.0], [1939.0, 1.0], [1940.0, 1.0], [1941.0, 1.0], [1942.0, 1.0], [1943.0, 1.0], [1944.0, 1.0], [1945.0, 1.0], [1946.0, 1.0], [1947.0, 1.0], [1948.0, 1.0], [1949.0, 1.0], [1950.0, 1.0], [1951.0, 1.0], [1952.0, 1.0], [1953.0, 1.0], [1954.0, 1.0], [1955.0, 1.0], [1956.0, 1.0], [1957.0, 1.0], [1958.0, 1.0], [1959.0, 1.0], [1960.0, 1.0], [1961.0, 1.0], [1962.0, 1.0], [1963.0, 1.0], [1964.0, 1.0], [1965.0, 1.0], [1966.0, 1.0], [1967.0, 1.0], [1968.0, 1.0], [1969.0, 1.0], [1970.0, 1.0], [1971.0, 1.0], [1972.0, 1.0], [1973.0, 1.0], [1974.0, 1.0], [1975.0, 1.0], [1976.0, 1.0], [1977.0, 1.0], [1978.0, 1.0], [1979.0, 1.0], [1980.0, 1.0], [1981.0, 1.0], [1982.0, 1.0], [1983.0, 1.0], [1984.0, 1.0], [1985.0, 1.0], [1986.0, 1.0], [1987.0, 1.0], [1988.0, 1.0], [1989.0, 1.0], [1990.0, 1.0], [1991.0, 1.0], [1992.0, 1.0], [1993.0, 1.0], [1994.0, 1.0], [1995.0, 1.0], [1996.0, 1.0], [1997.0, 1.0], [1998.0, 1.0], [1999.0, 1.0], [2000.0, 1.0], [2001.0, 1.0], [2002.0, 1.0], [2003.0, 1.0], [2004.0, 1.0], [2005.0, 1.0], [2006.0, 1.0], [2007.0, 1.0], [2008.0, 1.0], [2009.0, 1.0], [2010.0, 1.0], [2011.0, 1.0], [2012.0, 1.0], [2013.0, 1.0], [2014.0, 1.0], [2015.0, 1.0], [2016.0, 1.0], [2017.0, 1.0], [2018.0, 1.0], [2019.0, 1.0], [2020.0, 1.0], [2021.0, 1.0], [2022.0, 1.0], [2023.0, 1.0], [2024.0, 1.0], [2025.0, 1.0], [2026.0, 1.0], [2027.0, 1.0], [2028.0, 1.0], [2029.0, 1.0], [2030.0, 1.0], [2031.0, 1.0], [2032.0, 1.0], [2033.0, 1.0], [2034.0, 1.0], [2035.0, 1.0], [2036.0, 1.0], [2037.0, 1.0], [2038.0, 1.0], [2039.0, 1.0], [2040.0, 1.0], [2041.0, 1.0], [2042.0, 1.0], [2043.0, 1.0], [2044.0, 1.0], [2045.0, 1.0], [2046.0, 1.0], [2047.0, 1.0], [2048.0, 1.0], [2049.0, 1.0], [2050.0, 1.0], [2051.0, 1.0], [2052.0, 1.0], [2053.0, 1.0], [2054.0, 1.0], [2055.0, 1.0], [2056.0, 1.0], [2057.0, 1.0], [2058.0, 1.0], [2059.0, 1.0], [2060.0, 1.0], [2061.0, 1.0], [2062.0, 1.0], [2063.0, 1.0], [2064.0, 1.0], [2065.0, 1.0], [2066.0, 1.0], [2067.0, 1.0], [2068.0, 1.0], [2069.0, 1.0], [2070.0, 1.0], [2071.0, 1.0], [2072.0, 1.0], [2073.0, 1.0], [2074.0, 1.0], [2075.0, 1.0], [2076.0, 1.0], [2077.0, 1.0], [2078.0, 1.0], [2079.0, 1.0], [2080.0, 1.0], [2081.0, 1.0], [2082.0, 1.0], [2083.0, 1.0], [2084.0, 1.0], [2085.0, 1.0], [2086.0, 1.0], [2087.0, 1.0], [2088.0, 1.0], [2089.0, 1.0], [2090.0, 1.0], [2091.0, 1.0], [2092.0, 1.0], [2093.0, 1.0], [2094.0, 1.0], [2095.0, 1.0], [2096.0, 1.0], [2097.0, 1.0], [2098.0, 1.0], [2099.0, 1.0], [2100.0, 1.0], [2101.0, 1.0], [2102.0, 1.0], [2103.0, 1.0], [2104.0, 1.0], [2105.0, 1.0], [2106.0, 1.0], [2107.0, 1.0], [2108.0, 1.0], [2109.0, 1.0], [2110.0, 1.0], [2111.0, 1.0], [2112.0, 1.0], [2113.0, 1.0], [2114.0, 1.0], [2115.0, 1.0], [2116.0, 1.0], [2117.0, 1.0], [2118.0, 1.0], [2119.0, 1.0], [2120.0, 1.0], [2121.0, 1.0], [2122.0, 1.0], [2123.0, 1.0], [2124.0, 1.0], [2125.0, 1.0], [2126.0, 1.0], [2127.0, 1.0], [2128.0, 1.0], [2129.0, 1.0], [2130.0, 1.0], [2131.0, 1.0], [2132.0, 1.0], [2133.0, 1.0], [2134.0, 1.0], [2135.0, 1.0], [2136.0, 1.0], [2137.0, 1.0], [2138.0, 1.0], [2139.0, 1.0], [2140.0, 1.0], [2141.0, 1.0], [2142.0, 1.0], [2143.0, 1.0], [2144.0, 1.0], [2145.0, 1.0], [2146.0, 1.0], [2147.0, 1.0], [2148.0, 1.0], [2149.0, 1.0], [2150.0, 1.0], [2151.0, 1.0], [2152.0, 1.0], [2153.0, 1.0], [2154.0, 1.0], [2155.0, 1.0], [2156.0, 1.0], [2157.0, 1.0], [2158.0, 1.0], [2159.0, 1.0], [2160.0, 1.0], [2161.0, 1.0], [2162.0, 1.0], [2163.0, 1.0], [2164.0, 1.0], [2165.0, 1.0], [2166.0, 1.0], [2167.0, 1.0], [2168.0, 1.0], [2169.0, 1.0], [2170.0, 1.0], [2171.0, 1.0], [2172.0, 1.0], [2173.0, 1.0], [2174.0, 1.0], [2175.0, 1.0], [2176.0, 1.0], [2177.0, 1.0], [2178.0, 1.0], [2179.0, 1.0], [2180.0, 1.0], [2181.0, 1.0], [2182.0, 1.0], [2183.0, 1.0], [2184.0, 1.0], [2185.0, 1.0], [2186.0, 1.0], [2187.0, 1.0], [2188.0, 1.0], [2189.0, 1.0], [2190.0, 1.0], [2191.0, 1.0], [2192.0, 1.0], [2193.0, 1.0], [2194.0, 1.0], [2195.0, 1.0], [2196.0, 1.0], [2197.0, 1.0], [2198.0, 1.0], [2199.0, 1.0], [2200.0, 1.0], [2201.0, 1.0], [2202.0, 1.0], [2203.0, 1.0], [2204.0, 1.0], [2205.0, 1.0], [2206.0, 1.0], [2207.0, 1.0], [2208.0, 1.0], [2209.0, 1.0], [2210.0, 1.0], [2211.0, 1.0], [2212.0, 1.0], [2213.0, 1.0], [2214.0, 1.0], [2215.0, 1.0], [2216.0, 1.0], [2217.0, 1.0], [2218.0, 1.0], [2219.0, 1.0], [2220.0, 1.0], [2221.0, 1.0], [2222.0, 1.0], [2223.0, 1.0], [2224.0, 1.0], [2225.0, 1.0], [2226.0, 1.0], [2227.0, 1.0], [2228.0, 1.0], [2229.0, 1.0], [2230.0, 1.0], [2231.0, 1.0], [2232.0, 1.0], [2233.0, 1.0], [2234.0, 1.0], [2235.0, 1.0], [2236.0, 1.0], [2237.0, 1.0], [2238.0, 1.0], [2239.0, 1.0], [2240.0, 1.0], [2241.0, 1.0], [2242.0, 1.0], [2243.0, 1.0], [2244.0, 1.0], [2245.0, 1.0], [2246.0, 1.0], [2247.0, 1.0], [2248.0, 1.0], [2249.0, 1.0], [2250.0, 1.0], [2251.0, 1.0], [2252.0, 1.0], [2253.0, 1.0], [2254.0, 1.0], [2255.0, 1.0], [2256.0, 1.0], [2257.0, 1.0], [2258.0, 1.0], [2259.0, 1.0], [2260.0, 1.0], [2261.0, 1.0], [2262.0, 1.0], [2263.0, 1.0], [2264.0, 1.0], [2265.0, 1.0], [2266.0, 1.0], [2267.0, 1.0], [2268.0, 1.0], [2269.0, 1.0], [2270.0, 1.0], [2271.0, 1.0], [2272.0, 1.0], [2273.0, 1.0], [2274.0, 1.0], [2275.0, 1.0], [2276.0, 1.0], [2277.0, 1.0], [2278.0, 1.0], [2279.0, 1.0], [2280.0, 1.0], [2281.0, 1.0], [2282.0, 1.0], [2283.0, 1.0], [2284.0, 1.0], [2285.0, 1.0], [2286.0, 1.0], [2287.0, 1.0], [2288.0, 1.0], [2289.0, 1.0], [2290.0, 1.0], [2291.0, 1.0], [2292.0, 1.0], [2293.0, 1.0], [2294.0, 1.0], [2295.0, 1.0], [2296.0, 1.0], [2297.0, 1.0], [2298.0, 1.0], [2299.0, 1.0], [2300.0, 1.0], [2301.0, 1.0], [2302.0, 1.0], [2303.0, 1.0], [2304.0, 1.0], [2305.0, 1.0], [2306.0, 1.0], [2307.0, 1.0], [2308.0, 1.0], [2309.0, 1.0], [2310.0, 1.0], [2311.0, 1.0], [2312.0, 1.0], [2313.0, 1.0], [2314.0, 1.0], [2315.0, 1.0], [2316.0, 1.0], [2317.0, 1.0], [2318.0, 1.0], [2319.0, 1.0], [2320.0, 1.0], [2321.0, 1.0], [2322.0, 1.0], [2323.0, 1.0], [2324.0, 1.0], [2325.0, 1.0], [2326.0, 1.0], [2327.0, 1.0], [2328.0, 1.0], [2329.0, 1.0], [2330.0, 1.0], [2331.0, 1.0], [2332.0, 1.0], [2333.0, 1.0], [2334.0, 1.0], [2335.0, 1.0], [2336.0, 1.0], [2337.0, 1.0], [2338.0, 1.0], [2339.0, 1.0], [2340.0, 1.0], [2341.0, 1.0], [2342.0, 1.0], [2343.0, 1.0], [2344.0, 1.0], [2345.0, 1.0], [2346.0, 1.0], [2347.0, 1.0], [2348.0, 1.0], [2349.0, 1.0], [2350.0, 1.0], [2351.0, 1.0], [2352.0, 1.0], [2353.0, 1.0], [2354.0, 1.0], [2355.0, 1.0], [2356.0, 1.0], [2357.0, 1.0], [2358.0, 1.0], [2359.0, 1.0], [2360.0, 1.0], [2361.0, 1.0], [2362.0, 1.0], [2363.0, 1.0], [2364.0, 1.0], [2365.0, 1.0], [2366.0, 1.0], [2367.0, 1.0], [2368.0, 1.0], [2369.0, 1.0], [2370.0, 1.0], [2371.0, 1.0], [2372.0, 1.0], [2373.0, 1.0], [2374.0, 1.0], [2375.0, 1.0], [2376.0, 1.0], [2377.0, 1.0], [2378.0, 1.0], [2379.0, 1.0], [2380.0, 1.0], [2381.0, 1.0], [2382.0, 1.0], [2383.0, 1.0], [2384.0, 1.0], [2385.0, 1.0], [2386.0, 1.0], [2387.0, 1.0], [2388.0, 1.0], [2389.0, 1.0], [2390.0, 1.0], [2391.0, 1.0], [2392.0, 1.0], [2393.0, 1.0], [2394.0, 1.0], [2395.0, 1.0], [2396.0, 1.0], [2397.0, 1.0], [2398.0, 1.0], [2399.0, 1.0], [2400.0, 1.0], [2401.0, 1.0], [2402.0, 1.0], [2403.0, 1.0], [2404.0, 1.0], [2405.0, 1.0], [2406.0, 1.0], [2407.0, 1.0], [2408.0, 1.0], [2409.0, 1.0], [2410.0, 1.0], [2411.0, 1.0], [2412.0, 1.0], [2413.0, 1.0], [2414.0, 1.0], [2415.0, 1.0], [2416.0, 1.0], [2417.0, 1.0], [2418.0, 1.0], [2419.0, 1.0], [2420.0, 1.0], [2421.0, 1.0], [2422.0, 1.0], [2423.0, 1.0], [2424.0, 1.0], [2425.0, 1.0], [2426.0, 1.0], [2427.0, 1.0], [2428.0, 1.0], [2429.0, 1.0], [2430.0, 1.0], [2431.0, 1.0], [2432.0, 1.0], [2433.0, 1.0], [2434.0, 1.0], [2435.0, 1.0], [2436.0, 1.0], [2437.0, 1.0], [2438.0, 1.0], [2439.0, 1.0], [2440.0, 1.0], [2441.0, 1.0], [2442.0, 1.0], [2443.0, 1.0], [2444.0, 1.0], [2445.0, 1.0], [2446.0, 1.0], [2447.0, 1.0], [2448.0, 1.0], [2449.0, 1.0], [2450.0, 1.0], [2451.0, 1.0], [2452.0, 1.0], [2453.0, 1.0], [2454.0, 1.0], [2455.0, 1.0], [2456.0, 1.0], [2457.0, 1.0], [2458.0, 1.0], [2459.0, 1.0], [2460.0, 1.0], [2461.0, 1.0], [2462.0, 1.0], [2463.0, 1.0], [2464.0, 1.0], [2465.0, 1.0], [2466.0, 1.0], [2467.0, 1.0], [2468.0, 1.0], [2469.0, 1.0], [2470.0, 1.0], [2471.0, 1.0], [2472.0, 1.0], [2473.0, 1.0], [2474.0, 1.0], [2475.0, 1.0], [2476.0, 1.0], [2477.0, 1.0], [2478.0, 1.0], [2479.0, 1.0], [2480.0, 1.0], [2481.0, 1.0], [2482.0, 1.0], [2483.0, 1.0], [2484.0, 1.0], [2485.0, 1.0], [2486.0, 1.0], [2487.0, 1.0], [2488.0, 1.0], [2489.0, 1.0], [2490.0, 1.0], [2491.0, 1.0], [2492.0, 1.0], [2493.0, 1.0], [2494.0, 1.0], [2495.0, 1.0], [2496.0, 1.0], [2497.0, 1.0], [2498.0, 1.0], [2499.0, 1.0], [2500.0, 1.0], [2501.0, 1.0], [2502.0, 1.0], [2503.0, 1.0], [2504.0, 1.0], [2505.0, 1.0], [2506.0, 1.0], [2507.0, 1.0], [2508.0, 1.0], [2509.0, 1.0], [2510.0, 1.0], [2511.0, 1.0], [2512.0, 1.0], [2513.0, 1.0], [2514.0, 1.0], [2515.0, 1.0], [2516.0, 1.0], [2517.0, 1.0], [2518.0, 1.0], [2519.0, 1.0], [2520.0, 1.0], [2521.0, 1.0], [2522.0, 1.0], [2523.0, 1.0], [2524.0, 1.0], [2525.0, 1.0], [2526.0, 1.0], [2527.0, 1.0], [2528.0, 1.0], [2529.0, 1.0], [2530.0, 1.0], [2531.0, 1.0], [2532.0, 1.0], [2533.0, 1.0], [2534.0, 1.0], [2535.0, 1.0], [2536.0, 1.0], [2537.0, 1.0], [2538.0, 1.0], [2539.0, 1.0], [2540.0, 1.0], [2541.0, 1.0], [2542.0, 1.0], [2543.0, 1.0], [2544.0, 1.0], [2545.0, 1.0], [2546.0, 1.0], [2547.0, 1.0], [2548.0, 1.0], [2549.0, 1.0], [2550.0, 1.0], [2551.0, 1.0], [2552.0, 1.0], [2553.0, 1.0], [2554.0, 1.0], [2555.0, 1.0], [2556.0, 1.0]]}, \"height\": 320.0});\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & mpld3\n",
       "    mpld3_load_lib(\"https://mpld3.github.io/js/d3.v3.min.js\", function(){\n",
       "         mpld3_load_lib(\"https://mpld3.github.io/js/mpld3.v0.2.js\", function(){\n",
       "                 \n",
       "                 mpld3.draw_figure(\"fig_el5466445855727843659457194\", {\"width\": 480.0, \"plugins\": [{\"type\": \"reset\"}, {\"button\": true, \"enabled\": false, \"type\": \"zoom\"}, {\"button\": true, \"enabled\": false, \"type\": \"boxzoom\"}], \"id\": \"el546644585572784\", \"axes\": [{\"lines\": [{\"coordinates\": \"data\", \"xindex\": 0, \"dasharray\": \"10,0\", \"color\": \"#0000FF\", \"id\": \"el546644586961272\", \"alpha\": 1, \"zorder\": 2, \"data\": \"data01\", \"yindex\": 1, \"linewidth\": 1.0}], \"images\": [], \"xdomain\": [0.0, 3000.0], \"ylim\": [0.0, 1400.0], \"axesbg\": \"#FFFFFF\", \"yscale\": \"linear\", \"id\": \"el546644587386976\", \"zoomable\": true, \"xlim\": [0.0, 3000.0], \"collections\": [], \"ydomain\": [0.0, 1400.0], \"axesbgalpha\": null, \"markers\": [], \"sharex\": [], \"paths\": [], \"xscale\": \"linear\", \"axes\": [{\"grid\": {\"gridOn\": false}, \"tickformat\": null, \"position\": \"bottom\", \"tickvalues\": null, \"nticks\": 7, \"scale\": \"linear\", \"fontsize\": 10.0}, {\"grid\": {\"gridOn\": false}, \"tickformat\": null, \"position\": \"left\", \"tickvalues\": null, \"nticks\": 8, \"scale\": \"linear\", \"fontsize\": 10.0}], \"texts\": [], \"bbox\": [0.125, 0.125, 0.775, 0.775], \"sharey\": []}], \"data\": {\"data01\": [[1.0, 1255.0], [2.0, 1255.0], [3.0, 509.0], [4.0, 390.0], [5.0, 265.0], [6.0, 243.0], [7.0, 208.0], [8.0, 187.0], [9.0, 146.0], [10.0, 142.0], [11.0, 136.0], [12.0, 113.0], [13.0, 109.0], [14.0, 103.0], [15.0, 98.0], [16.0, 94.0], [17.0, 88.0], [18.0, 82.0], [19.0, 71.0], [20.0, 67.0], [21.0, 65.0], [22.0, 61.0], [23.0, 60.0], [24.0, 58.0], [25.0, 58.0], [26.0, 57.0], [27.0, 56.0], [28.0, 55.0], [29.0, 55.0], [30.0, 53.0], [31.0, 50.0], [32.0, 47.0], [33.0, 45.0], [34.0, 45.0], [35.0, 45.0], [36.0, 44.0], [37.0, 42.0], [38.0, 42.0], [39.0, 41.0], [40.0, 40.0], [41.0, 40.0], [42.0, 40.0], [43.0, 40.0], [44.0, 36.0], [45.0, 34.0], [46.0, 34.0], [47.0, 32.0], [48.0, 32.0], [49.0, 30.0], [50.0, 29.0], [51.0, 29.0], [52.0, 28.0], [53.0, 27.0], [54.0, 26.0], [55.0, 26.0], [56.0, 26.0], [57.0, 25.0], [58.0, 25.0], [59.0, 25.0], [60.0, 25.0], [61.0, 24.0], [62.0, 24.0], [63.0, 23.0], [64.0, 22.0], [65.0, 22.0], [66.0, 21.0], [67.0, 21.0], [68.0, 21.0], [69.0, 20.0], [70.0, 19.0], [71.0, 19.0], [72.0, 19.0], [73.0, 18.0], [74.0, 18.0], [75.0, 18.0], [76.0, 18.0], [77.0, 18.0], [78.0, 17.0], [79.0, 17.0], [80.0, 17.0], [81.0, 17.0], [82.0, 17.0], [83.0, 17.0], [84.0, 16.0], [85.0, 16.0], [86.0, 16.0], [87.0, 16.0], [88.0, 16.0], [89.0, 16.0], [90.0, 16.0], [91.0, 16.0], [92.0, 16.0], [93.0, 16.0], [94.0, 15.0], [95.0, 15.0], [96.0, 15.0], [97.0, 15.0], [98.0, 14.0], [99.0, 14.0], [100.0, 14.0], [101.0, 14.0], [102.0, 14.0], [103.0, 14.0], [104.0, 14.0], [105.0, 13.0], [106.0, 13.0], [107.0, 13.0], [108.0, 13.0], [109.0, 13.0], [110.0, 13.0], [111.0, 13.0], [112.0, 13.0], [113.0, 13.0], [114.0, 12.0], [115.0, 12.0], [116.0, 12.0], [117.0, 12.0], [118.0, 12.0], [119.0, 11.0], [120.0, 11.0], [121.0, 11.0], [122.0, 11.0], [123.0, 11.0], [124.0, 11.0], [125.0, 11.0], [126.0, 11.0], [127.0, 11.0], [128.0, 11.0], [129.0, 11.0], [130.0, 11.0], [131.0, 11.0], [132.0, 11.0], [133.0, 11.0], [134.0, 11.0], [135.0, 11.0], [136.0, 11.0], [137.0, 10.0], [138.0, 10.0], [139.0, 10.0], [140.0, 10.0], [141.0, 10.0], [142.0, 10.0], [143.0, 10.0], [144.0, 10.0], [145.0, 10.0], [146.0, 10.0], [147.0, 10.0], [148.0, 10.0], [149.0, 10.0], [150.0, 10.0], [151.0, 10.0], [152.0, 9.0], [153.0, 9.0], [154.0, 9.0], [155.0, 9.0], [156.0, 9.0], [157.0, 9.0], [158.0, 9.0], [159.0, 9.0], [160.0, 9.0], [161.0, 9.0], [162.0, 9.0], [163.0, 9.0], [164.0, 9.0], [165.0, 9.0], [166.0, 9.0], [167.0, 9.0], [168.0, 9.0], [169.0, 9.0], [170.0, 8.0], [171.0, 8.0], [172.0, 8.0], [173.0, 8.0], [174.0, 8.0], [175.0, 8.0], [176.0, 8.0], [177.0, 8.0], [178.0, 8.0], [179.0, 8.0], [180.0, 8.0], [181.0, 8.0], [182.0, 8.0], [183.0, 8.0], [184.0, 8.0], [185.0, 8.0], [186.0, 8.0], [187.0, 8.0], [188.0, 8.0], [189.0, 7.0], [190.0, 7.0], [191.0, 7.0], [192.0, 7.0], [193.0, 7.0], [194.0, 7.0], [195.0, 7.0], [196.0, 7.0], [197.0, 7.0], [198.0, 7.0], [199.0, 7.0], [200.0, 7.0], [201.0, 7.0], [202.0, 7.0], [203.0, 7.0], [204.0, 7.0], [205.0, 7.0], [206.0, 7.0], [207.0, 7.0], [208.0, 7.0], [209.0, 7.0], [210.0, 7.0], [211.0, 7.0], [212.0, 7.0], [213.0, 7.0], [214.0, 7.0], [215.0, 7.0], [216.0, 7.0], [217.0, 7.0], [218.0, 7.0], [219.0, 7.0], [220.0, 6.0], [221.0, 6.0], [222.0, 6.0], [223.0, 6.0], [224.0, 6.0], [225.0, 6.0], [226.0, 6.0], [227.0, 6.0], [228.0, 6.0], [229.0, 6.0], [230.0, 6.0], [231.0, 6.0], [232.0, 6.0], [233.0, 6.0], [234.0, 6.0], [235.0, 6.0], [236.0, 6.0], [237.0, 6.0], [238.0, 6.0], [239.0, 6.0], [240.0, 6.0], [241.0, 6.0], [242.0, 6.0], [243.0, 6.0], [244.0, 6.0], [245.0, 6.0], [246.0, 6.0], [247.0, 6.0], [248.0, 6.0], [249.0, 6.0], [250.0, 6.0], [251.0, 6.0], [252.0, 6.0], [253.0, 6.0], [254.0, 6.0], [255.0, 6.0], [256.0, 6.0], [257.0, 6.0], [258.0, 5.0], [259.0, 5.0], [260.0, 5.0], [261.0, 5.0], [262.0, 5.0], [263.0, 5.0], [264.0, 5.0], [265.0, 5.0], [266.0, 5.0], [267.0, 5.0], [268.0, 5.0], [269.0, 5.0], [270.0, 5.0], [271.0, 5.0], [272.0, 5.0], [273.0, 5.0], [274.0, 5.0], [275.0, 5.0], [276.0, 5.0], [277.0, 5.0], [278.0, 5.0], [279.0, 5.0], [280.0, 5.0], [281.0, 5.0], [282.0, 5.0], [283.0, 5.0], [284.0, 5.0], [285.0, 5.0], [286.0, 5.0], [287.0, 5.0], [288.0, 5.0], [289.0, 5.0], [290.0, 5.0], [291.0, 5.0], [292.0, 5.0], [293.0, 5.0], [294.0, 5.0], [295.0, 5.0], [296.0, 5.0], [297.0, 5.0], [298.0, 5.0], [299.0, 5.0], [300.0, 5.0], [301.0, 5.0], [302.0, 5.0], [303.0, 5.0], [304.0, 5.0], [305.0, 5.0], [306.0, 5.0], [307.0, 5.0], [308.0, 5.0], [309.0, 5.0], [310.0, 5.0], [311.0, 5.0], [312.0, 5.0], [313.0, 5.0], [314.0, 5.0], [315.0, 5.0], [316.0, 5.0], [317.0, 5.0], [318.0, 4.0], [319.0, 4.0], [320.0, 4.0], [321.0, 4.0], [322.0, 4.0], [323.0, 4.0], [324.0, 4.0], [325.0, 4.0], [326.0, 4.0], [327.0, 4.0], [328.0, 4.0], [329.0, 4.0], [330.0, 4.0], [331.0, 4.0], [332.0, 4.0], [333.0, 4.0], [334.0, 4.0], [335.0, 4.0], [336.0, 4.0], [337.0, 4.0], [338.0, 4.0], [339.0, 4.0], [340.0, 4.0], [341.0, 4.0], [342.0, 4.0], [343.0, 4.0], [344.0, 4.0], [345.0, 4.0], [346.0, 4.0], [347.0, 4.0], [348.0, 4.0], [349.0, 4.0], [350.0, 4.0], [351.0, 4.0], [352.0, 4.0], [353.0, 4.0], [354.0, 4.0], [355.0, 4.0], [356.0, 4.0], [357.0, 4.0], [358.0, 4.0], [359.0, 4.0], [360.0, 4.0], [361.0, 4.0], [362.0, 4.0], [363.0, 4.0], [364.0, 4.0], [365.0, 4.0], [366.0, 4.0], [367.0, 4.0], [368.0, 4.0], [369.0, 4.0], [370.0, 4.0], [371.0, 4.0], [372.0, 4.0], [373.0, 4.0], [374.0, 4.0], [375.0, 4.0], [376.0, 4.0], [377.0, 4.0], [378.0, 4.0], [379.0, 4.0], [380.0, 4.0], [381.0, 4.0], [382.0, 4.0], [383.0, 4.0], [384.0, 4.0], [385.0, 4.0], [386.0, 4.0], [387.0, 4.0], [388.0, 4.0], [389.0, 4.0], [390.0, 4.0], [391.0, 4.0], [392.0, 4.0], [393.0, 4.0], [394.0, 4.0], [395.0, 4.0], [396.0, 4.0], [397.0, 4.0], [398.0, 4.0], [399.0, 3.0], [400.0, 3.0], [401.0, 3.0], [402.0, 3.0], [403.0, 3.0], [404.0, 3.0], [405.0, 3.0], [406.0, 3.0], [407.0, 3.0], [408.0, 3.0], [409.0, 3.0], [410.0, 3.0], [411.0, 3.0], [412.0, 3.0], [413.0, 3.0], [414.0, 3.0], [415.0, 3.0], [416.0, 3.0], [417.0, 3.0], [418.0, 3.0], [419.0, 3.0], [420.0, 3.0], [421.0, 3.0], [422.0, 3.0], [423.0, 3.0], [424.0, 3.0], [425.0, 3.0], [426.0, 3.0], [427.0, 3.0], [428.0, 3.0], [429.0, 3.0], [430.0, 3.0], [431.0, 3.0], [432.0, 3.0], [433.0, 3.0], [434.0, 3.0], [435.0, 3.0], [436.0, 3.0], [437.0, 3.0], [438.0, 3.0], [439.0, 3.0], [440.0, 3.0], [441.0, 3.0], [442.0, 3.0], [443.0, 3.0], [444.0, 3.0], [445.0, 3.0], [446.0, 3.0], [447.0, 3.0], [448.0, 3.0], [449.0, 3.0], [450.0, 3.0], [451.0, 3.0], [452.0, 3.0], [453.0, 3.0], [454.0, 3.0], [455.0, 3.0], [456.0, 3.0], [457.0, 3.0], [458.0, 3.0], [459.0, 3.0], [460.0, 3.0], [461.0, 3.0], [462.0, 3.0], [463.0, 3.0], [464.0, 3.0], [465.0, 3.0], [466.0, 3.0], [467.0, 3.0], [468.0, 3.0], [469.0, 3.0], [470.0, 3.0], [471.0, 3.0], [472.0, 3.0], [473.0, 3.0], [474.0, 3.0], [475.0, 3.0], [476.0, 3.0], [477.0, 3.0], [478.0, 3.0], [479.0, 3.0], [480.0, 3.0], [481.0, 3.0], [482.0, 3.0], [483.0, 3.0], [484.0, 3.0], [485.0, 3.0], [486.0, 3.0], [487.0, 3.0], [488.0, 3.0], [489.0, 3.0], [490.0, 3.0], [491.0, 3.0], [492.0, 3.0], [493.0, 3.0], [494.0, 3.0], [495.0, 3.0], [496.0, 3.0], [497.0, 3.0], [498.0, 3.0], [499.0, 3.0], [500.0, 3.0], [501.0, 3.0], [502.0, 3.0], [503.0, 3.0], [504.0, 3.0], [505.0, 3.0], [506.0, 3.0], [507.0, 3.0], [508.0, 3.0], [509.0, 3.0], [510.0, 3.0], [511.0, 3.0], [512.0, 3.0], [513.0, 3.0], [514.0, 3.0], [515.0, 3.0], [516.0, 3.0], [517.0, 3.0], [518.0, 3.0], [519.0, 3.0], [520.0, 3.0], [521.0, 3.0], [522.0, 3.0], [523.0, 3.0], [524.0, 3.0], [525.0, 3.0], [526.0, 3.0], [527.0, 3.0], [528.0, 3.0], [529.0, 3.0], [530.0, 3.0], [531.0, 3.0], [532.0, 3.0], [533.0, 3.0], [534.0, 3.0], [535.0, 3.0], [536.0, 3.0], [537.0, 3.0], [538.0, 3.0], [539.0, 3.0], [540.0, 3.0], [541.0, 3.0], [542.0, 3.0], [543.0, 3.0], [544.0, 3.0], [545.0, 3.0], [546.0, 3.0], [547.0, 3.0], [548.0, 3.0], [549.0, 3.0], [550.0, 3.0], [551.0, 3.0], [552.0, 3.0], [553.0, 3.0], [554.0, 3.0], [555.0, 3.0], [556.0, 3.0], [557.0, 3.0], [558.0, 3.0], [559.0, 3.0], [560.0, 3.0], [561.0, 3.0], [562.0, 3.0], [563.0, 3.0], [564.0, 3.0], [565.0, 3.0], [566.0, 2.0], [567.0, 2.0], [568.0, 2.0], [569.0, 2.0], [570.0, 2.0], [571.0, 2.0], [572.0, 2.0], [573.0, 2.0], [574.0, 2.0], [575.0, 2.0], [576.0, 2.0], [577.0, 2.0], [578.0, 2.0], [579.0, 2.0], [580.0, 2.0], [581.0, 2.0], [582.0, 2.0], [583.0, 2.0], [584.0, 2.0], [585.0, 2.0], [586.0, 2.0], [587.0, 2.0], [588.0, 2.0], [589.0, 2.0], [590.0, 2.0], [591.0, 2.0], [592.0, 2.0], [593.0, 2.0], [594.0, 2.0], [595.0, 2.0], [596.0, 2.0], [597.0, 2.0], [598.0, 2.0], [599.0, 2.0], [600.0, 2.0], [601.0, 2.0], [602.0, 2.0], [603.0, 2.0], [604.0, 2.0], [605.0, 2.0], [606.0, 2.0], [607.0, 2.0], [608.0, 2.0], [609.0, 2.0], [610.0, 2.0], [611.0, 2.0], [612.0, 2.0], [613.0, 2.0], [614.0, 2.0], [615.0, 2.0], [616.0, 2.0], [617.0, 2.0], [618.0, 2.0], [619.0, 2.0], [620.0, 2.0], [621.0, 2.0], [622.0, 2.0], [623.0, 2.0], [624.0, 2.0], [625.0, 2.0], [626.0, 2.0], [627.0, 2.0], [628.0, 2.0], [629.0, 2.0], [630.0, 2.0], [631.0, 2.0], [632.0, 2.0], [633.0, 2.0], [634.0, 2.0], [635.0, 2.0], [636.0, 2.0], [637.0, 2.0], [638.0, 2.0], [639.0, 2.0], [640.0, 2.0], [641.0, 2.0], [642.0, 2.0], [643.0, 2.0], [644.0, 2.0], [645.0, 2.0], [646.0, 2.0], [647.0, 2.0], [648.0, 2.0], [649.0, 2.0], [650.0, 2.0], [651.0, 2.0], [652.0, 2.0], [653.0, 2.0], [654.0, 2.0], [655.0, 2.0], [656.0, 2.0], [657.0, 2.0], [658.0, 2.0], [659.0, 2.0], [660.0, 2.0], [661.0, 2.0], [662.0, 2.0], [663.0, 2.0], [664.0, 2.0], [665.0, 2.0], [666.0, 2.0], [667.0, 2.0], [668.0, 2.0], [669.0, 2.0], [670.0, 2.0], [671.0, 2.0], [672.0, 2.0], [673.0, 2.0], [674.0, 2.0], [675.0, 2.0], [676.0, 2.0], [677.0, 2.0], [678.0, 2.0], [679.0, 2.0], [680.0, 2.0], [681.0, 2.0], [682.0, 2.0], [683.0, 2.0], [684.0, 2.0], [685.0, 2.0], [686.0, 2.0], [687.0, 2.0], [688.0, 2.0], [689.0, 2.0], [690.0, 2.0], [691.0, 2.0], [692.0, 2.0], [693.0, 2.0], [694.0, 2.0], [695.0, 2.0], [696.0, 2.0], [697.0, 2.0], [698.0, 2.0], [699.0, 2.0], [700.0, 2.0], [701.0, 2.0], [702.0, 2.0], [703.0, 2.0], [704.0, 2.0], [705.0, 2.0], [706.0, 2.0], [707.0, 2.0], [708.0, 2.0], [709.0, 2.0], [710.0, 2.0], [711.0, 2.0], [712.0, 2.0], [713.0, 2.0], [714.0, 2.0], [715.0, 2.0], [716.0, 2.0], [717.0, 2.0], [718.0, 2.0], [719.0, 2.0], [720.0, 2.0], [721.0, 2.0], [722.0, 2.0], [723.0, 2.0], [724.0, 2.0], [725.0, 2.0], [726.0, 2.0], [727.0, 2.0], [728.0, 2.0], [729.0, 2.0], [730.0, 2.0], [731.0, 2.0], [732.0, 2.0], [733.0, 2.0], [734.0, 2.0], [735.0, 2.0], [736.0, 2.0], [737.0, 2.0], [738.0, 2.0], [739.0, 2.0], [740.0, 2.0], [741.0, 2.0], [742.0, 2.0], [743.0, 2.0], [744.0, 2.0], [745.0, 2.0], [746.0, 2.0], [747.0, 2.0], [748.0, 2.0], [749.0, 2.0], [750.0, 2.0], [751.0, 2.0], [752.0, 2.0], [753.0, 2.0], [754.0, 2.0], [755.0, 2.0], [756.0, 2.0], [757.0, 2.0], [758.0, 2.0], [759.0, 2.0], [760.0, 2.0], [761.0, 2.0], [762.0, 2.0], [763.0, 2.0], [764.0, 2.0], [765.0, 2.0], [766.0, 2.0], [767.0, 2.0], [768.0, 2.0], [769.0, 2.0], [770.0, 2.0], [771.0, 2.0], [772.0, 2.0], [773.0, 2.0], [774.0, 2.0], [775.0, 2.0], [776.0, 2.0], [777.0, 2.0], [778.0, 2.0], [779.0, 2.0], [780.0, 2.0], [781.0, 2.0], [782.0, 2.0], [783.0, 2.0], [784.0, 2.0], [785.0, 2.0], [786.0, 2.0], [787.0, 2.0], [788.0, 2.0], [789.0, 2.0], [790.0, 2.0], [791.0, 2.0], [792.0, 2.0], [793.0, 2.0], [794.0, 2.0], [795.0, 2.0], [796.0, 2.0], [797.0, 2.0], [798.0, 2.0], [799.0, 2.0], [800.0, 2.0], [801.0, 2.0], [802.0, 2.0], [803.0, 2.0], [804.0, 2.0], [805.0, 2.0], [806.0, 2.0], [807.0, 2.0], [808.0, 2.0], [809.0, 2.0], [810.0, 2.0], [811.0, 2.0], [812.0, 2.0], [813.0, 2.0], [814.0, 2.0], [815.0, 2.0], [816.0, 2.0], [817.0, 2.0], [818.0, 2.0], [819.0, 2.0], [820.0, 2.0], [821.0, 2.0], [822.0, 2.0], [823.0, 2.0], [824.0, 2.0], [825.0, 2.0], [826.0, 2.0], [827.0, 2.0], [828.0, 2.0], [829.0, 2.0], [830.0, 2.0], [831.0, 2.0], [832.0, 2.0], [833.0, 2.0], [834.0, 2.0], [835.0, 2.0], [836.0, 2.0], [837.0, 2.0], [838.0, 2.0], [839.0, 2.0], [840.0, 2.0], [841.0, 2.0], [842.0, 2.0], [843.0, 2.0], [844.0, 2.0], [845.0, 2.0], [846.0, 2.0], [847.0, 2.0], [848.0, 2.0], [849.0, 2.0], [850.0, 2.0], [851.0, 2.0], [852.0, 2.0], [853.0, 2.0], [854.0, 2.0], [855.0, 2.0], [856.0, 2.0], [857.0, 2.0], [858.0, 2.0], [859.0, 2.0], [860.0, 2.0], [861.0, 2.0], [862.0, 2.0], [863.0, 2.0], [864.0, 2.0], [865.0, 2.0], [866.0, 2.0], [867.0, 2.0], [868.0, 2.0], [869.0, 2.0], [870.0, 2.0], [871.0, 2.0], [872.0, 2.0], [873.0, 2.0], [874.0, 2.0], [875.0, 2.0], [876.0, 2.0], [877.0, 2.0], [878.0, 2.0], [879.0, 2.0], [880.0, 2.0], [881.0, 2.0], [882.0, 2.0], [883.0, 2.0], [884.0, 2.0], [885.0, 2.0], [886.0, 2.0], [887.0, 2.0], [888.0, 2.0], [889.0, 2.0], [890.0, 2.0], [891.0, 2.0], [892.0, 2.0], [893.0, 2.0], [894.0, 2.0], [895.0, 2.0], [896.0, 2.0], [897.0, 2.0], [898.0, 2.0], [899.0, 2.0], [900.0, 2.0], [901.0, 2.0], [902.0, 2.0], [903.0, 2.0], [904.0, 2.0], [905.0, 2.0], [906.0, 2.0], [907.0, 2.0], [908.0, 2.0], [909.0, 2.0], [910.0, 2.0], [911.0, 2.0], [912.0, 2.0], [913.0, 2.0], [914.0, 2.0], [915.0, 2.0], [916.0, 2.0], [917.0, 2.0], [918.0, 2.0], [919.0, 2.0], [920.0, 2.0], [921.0, 2.0], [922.0, 2.0], [923.0, 2.0], [924.0, 2.0], [925.0, 2.0], [926.0, 2.0], [927.0, 2.0], [928.0, 1.0], [929.0, 1.0], [930.0, 1.0], [931.0, 1.0], [932.0, 1.0], [933.0, 1.0], [934.0, 1.0], [935.0, 1.0], [936.0, 1.0], [937.0, 1.0], [938.0, 1.0], [939.0, 1.0], [940.0, 1.0], [941.0, 1.0], [942.0, 1.0], [943.0, 1.0], [944.0, 1.0], [945.0, 1.0], [946.0, 1.0], [947.0, 1.0], [948.0, 1.0], [949.0, 1.0], [950.0, 1.0], [951.0, 1.0], [952.0, 1.0], [953.0, 1.0], [954.0, 1.0], [955.0, 1.0], [956.0, 1.0], [957.0, 1.0], [958.0, 1.0], [959.0, 1.0], [960.0, 1.0], [961.0, 1.0], [962.0, 1.0], [963.0, 1.0], [964.0, 1.0], [965.0, 1.0], [966.0, 1.0], [967.0, 1.0], [968.0, 1.0], [969.0, 1.0], [970.0, 1.0], [971.0, 1.0], [972.0, 1.0], [973.0, 1.0], [974.0, 1.0], [975.0, 1.0], [976.0, 1.0], [977.0, 1.0], [978.0, 1.0], [979.0, 1.0], [980.0, 1.0], [981.0, 1.0], [982.0, 1.0], [983.0, 1.0], [984.0, 1.0], [985.0, 1.0], [986.0, 1.0], [987.0, 1.0], [988.0, 1.0], [989.0, 1.0], [990.0, 1.0], [991.0, 1.0], [992.0, 1.0], [993.0, 1.0], [994.0, 1.0], [995.0, 1.0], [996.0, 1.0], [997.0, 1.0], [998.0, 1.0], [999.0, 1.0], [1000.0, 1.0], [1001.0, 1.0], [1002.0, 1.0], [1003.0, 1.0], [1004.0, 1.0], [1005.0, 1.0], [1006.0, 1.0], [1007.0, 1.0], [1008.0, 1.0], [1009.0, 1.0], [1010.0, 1.0], [1011.0, 1.0], [1012.0, 1.0], [1013.0, 1.0], [1014.0, 1.0], [1015.0, 1.0], [1016.0, 1.0], [1017.0, 1.0], [1018.0, 1.0], [1019.0, 1.0], [1020.0, 1.0], [1021.0, 1.0], [1022.0, 1.0], [1023.0, 1.0], [1024.0, 1.0], [1025.0, 1.0], [1026.0, 1.0], [1027.0, 1.0], [1028.0, 1.0], [1029.0, 1.0], [1030.0, 1.0], [1031.0, 1.0], [1032.0, 1.0], [1033.0, 1.0], [1034.0, 1.0], [1035.0, 1.0], [1036.0, 1.0], [1037.0, 1.0], [1038.0, 1.0], [1039.0, 1.0], [1040.0, 1.0], [1041.0, 1.0], [1042.0, 1.0], [1043.0, 1.0], [1044.0, 1.0], [1045.0, 1.0], [1046.0, 1.0], [1047.0, 1.0], [1048.0, 1.0], [1049.0, 1.0], [1050.0, 1.0], [1051.0, 1.0], [1052.0, 1.0], [1053.0, 1.0], [1054.0, 1.0], [1055.0, 1.0], [1056.0, 1.0], [1057.0, 1.0], [1058.0, 1.0], [1059.0, 1.0], [1060.0, 1.0], [1061.0, 1.0], [1062.0, 1.0], [1063.0, 1.0], [1064.0, 1.0], [1065.0, 1.0], [1066.0, 1.0], [1067.0, 1.0], [1068.0, 1.0], [1069.0, 1.0], [1070.0, 1.0], [1071.0, 1.0], [1072.0, 1.0], [1073.0, 1.0], [1074.0, 1.0], [1075.0, 1.0], [1076.0, 1.0], [1077.0, 1.0], [1078.0, 1.0], [1079.0, 1.0], [1080.0, 1.0], [1081.0, 1.0], [1082.0, 1.0], [1083.0, 1.0], [1084.0, 1.0], [1085.0, 1.0], [1086.0, 1.0], [1087.0, 1.0], [1088.0, 1.0], [1089.0, 1.0], [1090.0, 1.0], [1091.0, 1.0], [1092.0, 1.0], [1093.0, 1.0], [1094.0, 1.0], [1095.0, 1.0], [1096.0, 1.0], [1097.0, 1.0], [1098.0, 1.0], [1099.0, 1.0], [1100.0, 1.0], [1101.0, 1.0], [1102.0, 1.0], [1103.0, 1.0], [1104.0, 1.0], [1105.0, 1.0], [1106.0, 1.0], [1107.0, 1.0], [1108.0, 1.0], [1109.0, 1.0], [1110.0, 1.0], [1111.0, 1.0], [1112.0, 1.0], [1113.0, 1.0], [1114.0, 1.0], [1115.0, 1.0], [1116.0, 1.0], [1117.0, 1.0], [1118.0, 1.0], [1119.0, 1.0], [1120.0, 1.0], [1121.0, 1.0], [1122.0, 1.0], [1123.0, 1.0], [1124.0, 1.0], [1125.0, 1.0], [1126.0, 1.0], [1127.0, 1.0], [1128.0, 1.0], [1129.0, 1.0], [1130.0, 1.0], [1131.0, 1.0], [1132.0, 1.0], [1133.0, 1.0], [1134.0, 1.0], [1135.0, 1.0], [1136.0, 1.0], [1137.0, 1.0], [1138.0, 1.0], [1139.0, 1.0], [1140.0, 1.0], [1141.0, 1.0], [1142.0, 1.0], [1143.0, 1.0], [1144.0, 1.0], [1145.0, 1.0], [1146.0, 1.0], [1147.0, 1.0], [1148.0, 1.0], [1149.0, 1.0], [1150.0, 1.0], [1151.0, 1.0], [1152.0, 1.0], [1153.0, 1.0], [1154.0, 1.0], [1155.0, 1.0], [1156.0, 1.0], [1157.0, 1.0], [1158.0, 1.0], [1159.0, 1.0], [1160.0, 1.0], [1161.0, 1.0], [1162.0, 1.0], [1163.0, 1.0], [1164.0, 1.0], [1165.0, 1.0], [1166.0, 1.0], [1167.0, 1.0], [1168.0, 1.0], [1169.0, 1.0], [1170.0, 1.0], [1171.0, 1.0], [1172.0, 1.0], [1173.0, 1.0], [1174.0, 1.0], [1175.0, 1.0], [1176.0, 1.0], [1177.0, 1.0], [1178.0, 1.0], [1179.0, 1.0], [1180.0, 1.0], [1181.0, 1.0], [1182.0, 1.0], [1183.0, 1.0], [1184.0, 1.0], [1185.0, 1.0], [1186.0, 1.0], [1187.0, 1.0], [1188.0, 1.0], [1189.0, 1.0], [1190.0, 1.0], [1191.0, 1.0], [1192.0, 1.0], [1193.0, 1.0], [1194.0, 1.0], [1195.0, 1.0], [1196.0, 1.0], [1197.0, 1.0], [1198.0, 1.0], [1199.0, 1.0], [1200.0, 1.0], [1201.0, 1.0], [1202.0, 1.0], [1203.0, 1.0], [1204.0, 1.0], [1205.0, 1.0], [1206.0, 1.0], [1207.0, 1.0], [1208.0, 1.0], [1209.0, 1.0], [1210.0, 1.0], [1211.0, 1.0], [1212.0, 1.0], [1213.0, 1.0], [1214.0, 1.0], [1215.0, 1.0], [1216.0, 1.0], [1217.0, 1.0], [1218.0, 1.0], [1219.0, 1.0], [1220.0, 1.0], [1221.0, 1.0], [1222.0, 1.0], [1223.0, 1.0], [1224.0, 1.0], [1225.0, 1.0], [1226.0, 1.0], [1227.0, 1.0], [1228.0, 1.0], [1229.0, 1.0], [1230.0, 1.0], [1231.0, 1.0], [1232.0, 1.0], [1233.0, 1.0], [1234.0, 1.0], [1235.0, 1.0], [1236.0, 1.0], [1237.0, 1.0], [1238.0, 1.0], [1239.0, 1.0], [1240.0, 1.0], [1241.0, 1.0], [1242.0, 1.0], [1243.0, 1.0], [1244.0, 1.0], [1245.0, 1.0], [1246.0, 1.0], [1247.0, 1.0], [1248.0, 1.0], [1249.0, 1.0], [1250.0, 1.0], [1251.0, 1.0], [1252.0, 1.0], [1253.0, 1.0], [1254.0, 1.0], [1255.0, 1.0], [1256.0, 1.0], [1257.0, 1.0], [1258.0, 1.0], [1259.0, 1.0], [1260.0, 1.0], [1261.0, 1.0], [1262.0, 1.0], [1263.0, 1.0], [1264.0, 1.0], [1265.0, 1.0], [1266.0, 1.0], [1267.0, 1.0], [1268.0, 1.0], [1269.0, 1.0], [1270.0, 1.0], [1271.0, 1.0], [1272.0, 1.0], [1273.0, 1.0], [1274.0, 1.0], [1275.0, 1.0], [1276.0, 1.0], [1277.0, 1.0], [1278.0, 1.0], [1279.0, 1.0], [1280.0, 1.0], [1281.0, 1.0], [1282.0, 1.0], [1283.0, 1.0], [1284.0, 1.0], [1285.0, 1.0], [1286.0, 1.0], [1287.0, 1.0], [1288.0, 1.0], [1289.0, 1.0], [1290.0, 1.0], [1291.0, 1.0], [1292.0, 1.0], [1293.0, 1.0], [1294.0, 1.0], [1295.0, 1.0], [1296.0, 1.0], [1297.0, 1.0], [1298.0, 1.0], [1299.0, 1.0], [1300.0, 1.0], [1301.0, 1.0], [1302.0, 1.0], [1303.0, 1.0], [1304.0, 1.0], [1305.0, 1.0], [1306.0, 1.0], [1307.0, 1.0], [1308.0, 1.0], [1309.0, 1.0], [1310.0, 1.0], [1311.0, 1.0], [1312.0, 1.0], [1313.0, 1.0], [1314.0, 1.0], [1315.0, 1.0], [1316.0, 1.0], [1317.0, 1.0], [1318.0, 1.0], [1319.0, 1.0], [1320.0, 1.0], [1321.0, 1.0], [1322.0, 1.0], [1323.0, 1.0], [1324.0, 1.0], [1325.0, 1.0], [1326.0, 1.0], [1327.0, 1.0], [1328.0, 1.0], [1329.0, 1.0], [1330.0, 1.0], [1331.0, 1.0], [1332.0, 1.0], [1333.0, 1.0], [1334.0, 1.0], [1335.0, 1.0], [1336.0, 1.0], [1337.0, 1.0], [1338.0, 1.0], [1339.0, 1.0], [1340.0, 1.0], [1341.0, 1.0], [1342.0, 1.0], [1343.0, 1.0], [1344.0, 1.0], [1345.0, 1.0], [1346.0, 1.0], [1347.0, 1.0], [1348.0, 1.0], [1349.0, 1.0], [1350.0, 1.0], [1351.0, 1.0], [1352.0, 1.0], [1353.0, 1.0], [1354.0, 1.0], [1355.0, 1.0], [1356.0, 1.0], [1357.0, 1.0], [1358.0, 1.0], [1359.0, 1.0], [1360.0, 1.0], [1361.0, 1.0], [1362.0, 1.0], [1363.0, 1.0], [1364.0, 1.0], [1365.0, 1.0], [1366.0, 1.0], [1367.0, 1.0], [1368.0, 1.0], [1369.0, 1.0], [1370.0, 1.0], [1371.0, 1.0], [1372.0, 1.0], [1373.0, 1.0], [1374.0, 1.0], [1375.0, 1.0], [1376.0, 1.0], [1377.0, 1.0], [1378.0, 1.0], [1379.0, 1.0], [1380.0, 1.0], [1381.0, 1.0], [1382.0, 1.0], [1383.0, 1.0], [1384.0, 1.0], [1385.0, 1.0], [1386.0, 1.0], [1387.0, 1.0], [1388.0, 1.0], [1389.0, 1.0], [1390.0, 1.0], [1391.0, 1.0], [1392.0, 1.0], [1393.0, 1.0], [1394.0, 1.0], [1395.0, 1.0], [1396.0, 1.0], [1397.0, 1.0], [1398.0, 1.0], [1399.0, 1.0], [1400.0, 1.0], [1401.0, 1.0], [1402.0, 1.0], [1403.0, 1.0], [1404.0, 1.0], [1405.0, 1.0], [1406.0, 1.0], [1407.0, 1.0], [1408.0, 1.0], [1409.0, 1.0], [1410.0, 1.0], [1411.0, 1.0], [1412.0, 1.0], [1413.0, 1.0], [1414.0, 1.0], [1415.0, 1.0], [1416.0, 1.0], [1417.0, 1.0], [1418.0, 1.0], [1419.0, 1.0], [1420.0, 1.0], [1421.0, 1.0], [1422.0, 1.0], [1423.0, 1.0], [1424.0, 1.0], [1425.0, 1.0], [1426.0, 1.0], [1427.0, 1.0], [1428.0, 1.0], [1429.0, 1.0], [1430.0, 1.0], [1431.0, 1.0], [1432.0, 1.0], [1433.0, 1.0], [1434.0, 1.0], [1435.0, 1.0], [1436.0, 1.0], [1437.0, 1.0], [1438.0, 1.0], [1439.0, 1.0], [1440.0, 1.0], [1441.0, 1.0], [1442.0, 1.0], [1443.0, 1.0], [1444.0, 1.0], [1445.0, 1.0], [1446.0, 1.0], [1447.0, 1.0], [1448.0, 1.0], [1449.0, 1.0], [1450.0, 1.0], [1451.0, 1.0], [1452.0, 1.0], [1453.0, 1.0], [1454.0, 1.0], [1455.0, 1.0], [1456.0, 1.0], [1457.0, 1.0], [1458.0, 1.0], [1459.0, 1.0], [1460.0, 1.0], [1461.0, 1.0], [1462.0, 1.0], [1463.0, 1.0], [1464.0, 1.0], [1465.0, 1.0], [1466.0, 1.0], [1467.0, 1.0], [1468.0, 1.0], [1469.0, 1.0], [1470.0, 1.0], [1471.0, 1.0], [1472.0, 1.0], [1473.0, 1.0], [1474.0, 1.0], [1475.0, 1.0], [1476.0, 1.0], [1477.0, 1.0], [1478.0, 1.0], [1479.0, 1.0], [1480.0, 1.0], [1481.0, 1.0], [1482.0, 1.0], [1483.0, 1.0], [1484.0, 1.0], [1485.0, 1.0], [1486.0, 1.0], [1487.0, 1.0], [1488.0, 1.0], [1489.0, 1.0], [1490.0, 1.0], [1491.0, 1.0], [1492.0, 1.0], [1493.0, 1.0], [1494.0, 1.0], [1495.0, 1.0], [1496.0, 1.0], [1497.0, 1.0], [1498.0, 1.0], [1499.0, 1.0], [1500.0, 1.0], [1501.0, 1.0], [1502.0, 1.0], [1503.0, 1.0], [1504.0, 1.0], [1505.0, 1.0], [1506.0, 1.0], [1507.0, 1.0], [1508.0, 1.0], [1509.0, 1.0], [1510.0, 1.0], [1511.0, 1.0], [1512.0, 1.0], [1513.0, 1.0], [1514.0, 1.0], [1515.0, 1.0], [1516.0, 1.0], [1517.0, 1.0], [1518.0, 1.0], [1519.0, 1.0], [1520.0, 1.0], [1521.0, 1.0], [1522.0, 1.0], [1523.0, 1.0], [1524.0, 1.0], [1525.0, 1.0], [1526.0, 1.0], [1527.0, 1.0], [1528.0, 1.0], [1529.0, 1.0], [1530.0, 1.0], [1531.0, 1.0], [1532.0, 1.0], [1533.0, 1.0], [1534.0, 1.0], [1535.0, 1.0], [1536.0, 1.0], [1537.0, 1.0], [1538.0, 1.0], [1539.0, 1.0], [1540.0, 1.0], [1541.0, 1.0], [1542.0, 1.0], [1543.0, 1.0], [1544.0, 1.0], [1545.0, 1.0], [1546.0, 1.0], [1547.0, 1.0], [1548.0, 1.0], [1549.0, 1.0], [1550.0, 1.0], [1551.0, 1.0], [1552.0, 1.0], [1553.0, 1.0], [1554.0, 1.0], [1555.0, 1.0], [1556.0, 1.0], [1557.0, 1.0], [1558.0, 1.0], [1559.0, 1.0], [1560.0, 1.0], [1561.0, 1.0], [1562.0, 1.0], [1563.0, 1.0], [1564.0, 1.0], [1565.0, 1.0], [1566.0, 1.0], [1567.0, 1.0], [1568.0, 1.0], [1569.0, 1.0], [1570.0, 1.0], [1571.0, 1.0], [1572.0, 1.0], [1573.0, 1.0], [1574.0, 1.0], [1575.0, 1.0], [1576.0, 1.0], [1577.0, 1.0], [1578.0, 1.0], [1579.0, 1.0], [1580.0, 1.0], [1581.0, 1.0], [1582.0, 1.0], [1583.0, 1.0], [1584.0, 1.0], [1585.0, 1.0], [1586.0, 1.0], [1587.0, 1.0], [1588.0, 1.0], [1589.0, 1.0], [1590.0, 1.0], [1591.0, 1.0], [1592.0, 1.0], [1593.0, 1.0], [1594.0, 1.0], [1595.0, 1.0], [1596.0, 1.0], [1597.0, 1.0], [1598.0, 1.0], [1599.0, 1.0], [1600.0, 1.0], [1601.0, 1.0], [1602.0, 1.0], [1603.0, 1.0], [1604.0, 1.0], [1605.0, 1.0], [1606.0, 1.0], [1607.0, 1.0], [1608.0, 1.0], [1609.0, 1.0], [1610.0, 1.0], [1611.0, 1.0], [1612.0, 1.0], [1613.0, 1.0], [1614.0, 1.0], [1615.0, 1.0], [1616.0, 1.0], [1617.0, 1.0], [1618.0, 1.0], [1619.0, 1.0], [1620.0, 1.0], [1621.0, 1.0], [1622.0, 1.0], [1623.0, 1.0], [1624.0, 1.0], [1625.0, 1.0], [1626.0, 1.0], [1627.0, 1.0], [1628.0, 1.0], [1629.0, 1.0], [1630.0, 1.0], [1631.0, 1.0], [1632.0, 1.0], [1633.0, 1.0], [1634.0, 1.0], [1635.0, 1.0], [1636.0, 1.0], [1637.0, 1.0], [1638.0, 1.0], [1639.0, 1.0], [1640.0, 1.0], [1641.0, 1.0], [1642.0, 1.0], [1643.0, 1.0], [1644.0, 1.0], [1645.0, 1.0], [1646.0, 1.0], [1647.0, 1.0], [1648.0, 1.0], [1649.0, 1.0], [1650.0, 1.0], [1651.0, 1.0], [1652.0, 1.0], [1653.0, 1.0], [1654.0, 1.0], [1655.0, 1.0], [1656.0, 1.0], [1657.0, 1.0], [1658.0, 1.0], [1659.0, 1.0], [1660.0, 1.0], [1661.0, 1.0], [1662.0, 1.0], [1663.0, 1.0], [1664.0, 1.0], [1665.0, 1.0], [1666.0, 1.0], [1667.0, 1.0], [1668.0, 1.0], [1669.0, 1.0], [1670.0, 1.0], [1671.0, 1.0], [1672.0, 1.0], [1673.0, 1.0], [1674.0, 1.0], [1675.0, 1.0], [1676.0, 1.0], [1677.0, 1.0], [1678.0, 1.0], [1679.0, 1.0], [1680.0, 1.0], [1681.0, 1.0], [1682.0, 1.0], [1683.0, 1.0], [1684.0, 1.0], [1685.0, 1.0], [1686.0, 1.0], [1687.0, 1.0], [1688.0, 1.0], [1689.0, 1.0], [1690.0, 1.0], [1691.0, 1.0], [1692.0, 1.0], [1693.0, 1.0], [1694.0, 1.0], [1695.0, 1.0], [1696.0, 1.0], [1697.0, 1.0], [1698.0, 1.0], [1699.0, 1.0], [1700.0, 1.0], [1701.0, 1.0], [1702.0, 1.0], [1703.0, 1.0], [1704.0, 1.0], [1705.0, 1.0], [1706.0, 1.0], [1707.0, 1.0], [1708.0, 1.0], [1709.0, 1.0], [1710.0, 1.0], [1711.0, 1.0], [1712.0, 1.0], [1713.0, 1.0], [1714.0, 1.0], [1715.0, 1.0], [1716.0, 1.0], [1717.0, 1.0], [1718.0, 1.0], [1719.0, 1.0], [1720.0, 1.0], [1721.0, 1.0], [1722.0, 1.0], [1723.0, 1.0], [1724.0, 1.0], [1725.0, 1.0], [1726.0, 1.0], [1727.0, 1.0], [1728.0, 1.0], [1729.0, 1.0], [1730.0, 1.0], [1731.0, 1.0], [1732.0, 1.0], [1733.0, 1.0], [1734.0, 1.0], [1735.0, 1.0], [1736.0, 1.0], [1737.0, 1.0], [1738.0, 1.0], [1739.0, 1.0], [1740.0, 1.0], [1741.0, 1.0], [1742.0, 1.0], [1743.0, 1.0], [1744.0, 1.0], [1745.0, 1.0], [1746.0, 1.0], [1747.0, 1.0], [1748.0, 1.0], [1749.0, 1.0], [1750.0, 1.0], [1751.0, 1.0], [1752.0, 1.0], [1753.0, 1.0], [1754.0, 1.0], [1755.0, 1.0], [1756.0, 1.0], [1757.0, 1.0], [1758.0, 1.0], [1759.0, 1.0], [1760.0, 1.0], [1761.0, 1.0], [1762.0, 1.0], [1763.0, 1.0], [1764.0, 1.0], [1765.0, 1.0], [1766.0, 1.0], [1767.0, 1.0], [1768.0, 1.0], [1769.0, 1.0], [1770.0, 1.0], [1771.0, 1.0], [1772.0, 1.0], [1773.0, 1.0], [1774.0, 1.0], [1775.0, 1.0], [1776.0, 1.0], [1777.0, 1.0], [1778.0, 1.0], [1779.0, 1.0], [1780.0, 1.0], [1781.0, 1.0], [1782.0, 1.0], [1783.0, 1.0], [1784.0, 1.0], [1785.0, 1.0], [1786.0, 1.0], [1787.0, 1.0], [1788.0, 1.0], [1789.0, 1.0], [1790.0, 1.0], [1791.0, 1.0], [1792.0, 1.0], [1793.0, 1.0], [1794.0, 1.0], [1795.0, 1.0], [1796.0, 1.0], [1797.0, 1.0], [1798.0, 1.0], [1799.0, 1.0], [1800.0, 1.0], [1801.0, 1.0], [1802.0, 1.0], [1803.0, 1.0], [1804.0, 1.0], [1805.0, 1.0], [1806.0, 1.0], [1807.0, 1.0], [1808.0, 1.0], [1809.0, 1.0], [1810.0, 1.0], [1811.0, 1.0], [1812.0, 1.0], [1813.0, 1.0], [1814.0, 1.0], [1815.0, 1.0], [1816.0, 1.0], [1817.0, 1.0], [1818.0, 1.0], [1819.0, 1.0], [1820.0, 1.0], [1821.0, 1.0], [1822.0, 1.0], [1823.0, 1.0], [1824.0, 1.0], [1825.0, 1.0], [1826.0, 1.0], [1827.0, 1.0], [1828.0, 1.0], [1829.0, 1.0], [1830.0, 1.0], [1831.0, 1.0], [1832.0, 1.0], [1833.0, 1.0], [1834.0, 1.0], [1835.0, 1.0], [1836.0, 1.0], [1837.0, 1.0], [1838.0, 1.0], [1839.0, 1.0], [1840.0, 1.0], [1841.0, 1.0], [1842.0, 1.0], [1843.0, 1.0], [1844.0, 1.0], [1845.0, 1.0], [1846.0, 1.0], [1847.0, 1.0], [1848.0, 1.0], [1849.0, 1.0], [1850.0, 1.0], [1851.0, 1.0], [1852.0, 1.0], [1853.0, 1.0], [1854.0, 1.0], [1855.0, 1.0], [1856.0, 1.0], [1857.0, 1.0], [1858.0, 1.0], [1859.0, 1.0], [1860.0, 1.0], [1861.0, 1.0], [1862.0, 1.0], [1863.0, 1.0], [1864.0, 1.0], [1865.0, 1.0], [1866.0, 1.0], [1867.0, 1.0], [1868.0, 1.0], [1869.0, 1.0], [1870.0, 1.0], [1871.0, 1.0], [1872.0, 1.0], [1873.0, 1.0], [1874.0, 1.0], [1875.0, 1.0], [1876.0, 1.0], [1877.0, 1.0], [1878.0, 1.0], [1879.0, 1.0], [1880.0, 1.0], [1881.0, 1.0], [1882.0, 1.0], [1883.0, 1.0], [1884.0, 1.0], [1885.0, 1.0], [1886.0, 1.0], [1887.0, 1.0], [1888.0, 1.0], [1889.0, 1.0], [1890.0, 1.0], [1891.0, 1.0], [1892.0, 1.0], [1893.0, 1.0], [1894.0, 1.0], [1895.0, 1.0], [1896.0, 1.0], [1897.0, 1.0], [1898.0, 1.0], [1899.0, 1.0], [1900.0, 1.0], [1901.0, 1.0], [1902.0, 1.0], [1903.0, 1.0], [1904.0, 1.0], [1905.0, 1.0], [1906.0, 1.0], [1907.0, 1.0], [1908.0, 1.0], [1909.0, 1.0], [1910.0, 1.0], [1911.0, 1.0], [1912.0, 1.0], [1913.0, 1.0], [1914.0, 1.0], [1915.0, 1.0], [1916.0, 1.0], [1917.0, 1.0], [1918.0, 1.0], [1919.0, 1.0], [1920.0, 1.0], [1921.0, 1.0], [1922.0, 1.0], [1923.0, 1.0], [1924.0, 1.0], [1925.0, 1.0], [1926.0, 1.0], [1927.0, 1.0], [1928.0, 1.0], [1929.0, 1.0], [1930.0, 1.0], [1931.0, 1.0], [1932.0, 1.0], [1933.0, 1.0], [1934.0, 1.0], [1935.0, 1.0], [1936.0, 1.0], [1937.0, 1.0], [1938.0, 1.0], [1939.0, 1.0], [1940.0, 1.0], [1941.0, 1.0], [1942.0, 1.0], [1943.0, 1.0], [1944.0, 1.0], [1945.0, 1.0], [1946.0, 1.0], [1947.0, 1.0], [1948.0, 1.0], [1949.0, 1.0], [1950.0, 1.0], [1951.0, 1.0], [1952.0, 1.0], [1953.0, 1.0], [1954.0, 1.0], [1955.0, 1.0], [1956.0, 1.0], [1957.0, 1.0], [1958.0, 1.0], [1959.0, 1.0], [1960.0, 1.0], [1961.0, 1.0], [1962.0, 1.0], [1963.0, 1.0], [1964.0, 1.0], [1965.0, 1.0], [1966.0, 1.0], [1967.0, 1.0], [1968.0, 1.0], [1969.0, 1.0], [1970.0, 1.0], [1971.0, 1.0], [1972.0, 1.0], [1973.0, 1.0], [1974.0, 1.0], [1975.0, 1.0], [1976.0, 1.0], [1977.0, 1.0], [1978.0, 1.0], [1979.0, 1.0], [1980.0, 1.0], [1981.0, 1.0], [1982.0, 1.0], [1983.0, 1.0], [1984.0, 1.0], [1985.0, 1.0], [1986.0, 1.0], [1987.0, 1.0], [1988.0, 1.0], [1989.0, 1.0], [1990.0, 1.0], [1991.0, 1.0], [1992.0, 1.0], [1993.0, 1.0], [1994.0, 1.0], [1995.0, 1.0], [1996.0, 1.0], [1997.0, 1.0], [1998.0, 1.0], [1999.0, 1.0], [2000.0, 1.0], [2001.0, 1.0], [2002.0, 1.0], [2003.0, 1.0], [2004.0, 1.0], [2005.0, 1.0], [2006.0, 1.0], [2007.0, 1.0], [2008.0, 1.0], [2009.0, 1.0], [2010.0, 1.0], [2011.0, 1.0], [2012.0, 1.0], [2013.0, 1.0], [2014.0, 1.0], [2015.0, 1.0], [2016.0, 1.0], [2017.0, 1.0], [2018.0, 1.0], [2019.0, 1.0], [2020.0, 1.0], [2021.0, 1.0], [2022.0, 1.0], [2023.0, 1.0], [2024.0, 1.0], [2025.0, 1.0], [2026.0, 1.0], [2027.0, 1.0], [2028.0, 1.0], [2029.0, 1.0], [2030.0, 1.0], [2031.0, 1.0], [2032.0, 1.0], [2033.0, 1.0], [2034.0, 1.0], [2035.0, 1.0], [2036.0, 1.0], [2037.0, 1.0], [2038.0, 1.0], [2039.0, 1.0], [2040.0, 1.0], [2041.0, 1.0], [2042.0, 1.0], [2043.0, 1.0], [2044.0, 1.0], [2045.0, 1.0], [2046.0, 1.0], [2047.0, 1.0], [2048.0, 1.0], [2049.0, 1.0], [2050.0, 1.0], [2051.0, 1.0], [2052.0, 1.0], [2053.0, 1.0], [2054.0, 1.0], [2055.0, 1.0], [2056.0, 1.0], [2057.0, 1.0], [2058.0, 1.0], [2059.0, 1.0], [2060.0, 1.0], [2061.0, 1.0], [2062.0, 1.0], [2063.0, 1.0], [2064.0, 1.0], [2065.0, 1.0], [2066.0, 1.0], [2067.0, 1.0], [2068.0, 1.0], [2069.0, 1.0], [2070.0, 1.0], [2071.0, 1.0], [2072.0, 1.0], [2073.0, 1.0], [2074.0, 1.0], [2075.0, 1.0], [2076.0, 1.0], [2077.0, 1.0], [2078.0, 1.0], [2079.0, 1.0], [2080.0, 1.0], [2081.0, 1.0], [2082.0, 1.0], [2083.0, 1.0], [2084.0, 1.0], [2085.0, 1.0], [2086.0, 1.0], [2087.0, 1.0], [2088.0, 1.0], [2089.0, 1.0], [2090.0, 1.0], [2091.0, 1.0], [2092.0, 1.0], [2093.0, 1.0], [2094.0, 1.0], [2095.0, 1.0], [2096.0, 1.0], [2097.0, 1.0], [2098.0, 1.0], [2099.0, 1.0], [2100.0, 1.0], [2101.0, 1.0], [2102.0, 1.0], [2103.0, 1.0], [2104.0, 1.0], [2105.0, 1.0], [2106.0, 1.0], [2107.0, 1.0], [2108.0, 1.0], [2109.0, 1.0], [2110.0, 1.0], [2111.0, 1.0], [2112.0, 1.0], [2113.0, 1.0], [2114.0, 1.0], [2115.0, 1.0], [2116.0, 1.0], [2117.0, 1.0], [2118.0, 1.0], [2119.0, 1.0], [2120.0, 1.0], [2121.0, 1.0], [2122.0, 1.0], [2123.0, 1.0], [2124.0, 1.0], [2125.0, 1.0], [2126.0, 1.0], [2127.0, 1.0], [2128.0, 1.0], [2129.0, 1.0], [2130.0, 1.0], [2131.0, 1.0], [2132.0, 1.0], [2133.0, 1.0], [2134.0, 1.0], [2135.0, 1.0], [2136.0, 1.0], [2137.0, 1.0], [2138.0, 1.0], [2139.0, 1.0], [2140.0, 1.0], [2141.0, 1.0], [2142.0, 1.0], [2143.0, 1.0], [2144.0, 1.0], [2145.0, 1.0], [2146.0, 1.0], [2147.0, 1.0], [2148.0, 1.0], [2149.0, 1.0], [2150.0, 1.0], [2151.0, 1.0], [2152.0, 1.0], [2153.0, 1.0], [2154.0, 1.0], [2155.0, 1.0], [2156.0, 1.0], [2157.0, 1.0], [2158.0, 1.0], [2159.0, 1.0], [2160.0, 1.0], [2161.0, 1.0], [2162.0, 1.0], [2163.0, 1.0], [2164.0, 1.0], [2165.0, 1.0], [2166.0, 1.0], [2167.0, 1.0], [2168.0, 1.0], [2169.0, 1.0], [2170.0, 1.0], [2171.0, 1.0], [2172.0, 1.0], [2173.0, 1.0], [2174.0, 1.0], [2175.0, 1.0], [2176.0, 1.0], [2177.0, 1.0], [2178.0, 1.0], [2179.0, 1.0], [2180.0, 1.0], [2181.0, 1.0], [2182.0, 1.0], [2183.0, 1.0], [2184.0, 1.0], [2185.0, 1.0], [2186.0, 1.0], [2187.0, 1.0], [2188.0, 1.0], [2189.0, 1.0], [2190.0, 1.0], [2191.0, 1.0], [2192.0, 1.0], [2193.0, 1.0], [2194.0, 1.0], [2195.0, 1.0], [2196.0, 1.0], [2197.0, 1.0], [2198.0, 1.0], [2199.0, 1.0], [2200.0, 1.0], [2201.0, 1.0], [2202.0, 1.0], [2203.0, 1.0], [2204.0, 1.0], [2205.0, 1.0], [2206.0, 1.0], [2207.0, 1.0], [2208.0, 1.0], [2209.0, 1.0], [2210.0, 1.0], [2211.0, 1.0], [2212.0, 1.0], [2213.0, 1.0], [2214.0, 1.0], [2215.0, 1.0], [2216.0, 1.0], [2217.0, 1.0], [2218.0, 1.0], [2219.0, 1.0], [2220.0, 1.0], [2221.0, 1.0], [2222.0, 1.0], [2223.0, 1.0], [2224.0, 1.0], [2225.0, 1.0], [2226.0, 1.0], [2227.0, 1.0], [2228.0, 1.0], [2229.0, 1.0], [2230.0, 1.0], [2231.0, 1.0], [2232.0, 1.0], [2233.0, 1.0], [2234.0, 1.0], [2235.0, 1.0], [2236.0, 1.0], [2237.0, 1.0], [2238.0, 1.0], [2239.0, 1.0], [2240.0, 1.0], [2241.0, 1.0], [2242.0, 1.0], [2243.0, 1.0], [2244.0, 1.0], [2245.0, 1.0], [2246.0, 1.0], [2247.0, 1.0], [2248.0, 1.0], [2249.0, 1.0], [2250.0, 1.0], [2251.0, 1.0], [2252.0, 1.0], [2253.0, 1.0], [2254.0, 1.0], [2255.0, 1.0], [2256.0, 1.0], [2257.0, 1.0], [2258.0, 1.0], [2259.0, 1.0], [2260.0, 1.0], [2261.0, 1.0], [2262.0, 1.0], [2263.0, 1.0], [2264.0, 1.0], [2265.0, 1.0], [2266.0, 1.0], [2267.0, 1.0], [2268.0, 1.0], [2269.0, 1.0], [2270.0, 1.0], [2271.0, 1.0], [2272.0, 1.0], [2273.0, 1.0], [2274.0, 1.0], [2275.0, 1.0], [2276.0, 1.0], [2277.0, 1.0], [2278.0, 1.0], [2279.0, 1.0], [2280.0, 1.0], [2281.0, 1.0], [2282.0, 1.0], [2283.0, 1.0], [2284.0, 1.0], [2285.0, 1.0], [2286.0, 1.0], [2287.0, 1.0], [2288.0, 1.0], [2289.0, 1.0], [2290.0, 1.0], [2291.0, 1.0], [2292.0, 1.0], [2293.0, 1.0], [2294.0, 1.0], [2295.0, 1.0], [2296.0, 1.0], [2297.0, 1.0], [2298.0, 1.0], [2299.0, 1.0], [2300.0, 1.0], [2301.0, 1.0], [2302.0, 1.0], [2303.0, 1.0], [2304.0, 1.0], [2305.0, 1.0], [2306.0, 1.0], [2307.0, 1.0], [2308.0, 1.0], [2309.0, 1.0], [2310.0, 1.0], [2311.0, 1.0], [2312.0, 1.0], [2313.0, 1.0], [2314.0, 1.0], [2315.0, 1.0], [2316.0, 1.0], [2317.0, 1.0], [2318.0, 1.0], [2319.0, 1.0], [2320.0, 1.0], [2321.0, 1.0], [2322.0, 1.0], [2323.0, 1.0], [2324.0, 1.0], [2325.0, 1.0], [2326.0, 1.0], [2327.0, 1.0], [2328.0, 1.0], [2329.0, 1.0], [2330.0, 1.0], [2331.0, 1.0], [2332.0, 1.0], [2333.0, 1.0], [2334.0, 1.0], [2335.0, 1.0], [2336.0, 1.0], [2337.0, 1.0], [2338.0, 1.0], [2339.0, 1.0], [2340.0, 1.0], [2341.0, 1.0], [2342.0, 1.0], [2343.0, 1.0], [2344.0, 1.0], [2345.0, 1.0], [2346.0, 1.0], [2347.0, 1.0], [2348.0, 1.0], [2349.0, 1.0], [2350.0, 1.0], [2351.0, 1.0], [2352.0, 1.0], [2353.0, 1.0], [2354.0, 1.0], [2355.0, 1.0], [2356.0, 1.0], [2357.0, 1.0], [2358.0, 1.0], [2359.0, 1.0], [2360.0, 1.0], [2361.0, 1.0], [2362.0, 1.0], [2363.0, 1.0], [2364.0, 1.0], [2365.0, 1.0], [2366.0, 1.0], [2367.0, 1.0], [2368.0, 1.0], [2369.0, 1.0], [2370.0, 1.0], [2371.0, 1.0], [2372.0, 1.0], [2373.0, 1.0], [2374.0, 1.0], [2375.0, 1.0], [2376.0, 1.0], [2377.0, 1.0], [2378.0, 1.0], [2379.0, 1.0], [2380.0, 1.0], [2381.0, 1.0], [2382.0, 1.0], [2383.0, 1.0], [2384.0, 1.0], [2385.0, 1.0], [2386.0, 1.0], [2387.0, 1.0], [2388.0, 1.0], [2389.0, 1.0], [2390.0, 1.0], [2391.0, 1.0], [2392.0, 1.0], [2393.0, 1.0], [2394.0, 1.0], [2395.0, 1.0], [2396.0, 1.0], [2397.0, 1.0], [2398.0, 1.0], [2399.0, 1.0], [2400.0, 1.0], [2401.0, 1.0], [2402.0, 1.0], [2403.0, 1.0], [2404.0, 1.0], [2405.0, 1.0], [2406.0, 1.0], [2407.0, 1.0], [2408.0, 1.0], [2409.0, 1.0], [2410.0, 1.0], [2411.0, 1.0], [2412.0, 1.0], [2413.0, 1.0], [2414.0, 1.0], [2415.0, 1.0], [2416.0, 1.0], [2417.0, 1.0], [2418.0, 1.0], [2419.0, 1.0], [2420.0, 1.0], [2421.0, 1.0], [2422.0, 1.0], [2423.0, 1.0], [2424.0, 1.0], [2425.0, 1.0], [2426.0, 1.0], [2427.0, 1.0], [2428.0, 1.0], [2429.0, 1.0], [2430.0, 1.0], [2431.0, 1.0], [2432.0, 1.0], [2433.0, 1.0], [2434.0, 1.0], [2435.0, 1.0], [2436.0, 1.0], [2437.0, 1.0], [2438.0, 1.0], [2439.0, 1.0], [2440.0, 1.0], [2441.0, 1.0], [2442.0, 1.0], [2443.0, 1.0], [2444.0, 1.0], [2445.0, 1.0], [2446.0, 1.0], [2447.0, 1.0], [2448.0, 1.0], [2449.0, 1.0], [2450.0, 1.0], [2451.0, 1.0], [2452.0, 1.0], [2453.0, 1.0], [2454.0, 1.0], [2455.0, 1.0], [2456.0, 1.0], [2457.0, 1.0], [2458.0, 1.0], [2459.0, 1.0], [2460.0, 1.0], [2461.0, 1.0], [2462.0, 1.0], [2463.0, 1.0], [2464.0, 1.0], [2465.0, 1.0], [2466.0, 1.0], [2467.0, 1.0], [2468.0, 1.0], [2469.0, 1.0], [2470.0, 1.0], [2471.0, 1.0], [2472.0, 1.0], [2473.0, 1.0], [2474.0, 1.0], [2475.0, 1.0], [2476.0, 1.0], [2477.0, 1.0], [2478.0, 1.0], [2479.0, 1.0], [2480.0, 1.0], [2481.0, 1.0], [2482.0, 1.0], [2483.0, 1.0], [2484.0, 1.0], [2485.0, 1.0], [2486.0, 1.0], [2487.0, 1.0], [2488.0, 1.0], [2489.0, 1.0], [2490.0, 1.0], [2491.0, 1.0], [2492.0, 1.0], [2493.0, 1.0], [2494.0, 1.0], [2495.0, 1.0], [2496.0, 1.0], [2497.0, 1.0], [2498.0, 1.0], [2499.0, 1.0], [2500.0, 1.0], [2501.0, 1.0], [2502.0, 1.0], [2503.0, 1.0], [2504.0, 1.0], [2505.0, 1.0], [2506.0, 1.0], [2507.0, 1.0], [2508.0, 1.0], [2509.0, 1.0], [2510.0, 1.0], [2511.0, 1.0], [2512.0, 1.0], [2513.0, 1.0], [2514.0, 1.0], [2515.0, 1.0], [2516.0, 1.0], [2517.0, 1.0], [2518.0, 1.0], [2519.0, 1.0], [2520.0, 1.0], [2521.0, 1.0], [2522.0, 1.0], [2523.0, 1.0], [2524.0, 1.0], [2525.0, 1.0], [2526.0, 1.0], [2527.0, 1.0], [2528.0, 1.0], [2529.0, 1.0], [2530.0, 1.0], [2531.0, 1.0], [2532.0, 1.0], [2533.0, 1.0], [2534.0, 1.0], [2535.0, 1.0], [2536.0, 1.0], [2537.0, 1.0], [2538.0, 1.0], [2539.0, 1.0], [2540.0, 1.0], [2541.0, 1.0], [2542.0, 1.0], [2543.0, 1.0], [2544.0, 1.0], [2545.0, 1.0], [2546.0, 1.0], [2547.0, 1.0], [2548.0, 1.0], [2549.0, 1.0], [2550.0, 1.0], [2551.0, 1.0], [2552.0, 1.0], [2553.0, 1.0], [2554.0, 1.0], [2555.0, 1.0], [2556.0, 1.0]]}, \"height\": 320.0});\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.xscale('linear')\n",
    "plt.yscale('linear')\n",
    "plt.plot(ranks, sorted_counts)\n",
    "mpld3.display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In log-space such rank vs frequency graphs resemble linear functions. This observation is known as *Zipf's Law*, and can be formalized as follows. Let \\\\(r\\_w\\\\) be the rank of a word \\\\(w\\\\), and \\\\(f\\_w\\\\) its frequency, then we have:\n",
    "\n",
    "$$\n",
    "  f_w \\propto \\frac{1}{r_w}.\n",
    "$$\n",
    "\n",
    "## Inserting Out-of-Vocabularly Tokens\n",
    "The long tail of infrequent words is a problem for LMs because it means there will always be words with zero counts in your training set. There are various solutions to this problem. For example, when it comes to calculating the LM perplexity we could remove words that do not appear in the training set. This overcomes the problem of infinite perplexity but doesn't solve the actual issue: the LM assigns too low probability to unseen words. Moreover, the problem only gets worse when one considers n-gram models with larger \\\\(n\\\\), because these will encounter many unseen n-grams, which, when removed, will only leave small fractions of the original sentences.\n",
    "\n",
    "The principled solution to this problem is smoothing, and we will discuss it in more detail later. Before we get there we present a simple preprocessing step that generally simplifies the handling of unseen words, and gives rise to a simple smoothing heuristic. Namely, we replace unseen words in the test corpus with an out-of-vocabularly token, say `OOV`. This means that LMs can still work with a fixed vocabularly that consists of all training words, and the `OOV` token. Now we just need a way to estimate the probability of the `OOV` token to avoid the infinite perplexity problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[BAR]',\n",
       " '[OOV]',\n",
       " '[OOV]',\n",
       " 'to',\n",
       " 'the',\n",
       " '[OOV]',\n",
       " '[/BAR]',\n",
       " '[BAR]',\n",
       " '[/BAR]',\n",
       " '[BAR]']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OOV = '[OOV]'\n",
    "def replace_OOVs(vocab,data):\n",
    "    return [word if word in vocab else OOV for word in data]\n",
    "\n",
    "replace_OOVs(baseline.vocab, test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to (heuristically) estimate the `OOV` probability is to replace the first encounter of each word in the training set with the `OOV` token. Now we can estimate LMs as before, and will automatically get some estimate of the `OOV` probability. The underlying assumption of this heuristic is that the probability of unseen words is identical to the probability of encountering a new word. We illustrate the two operations of this method in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[OOV]', 'AA', '[OOV]', 'BB', 'AA']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inject_OOVs(data):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for word in data:\n",
    "        if word in seen:\n",
    "            result.append(word)\n",
    "        else:\n",
    "            result.append(OOV)\n",
    "            seen.add(word)\n",
    "    return result\n",
    "\n",
    "inject_OOVs([\"AA\",\"AA\",\"BB\",\"BB\",\"AA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply this to our training and test set, and create a new uniform model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "928.0000000011556"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_train = inject_OOVs(train)\n",
    "oov_vocab = set(oov_train)\n",
    "oov_test = replace_OOVs(oov_vocab, test)\n",
    "oov_baseline = UniformLM(oov_vocab)\n",
    "perplexity(oov_baseline,oov_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Language Models\n",
    "The uniform LM is obviously not good at modelling actual language. To improve upon this baseline, we can estimate the conditional n-gram distributions from the training data. To this end let us first introduce one parameter $\\param_{w,h}$ for each word $w$ and history $h$ of length $n - 1$, and define a parametrized language model $p_\\params$: \n",
    "\n",
    "$$\n",
    "\\prob_\\params(w|h) = \\param_{w,h}\n",
    "$$\n",
    "\n",
    "Training an n-gram LM amounts to estimating \\\\(\\params\\\\) from some training set \\\\(\\train=(w_1,\\ldots,w_n)\\\\).\n",
    "One way to do this is to choose the \\\\(\\params\\\\) that maximizes the log-likelihood of \\\\(\\train\\\\):\n",
    "$$\n",
    "\\params^* = \\argmax_\\params \\log p_\\params(\\train)\n",
    "$$\n",
    "\n",
    "As it turns out, this maximum-log-likelihood estimate (MLE) can calculated in closed form, simply by counting:\n",
    "$$\n",
    "\\param^*_{w,h} = \\frac{\\counts{\\train}{w,h}}{\\counts{\\train}{h}} \n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\counts{D}{e} = \\text{Count of event } e \\text{ in }  D \n",
    "$$\n",
    "\n",
    "Here the event $h$ means seeing the history $h$, and $w,h$ seeing the history $h$ followed by word $w$.  \n",
    "\n",
    "Many LM variants can be implemented simply by estimating the counts in the nominator and denominator differently. We therefore introduce an interface for such count-based LMs. This will help us later to implement LM variants by modifying the counts of a base-LM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CountLM(LanguageModel):\n",
    "    \"\"\"\n",
    "    A Language Model that uses counts of events and histories to calculate probabilities of words in context.\n",
    "    \"\"\"\n",
    "    @abc.abstractmethod\n",
    "    def counts(self, word_and_history):\n",
    "        pass\n",
    "    @abc.abstractmethod\n",
    "    def norm(self, history):\n",
    "        pass\n",
    "    \n",
    "    def probability(self, word, *history):\n",
    "        sub_history = tuple(history[-(self.order-1):]) if self.order > 1 else () \n",
    "        return self.counts((word,) + sub_history) / self.norm(sub_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use this to code up a generic NGram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramLM(CountLM):\n",
    "    def __init__(self, train, order):\n",
    "        \"\"\"\n",
    "        Create an NGram language model.\n",
    "        Args:\n",
    "            train: list of training tokens.\n",
    "            order: order of the LM.\n",
    "        \"\"\"\n",
    "        super().__init__(set(train), order)\n",
    "        self._counts = collections.defaultdict(float)\n",
    "        self._norm = collections.defaultdict(float)\n",
    "        for i in range(self.order, len(train)):\n",
    "            history = tuple(train[i - self.order + 1 : i])\n",
    "            word = train[i]\n",
    "            self._counts[(word,) + history] += 1.0\n",
    "            self._norm[history] += 1.0\n",
    "    def counts(self, word_and_history):\n",
    "        return self._counts[word_and_history]\n",
    "    def norm(self, history):\n",
    "        return self._norm[history]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train a unigram model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAE+9JREFUeJzt3X20ZXV93/H3ZxhYBGKJRMpEEIhANTQ+xCqOBcMYVnT0\nj5C2qRlsYspaEtqGVmubYFerkDZNA01MYowxrBDXissVNLEoyVILtpy2hAAjTz7NOCPIlIdxIhEN\nmGDH4ds/9r665859OPfOufee+5v3a62z5uy9f3v/vudhPnuf397n3FQVkqS2bFjrAiRJk2e4S1KD\nDHdJapDhLkkNMtwlqUGGuyQ1aKxwT7I1yc4ku5JcMcfyNyS5r7/dmuSFg2UP9vPvSXLnJIuXJM0t\ni13nnmQDsAu4EHgU2A5sq6qdgzabgR1V9fUkW4Grqmpzv+wB4O9V1eMr9BgkSbOMc+R+LrC7qvZU\n1X7geuCiYYOqur2qvt5P3g6cMlicMfuRJE3IOKF7CvDQYPphDg7v2d4EfHwwXcDNSbYnuXTpJUqS\nlmrjJDeW5FXAJcD5g9nnVdXeJCfRhfyOqrp1kv1Kkg42Trg/Apw2mD61n3eQ/iTqtcDW4fh6Ve3t\n//1KkhvohnkOCfck/siNJC1RVWWu+eMMy2wHzkpyepJjgG3AjcMGSU4DPgz8dFXdP5h/XJLv7u8f\nD7wa+OwCRS7pduWVVy55nZW4TUsd01SLdVjHeqhjmmpZTh0LWfTIvaoOJLkcuIluZ3BdVe1Iclm3\nuK4F3g6cCLwnSYD9VXUucDJwQ39UvhH4QFXdtFifkqTDM9aYe1V9AnjerHm/O7h/KXDIydKq+hLw\n4sOsUZK0ROv6EsUtW7asdQnA9NQB01OLdRzMOg42LXXA9NQy6ToW/RLTaklS01KLJK0HSajDOKEq\nSVpnDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrsk\nNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KD\nDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWrQWOGeZGuSnUl2JblijuVv\nSHJff7s1yQvHXVeSNHmpqoUbJBuAXcCFwKPAdmBbVe0ctNkM7KiqryfZClxVVZvHWXewjVqsFknS\ndyShqjLXsnGO3M8FdlfVnqraD1wPXDRsUFW3V9XX+8nbgVPGXVeSNHnjhPspwEOD6Yf5TnjP5U3A\nx5ezbpJVuW3adMYYD1uS1q+Nk9xYklcBlwDnL28LVw7ub+lvk7dv35yfYiRpqo1GI0aj0Vhtxxlz\n30w3hr61n34bUFV19ax2LwQ+DGytqvuXsm6/rGC1xtyD4/uS1rvDHXPfDpyV5PQkxwDbgBtndXAa\nXbD/9Eywj7uuJGnyFh2WqaoDSS4HbqLbGVxXVTuSXNYtrmuBtwMnAu9JEmB/VZ0737or9mgkScAY\nwzKrxWEZSVqawx2WkSStM4a7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1\nyHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMM\nd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCX\npAaNFe5JtibZmWRXkivmWP68JLcleSrJW2ctezDJfUnuSXLnpAqXJM1v42INkmwA3g1cCDwKbE/y\n0araOWj2l8C/BH58jk08DWypqscnUK8kaQzjHLmfC+yuqj1VtR+4Hrho2KCqHququ4BvzbF+xuxH\nkjQh44TuKcBDg+mH+3njKuDmJNuTXLqU4iRJy7PosMwEnFdVe5OcRBfyO6rq1lXoV5KOWOOE+yPA\naYPpU/t5Y6mqvf2/X0lyA90wzzzhftXg/pb+JkkCGI1GjEajsdqmqhZukBwFfIHuhOpe4E7g4qra\nMUfbK4Enq+rX+unjgA1V9WSS44GbgF+sqpvmWLe6EZzVEBZ73JI07ZJQVZlr2aJH7lV1IMnldMG8\nAbiuqnYkuaxbXNcmORn4FPAM4OkkbwbOAU4CbuiCm43AB+YKdknSZC165L5aPHKXpKVZ6MjdSxQl\nqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIa\nZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGG\nuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1KCxwj3J1iQ7\nk+xKcsUcy5+X5LYkTyV561LWlSRNXqpq4QbJBmAXcCHwKLAd2FZVOwdtngWcDvw48HhVvXPcdQfb\nKFi4lskJiz1uSZp2SaiqzLVsnCP3c4HdVbWnqvYD1wMXDRtU1WNVdRfwraWuK0mavHHC/RTgocH0\nw/28cRzOupKkZfKEqiQ1aOMYbR4BThtMn9rPG8cS171qcH9Lf5MkAYxGI0aj0VhtxzmhehTwBbqT\nonuBO4GLq2rHHG2vBJ6sql9bxrpTcUJ106Yz2Ldvz6pUcfLJp/PlLz841XVIml4LnVBdNNz7DWwF\nfpNuGOe6qvqVJJcBVVXXJjkZ+BTwDOBp4EngnKp6cq515+ljKsI9CdYhaT047HBfDYb7dNYhaXod\n7qWQkqR1xnCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIa\nZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGG\nuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaNFa4\nJ9maZGeSXUmumKfNu5LsTnJvkh8azH8wyX1J7kly56QKlyTNb+NiDZJsAN4NXAg8CmxP8tGq2jlo\n81rgzKo6O8nLgd8BNveLnwa2VNXjE69ekjSncY7czwV2V9WeqtoPXA9cNKvNRcAfAFTVHcAJSU7u\nl2XMfiRJEzJO6J4CPDSYfrift1CbRwZtCrg5yfYkly63UEnS+BYdlpmA86pqb5KT6EJ+R1XdOnfT\nqwb3t/Q3SRLAaDRiNBqN1TZVtXCDZDNwVVVt7affBlRVXT1o817glqr6YD+9E7igqvbN2taVwBNV\n9c45+qnuIH81hPkedxKsQ9J6kISqylzLxhmW2Q6cleT0JMcA24AbZ7W5EXhj39lm4GtVtS/JcUm+\nu59/PPBq4LPLfBySpDEtOixTVQeSXA7cRLczuK6qdiS5rFtc11bVx5K8LskXgW8Al/Srnwzc0B2V\nsxH4QFXdtDIPRZI0Y9FhmdXisMx01iFpeh3usIwkaZ0x3CWpQYa7JDXIcJekBhnuWtCmTWeQZFVu\nmzadsdYPV2qGV8scWgfWMX11SDqUV8tI0hHGcJekBhnuktQgw12SGmS4S1KDDHdJapDhrnVhNa+3\n95p7tcDr3A+tA+s40utYuBZpWniduyQdYQx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXVoC\nv0yl9cIvMR1aB9ZxpNcxfy3TUocEfolJapJ/AlEL8cj90DqwjiO9jvlrmZY6Vr8WP0FMI4/cJa0Y\nP0FMJ4/cD60D6zjS65i/lmmpY/Vrmf46jkQeuUtqnp8gDuaR+6F1YB1Heh3z1zItdax+LdYxbh2r\nySN3STrCbFzrAiSpJZs2ncG+fXvWugzDXZImqQv21Rsemo/DMpLUIMNdkhpkuEtSg8YK9yRbk+xM\nsivJFfO0eVeS3UnuTfLipawrSZqsRcM9yQbg3cBrgL8LXJzk+bPavBY4s6rOBi4D3jvuuodnNLlN\nHZbRWhcwMFrrAnqjtS6gN1rrAnqjtS6gN1rrAnqjtS5gYLTWBfRGE93aOEfu5wK7q2pPVe0Hrgcu\nmtXmIuAPAKrqDuCEJCePue5hGE1uU4dltNYFDIzWuoDeaK0L6I3WuoDeaK0L6I3WuoDeaK0LGBit\ndQG90US3Nk64nwI8NJh+uJ83Tptx1pUkTdhKnVCd/+JLSdKKW/S3ZZJsBq6qqq399NuAqqqrB23e\nC9xSVR/sp3cCFwDfv9i6g22s/Q81SNI6M99vy4zzDdXtwFlJTgf2AtuAi2e1uRH4OeCD/c7ga1W1\nL8ljY6y7YIGSpKVbNNyr6kCSy4Gb6IZxrquqHUku6xbXtVX1sSSvS/JF4BvAJQutu2KPRpIETNFP\n/kqSJsdvqGpdS3JCkn/e378gyZ+sdU3TLMkTa9Tvt18nLd1yXrepCPckpyf5myR399OnJvlI/63W\n3Ul+PcnGQfvzk9yRZEeSzye5tJ//w0lum7Xto5J8OcmmJNck2ZvkrYvV0M/7nSSvSPK+JA8kubvv\n7+2z1v3eJP8vyc/Omv9gkvv6b+3enOT7+vnHJrknyVNJThzzOTnQ939vkk/15zaG7d/St3/GYN4F\nSb7Wr/e5JL80WPb6/rm9cf5XZnmSfGnS21zAM4F/MdM1q/uXNNajtXp+hq+Tlm7pr1tVrfkNOB34\n9GD6DuCN/f0Avwdc009vAvYAL+qnTwQ+Bby2b7sHeM5gW68BPjmYfgfw1sVq6Ofd3W/zfcA/7Ocd\nA9wPnD5o98/oTirfMmv9B4Bn9vevAt41x/ITx3xO/mpw/9XAaFb72/safmYw7wLgxv7+scAO4CVz\nLZ/w6/nAKr53/pDuPM/d/fvmFuCP+sf6/kG7l9B9S2Q78HHg5AnW8IvAmwfTvwT8K+Aa4DPAfcDr\nB8/5nwza/tbMe32Vnq+/Wq2+Fnidrp7ruVmDmm7o3w+fAd60Fv0BT/Tvl3uB24CT+vln9NP3Af9p\nOa/bVBy5DyX5EeBvqmrmG68F/GvgkiTH0u3931dV9/XLvwr8AvDv+rZ/RHdVzoxtdG+sb3cxZh3P\nB3b12xyudxzdXvQbg+YXA/8B+NtJnj2rr5n1/hw4c3Y349QyR9sTgK8Oan0ucDTwn4E3zLVyVT1F\n9waaXcNK+Moq9DHjbcD9VfUSuvfBi+mC9RzgzCR/v//U91vAP6qql9HtrH95gjX8PvBGgHR/620b\n3Zf3XlRVLwB+FPiv/be24cj8dDF8ne5g/udmNV3Svx9eBrw5yTNXub8TgeOB26rqxcD/AS7t2/4m\n8NtV9SK6Kw2XbOrCne43aO4azqiqJ4D/C5w113K6I/dz+vt/SH+5ZZJjgNcBH15GHa8FPjGYvibJ\nPX0d11fVY30fp9LtbT8N/DEH71iGtgKfW0YdM76rH17ZAVxLtzefsQ34UHU//XBmkpMGy9LX+Uy6\nN9XnD6OGsVTVy1e6jwXcWVV7+53yvXRHQM8DfhC4uX8N/z3w7Pk3sTRVtQd4LMmL6D5V3Q28kv6g\noqr+gu5Tw8sm1ec6dz7T8dy8Jcm9dJ96TwXOXoP+vllVH+uX30X3fgU4j+7nWgDev5zO1tNfYhrr\nKLeq7kpyfJKz6QL/9qr62jL6ew3wTwfTP19V/y3JccD/TPKnVXU78JN0oU7/7+8D7xysd0uS7wX2\nAy9YRh0z/ro/6pn5Ytn76QILup3ZzG/2fAT4x8B7+ulX9oF2NvC7VXU4O5j14JuD+wfo3uMBPltV\n561gv79HdwnwJrr3wKtnLZ95/34LOGow/9gVrGm9WPXvuCS5APgR4OVV9c0kt7CCr8UC/e0fNJt5\nv0L36W72qMGSTOOR++eBlw5nJPlbwHOAL861vJ8ehtbM0fvsIZmxJPku4ISq+vLsZVX113RHGuf3\nsy6mGzJ6gG7M+wVJhkMfW4DT6PbWlzIB/U7lWUmeleQH6YL7k30Ns78o9r+r6ofodgT/oP+k0ZIn\ngJmTyPP9J/gCcNLMSegkG5OcM0/b5foI3aezlwL/ne4j9k8m2dB/knolcCfdOaEfSHJ0ku8BLpxw\nHYtZqy8LDl+n+Z6b1XQC8HgftM8HNi+2wgr1N9/r8Wd85//xP1lOh1MX7lX1P+iGIH4KuqtdgF+l\nG2d/Cvht4Gf6j8D0R8W/QneSZsb1wE8BrwI+uowyXkV3Ym5oZnhjI/By4P7+08HxVfWcqnpuVX0/\n8F84eNw7VfU03XmDf5Pk+GXU8+3++xqeT/fa/WXf15V9/8+tqlOBZyd5znDlqnqQbhzvHcvsfyr1\n51z+LMmnOfg9AP2RT3W/SPoTwNX9x+J7gFdMuI79dO+ZD1XnBuDTdCfEPkn3ye8vquph4EPAZ+ne\np3fPt80Vsibj/bNep83M8dysckmfAI5O8jm68y9/vsr9zVzVN9/r8Rbg55LcB3zfsnpc6TPEY55F\nnn1lyCl0R8G7gN10oXT0YPn5dHv6Hf3tZ+fY5t3AB+aYfyWLXC1Dd/LthwfL3kd3hczddP8pf6Of\n/w7gl2dt5wXA5/r7B10N0z+OKwbTX2L8q2X29/3f09+29vO/CPydWev+KvDzzLoahu5j4B7g1H56\nRa6WORJvdDvbe+j+rsGa1+PN2zSNuX/7yLSqHgF+bL6GVXUr3W/Fz6v68ellegXdnnNmW5fM08d/\nnGPeZ+hO+lJVz5217M1LrGP4nBw9Tw1nzTHv3w4m/9dg/lN0O41Dtq/lS/IDwJ8CH66q+9e6Hgmm\nZ1jmAN0f+FjRj6hJrqEbv/rGHIsPAN+T5O6qemlVHVjBOo7tT3IeBTw9T7MVfU6SvJ5uiOuri7XV\nwqpqR1WdWVW/sNa1SDP8bRlJatC0HLlLkibIcJekBhnuktQgw12SGmS4S1KD/j9QH5T5+2Zr9gAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111a46f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unigram = NGramLM(oov_train,1)\n",
    "def plot_probabilities(lm, context = (), how_many = 10):    \n",
    "    probs = sorted([(word,lm.probability(word,*context)) for word in lm.vocab], key=lambda x:x[1], reverse=True)[:how_many]\n",
    "    util.plot_bar_graph([prob for _,prob in probs], [word for word, _ in probs])\n",
    "plot_probabilities(unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unigram LM has substantially reduced (and hence better) perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.11302463241343"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(unigram,oov_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also look at the language the unigram LM generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'enough', 'We', \"'\", 'ass', '[OOV]', '[OOV]', 'fine', 'would', 'See']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(unigram, [], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram LM\n",
    "\n",
    "The unigram model ignores any correlation between consecutive words in a sentence. The next best model to overcome this shortcoming is a bigram model. This model conditions the probability of the current word on the previous word. Let us construct such model from the training data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1116df8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigram = NGramLM(oov_train,2)\n",
    "plot_probabilities(bigram, ('I',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a more peaked distribution conditioned on \"I\" than in the case of the unigram model. Let us see how the bigram LM generates language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[BAR] But they [OOV] the world of [OOV] [OOV] [/BAR] [BAR] You got you let nobody [OOV] ' [OOV] [/BAR] [BAR] To the feel and [OOV] on straight [/BAR] [BAR] [OOV]\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(sample(bigram, ['[BAR]'], 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the bigram model improve perplexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(bigram,oov_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the bigram model has the problem we tried to avoid using the OOV preprocessing method above. The problem is that there are contexts in which the OOV word (and other words) hasn't been seen, and hence it receives 0 probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.probability(\"[OOV]\",\"money\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing\n",
    "\n",
    "The general problem is that maximum likelhood estimates will always underestimate the true probability of some words, and in turn overestimate the (context-dependent) probabilities of other words. To overcome this issue we aim to _smooth_ the probabilities and move mass from seen events to unseen events.\n",
    "\n",
    "### Laplace Smoothing\n",
    "\n",
    "The easiest way to overcome the problem of zero probabilities is to simply add pseudo counts to each event in the dataset (in a Bayesian setting this amounts to a maximum posteriori estimate under a dirichlet prior on parameters).\n",
    "\n",
    "$$\n",
    "\\param^{\\alpha}_{w,h} = \\frac{\\counts{\\train}{h,w} + \\alpha}{\\counts{\\train}{h} + \\alpha \\lvert V \\rvert } \n",
    "$$\n",
    "\n",
    "Let us implement this in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010660980810234541"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LaplaceLM(CountLM):\n",
    "    def __init__(self, base_lm, alpha):\n",
    "        super().__init__(base_lm.vocab, base_lm.order)\n",
    "        self.base_lm = base_lm\n",
    "        self.alpha = alpha\n",
    "    def counts(self, word_and_history):\n",
    "        return self.base_lm.counts(word_and_history) + self.alpha\n",
    "    def norm(self, history):\n",
    "        return self.base_lm.norm(history) + self.alpha * len(self.base_lm.vocab)\n",
    "\n",
    "laplace_bigram = LaplaceLM(bigram, 0.1) \n",
    "laplace_bigram.probability(\"[OOV]\",\"money\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give a better perplexity value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.51634219903197"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(laplace_bigram,oov_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted counts\n",
    "It is often useful to think of smoothing algorithms as un-smoothed Maximum-Likelhood estimators that work with *adjusted* n-gram counts in the numerator, and fixed history counts in the denominator. This allows us to see how counts from high-frequency words are reduced, and counts of unseen words increased. If these changes are too big, the smoothing method is likely not very effective.\n",
    "\n",
    "Let us reformulate the laplace LM using adjusted counts. Note that we since we have histories with count 0, we do need to increase the original denominator by a small \\\\(\\epsilon\\\\) to avoid division by zero. \n",
    "$$\n",
    "\\begin{split}\n",
    "\\counts{\\train,\\alpha}{h,w} &= \\param^{\\alpha}_{w,h} \\cdot (\\counts{\\train}{h} +  \\epsilon)\\\\\\\\\n",
    "\\counts{\\train,\\alpha}{h} &= \\counts{\\train}{h} + \\epsilon\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585.0, 564.5934362811013)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AdjustedLaplaceLM(CountLM):\n",
    "    def __init__(self, base_lm, alpha):\n",
    "        super().__init__(base_lm.vocab, base_lm.order)\n",
    "        self.base_lm = base_lm\n",
    "        self.alpha = alpha\n",
    "        self.eps = 0.000001\n",
    "    def counts(self, word_and_history):\n",
    "        history = word_and_history[1:]\n",
    "        word = word_and_history[0]\n",
    "        return 0.0 if word not in self.vocab else \\\n",
    "               (self.base_lm.counts(word_and_history) + self.alpha) / \\\n",
    "               (self.base_lm.norm(history) + self.alpha * len(self.base_lm.vocab)) * \\\n",
    "               (self.base_lm.norm(history) + self.eps)\n",
    "    def norm(self, history):\n",
    "        return self.base_lm.norm(history) + self.eps\n",
    "\n",
    "adjusted_laplace_bigram = AdjustedLaplaceLM(bigram, 0.1)\n",
    "bigram.counts((OOV,OOV)), adjusted_laplace_bigram.counts((OOV,OOV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that for high frequency words the absolute counts are altered quite substantially. This is unfortunate because for high frequency words we would expect the counts to be relatively accurate. Can we test more generally wether our adjusted counts are sensible?\n",
    "\n",
    "One option is to compare the adjusted counts to average counts in a held-out set. For example, for words of count 0 in the training set, how does their average count in the held-out set compare to their adjusted count in the smoothed model? To test this we need some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_counts(train_lm, test_lm, vocab):\n",
    "    \"\"\"\n",
    "    Calculate a dictionary from counts in the training-LM to counts in the test-LM. \n",
    "    \"\"\"\n",
    "    avg_test_counts = collections.defaultdict(float)\n",
    "    norm = collections.defaultdict(float)\n",
    "    for ngram in util.cross_product([list(train_lm.vocab)] * train_lm.order):\n",
    "        train_count = train_lm.counts(ngram)\n",
    "        test_count = test_lm.counts(ngram)\n",
    "        avg_test_counts[train_count] += test_count\n",
    "        norm[train_count] += 1.0\n",
    "    for c in avg_test_counts.keys():\n",
    "        avg_test_counts[c] /= norm[c]\n",
    "    return avg_test_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate a table of training counts, test counts, and smoothed counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0.0041     0.0059\n",
      "1     0.4125     0.3034\n",
      "2     1.0836     0.7959\n",
      "3     1.7626     1.2479\n",
      "4     2.9123     1.7678\n",
      "5     3.6724     2.4055\n"
     ]
    }
   ],
   "source": [
    "test_bigram = NGramLM(oov_test, 2)\n",
    "joint_vocab = set(oov_test + oov_train)\n",
    "avg_test_counts = avg_counts(bigram, test_bigram, joint_vocab)\n",
    "avg_laplace_counts = avg_counts(bigram, AdjustedLaplaceLM(bigram, 0.1), joint_vocab)\n",
    "for count in range(0, 6):\n",
    "    print(\"{} {:10.4f} {:10.4f}\".format(count, avg_test_counts[count], avg_laplace_counts[count]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "For a given context the smoothing methods discussed above shift mass uniformly across the words that haven't been seen in this context. This makes sense when the words are not in the vocabularly. However, when words are in the vocabularly but just have not been seen in the given context, we can do better because we can leverage statistics about the word from other contexts. In particular, we can *back-off* to the statistics of \\\\(n-1\\\\) grams. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0010548523206751054, 0.0010548523206751054)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_laplace_bigram.probability('skies','skies'), adjusted_laplace_bigram.probability('[/BAR]','skies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple technique to use the \\\\(n-1\\\\) gram statistics is interpolation. Here we  compose the probability of a word as the weighted sum of the probability of an \\\\(n\\\\)-gram model \\\\(p'\\\\) and a back-off \\\\(n-1\\\\) model \\\\(p''\\\\): \n",
    "\n",
    "$$\n",
    "\\prob_{\\alpha}(w_i|w_{i-n},\\ldots,w_{i-1}) = \\alpha \\cdot \\prob'(w_i|w_{i-n},\\ldots,w_{i-1}) + (1 - \\alpha) \\cdot \\prob''(w_i|w_{i-n+1},\\ldots,w_{i-1})\n",
    "$$\n",
    "\n",
    "A Python implementation of this model can be seen below. We also show how a more likely unigram now has a higher probability in a context it hasn't seen in before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0001662701118815446, 0.09764798462230231)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class InterpolatedLM(LanguageModel):\n",
    "    def __init__(self, main, backoff, alpha):\n",
    "        super().__init__(main.vocab, main.order)\n",
    "        self.main = main\n",
    "        self.backoff = backoff\n",
    "        self.alpha = alpha\n",
    "    def probability(self, word, *history):\n",
    "        return self.alpha * self.main.probability(word,*history) + \\\n",
    "               (1.0 - self.alpha) * self.backoff.probability(word,*history)\n",
    "\n",
    "interpolated = InterpolatedLM(adjusted_laplace_bigram,unigram,0.01)\n",
    "interpolated.probability('skies','skies'), interpolated.probability('[/BAR]','skies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now find a good $\\alpha$ parameter to optimise for perplexity. Notice that in practice this should be done using a development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "<style>\n",
       "\n",
       "</style>\n",
       "\n",
       "<div id=\"fig_el5466445844449846828152832\"></div>\n",
       "<script>\n",
       "function mpld3_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(mpld3) !== \"undefined\" && mpld3._mpld3IsLoaded){\n",
       "   // already loaded: just create the figure\n",
       "   !function(mpld3){\n",
       "       \n",
       "       mpld3.draw_figure(\"fig_el5466445844449846828152832\", {\"width\": 480.0, \"plugins\": [{\"type\": \"reset\"}, {\"button\": true, \"enabled\": false, \"type\": \"zoom\"}, {\"button\": true, \"enabled\": false, \"type\": \"boxzoom\"}], \"id\": \"el546644584444984\", \"axes\": [{\"lines\": [{\"coordinates\": \"data\", \"xindex\": 0, \"dasharray\": \"10,0\", \"color\": \"#0000FF\", \"id\": \"el546644584254600\", \"alpha\": 1, \"zorder\": 2, \"data\": \"data01\", \"yindex\": 1, \"linewidth\": 1.0}], \"images\": [], \"xdomain\": [0.0, 1.0], \"ylim\": [45.0, 80.0], \"axesbg\": \"#FFFFFF\", \"yscale\": \"linear\", \"id\": \"el546644584447448\", \"zoomable\": true, \"xlim\": [0.0, 1.0], \"collections\": [], \"ydomain\": [45.0, 80.0], \"axesbgalpha\": null, \"markers\": [], \"sharex\": [], \"paths\": [], \"xscale\": \"linear\", \"axes\": [{\"grid\": {\"gridOn\": false}, \"tickformat\": null, \"position\": \"bottom\", \"tickvalues\": null, \"nticks\": 6, \"scale\": \"linear\", \"fontsize\": 10.0}, {\"grid\": {\"gridOn\": false}, \"tickformat\": null, \"position\": \"left\", \"tickvalues\": null, \"nticks\": 8, \"scale\": \"linear\", \"fontsize\": 10.0}], \"texts\": [], \"bbox\": [0.125, 0.125, 0.775, 0.775], \"sharey\": []}], \"data\": {\"data01\": [[0.0, 78.12529362403502], [0.1, 64.18965602468867], [0.2, 57.547909002234285], [0.30000000000000004, 53.482031938971595], [0.4, 50.818123773770786], [0.5, 49.095724655180675], [0.6000000000000001, 48.124642575720316], [0.7000000000000001, 47.875434794372126], [0.8, 48.500856130800784], [0.9, 50.60716743812544], [1.0, 59.51634219903197]]}, \"height\": 320.0});\n",
       "   }(mpld3);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/mpld3\n",
       "   require.config({paths: {d3: \"https://mpld3.github.io/js/d3.v3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      mpld3_load_lib(\"https://mpld3.github.io/js/mpld3.v0.2.js\", function(){\n",
       "         \n",
       "         mpld3.draw_figure(\"fig_el5466445844449846828152832\", {\"width\": 480.0, \"plugins\": [{\"type\": \"reset\"}, {\"button\": true, \"enabled\": false, \"type\": \"zoom\"}, {\"button\": true, \"enabled\": false, \"type\": \"boxzoom\"}], \"id\": \"el546644584444984\", \"axes\": [{\"lines\": [{\"coordinates\": \"data\", \"xindex\": 0, \"dasharray\": \"10,0\", \"color\": \"#0000FF\", \"id\": \"el546644584254600\", \"alpha\": 1, \"zorder\": 2, \"data\": \"data01\", \"yindex\": 1, \"linewidth\": 1.0}], \"images\": [], \"xdomain\": [0.0, 1.0], \"ylim\": [45.0, 80.0], \"axesbg\": \"#FFFFFF\", \"yscale\": \"linear\", \"id\": \"el546644584447448\", \"zoomable\": true, \"xlim\": [0.0, 1.0], \"collections\": [], \"ydomain\": [45.0, 80.0], \"axesbgalpha\": null, \"markers\": [], \"sharex\": [], \"paths\": [], \"xscale\": \"linear\", \"axes\": [{\"grid\": {\"gridOn\": false}, \"tickformat\": null, \"position\": \"bottom\", \"tickvalues\": null, \"nticks\": 6, \"scale\": \"linear\", \"fontsize\": 10.0}, {\"grid\": {\"gridOn\": false}, \"tickformat\": null, \"position\": \"left\", \"tickvalues\": null, \"nticks\": 8, \"scale\": \"linear\", \"fontsize\": 10.0}], \"texts\": [], \"bbox\": [0.125, 0.125, 0.775, 0.775], \"sharey\": []}], \"data\": {\"data01\": [[0.0, 78.12529362403502], [0.1, 64.18965602468867], [0.2, 57.547909002234285], [0.30000000000000004, 53.482031938971595], [0.4, 50.818123773770786], [0.5, 49.095724655180675], [0.6000000000000001, 48.124642575720316], [0.7000000000000001, 47.875434794372126], [0.8, 48.500856130800784], [0.9, 50.60716743812544], [1.0, 59.51634219903197]]}, \"height\": 320.0});\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & mpld3\n",
       "    mpld3_load_lib(\"https://mpld3.github.io/js/d3.v3.min.js\", function(){\n",
       "         mpld3_load_lib(\"https://mpld3.github.io/js/mpld3.v0.2.js\", function(){\n",
       "                 \n",
       "                 mpld3.draw_figure(\"fig_el5466445844449846828152832\", {\"width\": 480.0, \"plugins\": [{\"type\": \"reset\"}, {\"button\": true, \"enabled\": false, \"type\": \"zoom\"}, {\"button\": true, \"enabled\": false, \"type\": \"boxzoom\"}], \"id\": \"el546644584444984\", \"axes\": [{\"lines\": [{\"coordinates\": \"data\", \"xindex\": 0, \"dasharray\": \"10,0\", \"color\": \"#0000FF\", \"id\": \"el546644584254600\", \"alpha\": 1, \"zorder\": 2, \"data\": \"data01\", \"yindex\": 1, \"linewidth\": 1.0}], \"images\": [], \"xdomain\": [0.0, 1.0], \"ylim\": [45.0, 80.0], \"axesbg\": \"#FFFFFF\", \"yscale\": \"linear\", \"id\": \"el546644584447448\", \"zoomable\": true, \"xlim\": [0.0, 1.0], \"collections\": [], \"ydomain\": [45.0, 80.0], \"axesbgalpha\": null, \"markers\": [], \"sharex\": [], \"paths\": [], \"xscale\": \"linear\", \"axes\": [{\"grid\": {\"gridOn\": false}, \"tickformat\": null, \"position\": \"bottom\", \"tickvalues\": null, \"nticks\": 6, \"scale\": \"linear\", \"fontsize\": 10.0}, {\"grid\": {\"gridOn\": false}, \"tickformat\": null, \"position\": \"left\", \"tickvalues\": null, \"nticks\": 8, \"scale\": \"linear\", \"fontsize\": 10.0}], \"texts\": [], \"bbox\": [0.125, 0.125, 0.775, 0.775], \"sharey\": []}], \"data\": {\"data01\": [[0.0, 78.12529362403502], [0.1, 64.18965602468867], [0.2, 57.547909002234285], [0.30000000000000004, 53.482031938971595], [0.4, 50.818123773770786], [0.5, 49.095724655180675], [0.6000000000000001, 48.124642575720316], [0.7000000000000001, 47.875434794372126], [0.8, 48.500856130800784], [0.9, 50.60716743812544], [1.0, 59.51634219903197]]}, \"height\": 320.0});\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = np.arange(0,1.1,0.1)\n",
    "perplexities = [perplexity(InterpolatedLM(adjusted_laplace_bigram,unigram,alpha),oov_test) for alpha in alphas]\n",
    "fig = plt.figure()\n",
    "plt.plot(alphas,perplexities)\n",
    "mpld3.display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backoff \n",
    "Instead of combining probabilities for all words given a context, it makes sense to back-off only when no counts for a given event are available and rely on available counts where possible. \n",
    "\n",
    "A particularly simple, if not to say stupid, backoff method is [Stupid Backoff](http://www.aclweb.org/anthology/D07-1090.pdf). Let \\\\(w\\\\) be a word and \\\\(h_{n}\\\\) be an n-gram of length \\\\(n\\\\):  \n",
    "\n",
    "$$\n",
    "\\prob_{\\mbox{Stupid}}(w|h_n) = \n",
    "\\begin{cases}\n",
    "\\frac{\\counts{\\train}{h_n,w}}{\\counts{\\train}{h_n}}  &= \\mbox{if }\\counts{\\train}{h_n,w} > 0 \\\\\\\\\n",
    "\\prob_{\\mbox{Stupid}}(w|h_{n-1}) & \\mbox{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class StupidBackoff(LanguageModel):\n",
    "    def __init__(self, main, backoff, alpha):\n",
    "        super().__init__(main.vocab, main.order)\n",
    "        self.main = main\n",
    "        self.backoff = backoff\n",
    "        self.alpha = alpha\n",
    "    def probability(self, word, *history):\n",
    "        return self.main.probability(word,*history) \\\n",
    "          if self.main.counts((word,)+tuple(history)) > 0 \\\n",
    "          else self.alpha * self.backoff.probability(word,*history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the Stupid LM is very effective when it comes to *extrinsic* evaluations, but it doesn't represent a valid probability distribution: when you sum over the probabilities of all words given a history, the result may be larger than 1. This is the case because the main n-gram model probabilities for all non-zero count words already sum to 1. The fact that the probabilities sum to more than 1 makes perplexity values meaningless. The code below illustrates the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0711443177349649"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stupid = StupidBackoff(bigram, unigram, 0.1)\n",
    "sum([stupid.probability(word, 'the') for word in stupid.vocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are several \"proper backoff models\" that do not have this problem, e.g. the Katz-Backoff method. We refer to other material below for a deeper discussion of these.\n",
    "\n",
    "### Background Reading\n",
    "\n",
    "* Jurafsky & Martin, Speech and Language Processing: Chapter 4, N-Grams.\n",
    "* Bill MacCartney, Stanford NLP Lunch Tutorial: [Smoothing](http://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
