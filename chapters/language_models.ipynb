{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# %cd .. \n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "Language models (LMs) calculate the probability to see a given sequence of words, as defined through a [tokenization](todo) algorithm, in a given language or sub-language/domain/genre. For example, an English language model may assign a higher probability to seeing the sequence \"How are you?\" than to \"Wassup' dawg?\", and for a hip-hop language model this proportion may be reversed. <span class=\"summary\">Language models (LMs) calculate the probability to see a given sequence of words.\n",
    "\n",
    "There are several use cases for such models: \n",
    "\n",
    "* To filter out bad translations in machine translation.\n",
    "* To rank speech recognition output. \n",
    "* In concept-to-text generation.\n",
    "\n",
    "More formally, a language model is a stochastic process that models the probability \\\\(\\prob(w_1,\\ldots,w_d)\\\\) of observing sequences of words \\\\(w_1,\\ldots,w_d\\\\). We can, without loss of generality, decompose the probability of such sequences into  \n",
    "\n",
    "$$\n",
    "\\prob(w_1,\\ldots,w_d) = \\prob(w_1) \\prod_{i = 2}^d \\prob(w_i|w_1,\\ldots,w_{i-1}).\n",
    "$$\n",
    "\n",
    "This means that a language model can be defined by how it models the conditional probablity $\\prob(w_i|w_1,\\ldots,w_{i-1})$ of seeing a word \\\\(w_i\\\\) after having seen the *history* of previous words $w_1,\\ldots,w_{i-1}$. We also have to model the prior probability $\\prob(w_1)$, but it is easy to reduce this prior to a conditional probability as well.\n",
    "\n",
    "In practice it is common to define language models based on *equivalence classes* of histories instead of having different conditional distributions for each possible history. This overcomes sparsity and efficiency problems when working with full histories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Language Models\n",
    "\n",
    "The most common type of equivalence class relies on *truncating* histories $w_1,\\ldots,w_{i-1}$ to length $n-1$:\n",
    "$$\n",
    "\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\prob(w_i|w_{i-n},\\ldots,w_{i-1}).\n",
    "$$\n",
    "\n",
    "That is, the probability of a word only depends on the last $n-1$ previous words. We will refer to such model as a *n-gram language model*.\n",
    "\n",
    "## A Uniform Baseline LM\n",
    "\n",
    "*Unigram* models are the simplest 1-gram language models. That is, they model the conditional probability of word using the prior probability of seeing that word:\n",
    "$$\n",
    "\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\prob(w_i).\n",
    "$$\n",
    "\n",
    "To setup datasets and as baseline for more complex language models, we first introduce the simplest instantituation of a unigram model: a *uniform* language model which assigns the same prior probability to each word. That is, given a *vocabulary* of words \\\\(\\vocab\\\\), the uniform LM is defined as:\n",
    "\n",
    "$$\n",
    "\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\frac{1}{|\\vocab|}.\n",
    "$$\n",
    "\n",
    "Let us \"train\" and test such a language model on the OHHLA corpus. First we need to load this corpus. Below we focus on a subset to make our code more responsive and to allow us to test models more quickly. Check the [loading from OHHLA](load_ohhla.ipynb) notebook to see how `load_albums` and `words` are defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[BAR] Can ' t even call this a blues song [/BAR] [BAR] It ' s been so long [/BAR] [BAR] Neither one of us was wrong or anything like that [/BAR] [BAR] It seems like\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statnlpbook.util as util\n",
    "util.execute_notebook('load_ohhla.ipynb')\n",
    "docs = load_albums(j_live)\n",
    "trainDocs, testDocs = docs[:len(docs)//2], docs[len(docs)//2:] \n",
    "train = words(trainDocs)\n",
    "test = words(testDocs)\n",
    "\" \".join(train[0:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a uniform language model. Language models in this book implement the `LanguageModel` [abstract base class](https://docs.python.org/3/library/abc.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import abc \n",
    "class LanguageModel(metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocab: the vocabulary underlying this language model. Should be a set of words.\n",
    "        order: history length (-1).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab, order):\n",
    "        self.vocab = vocab\n",
    "        self.order = order\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def probability(self, word,*history):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word: the word we need the probability of\n",
    "            history: words to condition on.\n",
    "        Returns:\n",
    "            the probability p(w|history)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important method we have to provide is `probability(word,history)` which returns the probability of a word given a history. Let us implement a uniform LM using this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003912363067292645"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class UniformLM(LanguageModel):\n",
    "    \"\"\"\n",
    "    A uniform language model that assigns the same probability to each word in the vocabulary. \n",
    "    \"\"\"\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__(vocab, 1)\n",
    "    def probability(self, word,*history):\n",
    "        return 1.0 / len(self.vocab) if word in self.vocab else 0.0\n",
    "    \n",
    "vocab = set(train)\n",
    "baseline = UniformLM(vocab)\n",
    "baseline.probability(\"call\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "It is instructive and easy to sample language from a language model. In many, but not all, cases the more natural the generated language of an LM looks, the better this LM is.\n",
    "\n",
    "To sample from an LM one simply needs to iteratively sample from the LM conditional probability over words, and add newly sampled words to the next history. The only challenge in implementing this is to sample from a categorical distribution over words. Here we provide this functionality via `np.random.choice` from [numpy](http://www.numpy.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style',\n",
       " 'earths',\n",
       " 'worse',\n",
       " 'wha',\n",
       " 'pitch',\n",
       " 'animocity',\n",
       " 'OUT',\n",
       " 'sins',\n",
       " 'rock',\n",
       " 'dimples']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample(lm, init, amount):\n",
    "    \"\"\"\n",
    "    Sample from a language model.\n",
    "    Args:\n",
    "        lm: the language model\n",
    "        init: the initial sequence of words to condition on\n",
    "        amount: how long should the sampled sequence be\n",
    "    \"\"\"\n",
    "    words = list(lm.vocab)\n",
    "    result = []\n",
    "    result += init\n",
    "    for _ in range(0, amount):\n",
    "        history = result[-(lm.order-1):]\n",
    "        probs = [lm.probability(word, *history) for word in words]\n",
    "        sampled = np.random.choice(words,p=probs)\n",
    "        result.append(sampled)\n",
    "    return result\n",
    "\n",
    "sample(baseline, [], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "How do we determine the quality of an (n-gram) LM? One way is through *extrinsic* evaluation: assess how much the LM improves performance on *downstream tasks* such as machine translation or speech recognition. Arguably this is the most important measure of LM quality, but it can be costly as re-training such systems may take days, and when we seek to develop general-purpose LMs we may have to evaluate performance on several tasks. This is problematic when one wants to iteratively improve LMs and test new models and parameters. It is hence useful to find *intrinsic* means of evaluation that assess the stand-alone quality of LMs with minimal overhead.\n",
    "\n",
    "One intrinsic way is to measure how well the LM plays the \"Shannon Game\": Predict what the next word in actual context should be, and win if your predictions match the words in an actual corpus. This can be formalized  using the notion of *perplexity* of the LM on a given dataset. Given a test sequence \\\\(w_1,\\ldots,w_T\\\\) of \\\\(T\\\\) words, we calculate the perplexity \\\\(\\perplexity\\\\) as follows:\n",
    "\n",
    "$$\n",
    "\\perplexity(w_1,\\ldots,w_T) = \\prob(w_1,\\ldots,w_T)^{-\\frac{1}{T}} = \\sqrt[T]{\\prod_i^T \\frac{1}{\\prob(w_i|w_{i-n},\\ldots,w_{i-1})}}\n",
    "$$\n",
    "\n",
    "We can implement a perplexity function based on the `LanguageModel` interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def perplexity(lm, data):\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of the language model given the provided data.\n",
    "    Args:\n",
    "        lm: a language model.\n",
    "        data: the data to calculate perplexity on.\n",
    "\n",
    "    Returns:\n",
    "        the perplexity of `lm` on `data`.\n",
    "\n",
    "    \"\"\"\n",
    "    log_prob = 0.0\n",
    "    history_order = lm.order - 1\n",
    "    for i in range(history_order, len(data)):\n",
    "        history = data[i - history_order : i]\n",
    "        word = data[i]\n",
    "        p = lm.probability(word, *history)\n",
    "        log_prob += math.log(p) if p > 0.0 else float(\"-inf\")\n",
    "    return math.exp(-log_prob / (len(data) - history_order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the uniform model does on our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(baseline, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Vocabularly Words\n",
    "The problem in the above example is that the baseline model assigns zero probability to words that are not in the vocabulary. Test sets will usually contain such words, and this leads to the above result of infinite perplexity. For example, the following three words do not appear in the training set vocabulary `vocab` and hence receive 0 probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('send', 0.0), ('corrections', 0.0), ('typist', 0.0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(w,baseline.probability(w)) for w in test if w not in vocab][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Long Tail\n",
    "The fact that we regularly encounter new words in our corpus is a common phenomenon not specific to our corpus. Generally we will see a few words that appear repeatedly, and a long tail of words that appear only a few times. While each individual long-tail word is rare, the probability of seeing any long-tail word is quite high (the long tail covers a lot of the frequency mass).\n",
    "\n",
    "Let us observe this phenomenon for our data: we will rank the words according to their frequency, and plot this frequency against the rank. Let us first extracted the sorted counts and their ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "counts = collections.defaultdict(int)\n",
    "for word in train:\n",
    "    counts[word] += 1\n",
    "sorted_counts = sorted(counts.values(),reverse=True)\n",
    "ranks = range(1,len(sorted_counts)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the counts against their rank. Play around with the x and y scale and change them to `'log'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "<style>\n",
       "\n",
       "</style>\n",
       "\n",
       "<div id=\"fig_el251401710293530243407788272\"></div>\n",
       "<script>\n",
       "function mpld3_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(mpld3) !== \"undefined\" && mpld3._mpld3IsLoaded){\n",
       "   // already loaded: just create the figure\n",
       "   !function(mpld3){\n",
       "       \n",
       "       mpld3.draw_figure(\"fig_el251401710293530243407788272\", {\"plugins\": [{\"type\": \"reset\"}, {\"button\": true, \"type\": \"zoom\", \"enabled\": false}, {\"button\": true, \"type\": \"boxzoom\", \"enabled\": false}], \"height\": 320.0, \"id\": \"el25140171029353024\", \"data\": {\"data01\": [[1.0, 1255.0], [2.0, 1255.0], [3.0, 509.0], [4.0, 390.0], [5.0, 265.0], [6.0, 243.0], [7.0, 208.0], [8.0, 187.0], [9.0, 146.0], [10.0, 142.0], [11.0, 136.0], [12.0, 113.0], [13.0, 109.0], [14.0, 103.0], [15.0, 98.0], [16.0, 94.0], [17.0, 88.0], [18.0, 82.0], [19.0, 71.0], [20.0, 67.0], [21.0, 65.0], [22.0, 61.0], [23.0, 60.0], [24.0, 58.0], [25.0, 58.0], [26.0, 57.0], [27.0, 56.0], [28.0, 55.0], [29.0, 55.0], [30.0, 53.0], [31.0, 50.0], [32.0, 47.0], [33.0, 45.0], [34.0, 45.0], [35.0, 45.0], [36.0, 44.0], [37.0, 42.0], [38.0, 42.0], [39.0, 41.0], [40.0, 40.0], [41.0, 40.0], [42.0, 40.0], [43.0, 40.0], [44.0, 36.0], [45.0, 34.0], [46.0, 34.0], [47.0, 32.0], [48.0, 32.0], [49.0, 30.0], [50.0, 29.0], [51.0, 29.0], [52.0, 28.0], [53.0, 27.0], [54.0, 26.0], [55.0, 26.0], [56.0, 26.0], [57.0, 25.0], [58.0, 25.0], [59.0, 25.0], [60.0, 25.0], [61.0, 24.0], [62.0, 24.0], [63.0, 23.0], [64.0, 22.0], [65.0, 22.0], [66.0, 21.0], [67.0, 21.0], [68.0, 21.0], [69.0, 20.0], [70.0, 19.0], [71.0, 19.0], [72.0, 19.0], [73.0, 18.0], [74.0, 18.0], [75.0, 18.0], [76.0, 18.0], [77.0, 18.0], [78.0, 17.0], [79.0, 17.0], [80.0, 17.0], [81.0, 17.0], [82.0, 17.0], [83.0, 17.0], [84.0, 16.0], [85.0, 16.0], [86.0, 16.0], [87.0, 16.0], [88.0, 16.0], [89.0, 16.0], [90.0, 16.0], [91.0, 16.0], [92.0, 16.0], [93.0, 16.0], [94.0, 15.0], [95.0, 15.0], [96.0, 15.0], [97.0, 15.0], [98.0, 14.0], [99.0, 14.0], [100.0, 14.0], [101.0, 14.0], [102.0, 14.0], [103.0, 14.0], [104.0, 14.0], [105.0, 13.0], [106.0, 13.0], [107.0, 13.0], [108.0, 13.0], [109.0, 13.0], [110.0, 13.0], [111.0, 13.0], [112.0, 13.0], [113.0, 13.0], [114.0, 12.0], [115.0, 12.0], [116.0, 12.0], [117.0, 12.0], [118.0, 12.0], [119.0, 11.0], [120.0, 11.0], [121.0, 11.0], [122.0, 11.0], [123.0, 11.0], [124.0, 11.0], [125.0, 11.0], [126.0, 11.0], [127.0, 11.0], [128.0, 11.0], [129.0, 11.0], [130.0, 11.0], [131.0, 11.0], [132.0, 11.0], [133.0, 11.0], [134.0, 11.0], [135.0, 11.0], [136.0, 11.0], [137.0, 10.0], [138.0, 10.0], [139.0, 10.0], [140.0, 10.0], [141.0, 10.0], [142.0, 10.0], [143.0, 10.0], [144.0, 10.0], [145.0, 10.0], [146.0, 10.0], [147.0, 10.0], [148.0, 10.0], [149.0, 10.0], [150.0, 10.0], [151.0, 10.0], [152.0, 9.0], [153.0, 9.0], [154.0, 9.0], [155.0, 9.0], [156.0, 9.0], [157.0, 9.0], [158.0, 9.0], [159.0, 9.0], [160.0, 9.0], [161.0, 9.0], [162.0, 9.0], [163.0, 9.0], [164.0, 9.0], [165.0, 9.0], [166.0, 9.0], [167.0, 9.0], [168.0, 9.0], [169.0, 9.0], [170.0, 8.0], [171.0, 8.0], [172.0, 8.0], [173.0, 8.0], [174.0, 8.0], [175.0, 8.0], [176.0, 8.0], [177.0, 8.0], [178.0, 8.0], [179.0, 8.0], [180.0, 8.0], [181.0, 8.0], [182.0, 8.0], [183.0, 8.0], [184.0, 8.0], [185.0, 8.0], [186.0, 8.0], [187.0, 8.0], [188.0, 8.0], [189.0, 7.0], [190.0, 7.0], [191.0, 7.0], [192.0, 7.0], [193.0, 7.0], [194.0, 7.0], [195.0, 7.0], [196.0, 7.0], [197.0, 7.0], [198.0, 7.0], [199.0, 7.0], [200.0, 7.0], [201.0, 7.0], [202.0, 7.0], [203.0, 7.0], [204.0, 7.0], [205.0, 7.0], [206.0, 7.0], [207.0, 7.0], [208.0, 7.0], [209.0, 7.0], [210.0, 7.0], [211.0, 7.0], [212.0, 7.0], [213.0, 7.0], [214.0, 7.0], [215.0, 7.0], [216.0, 7.0], [217.0, 7.0], [218.0, 7.0], [219.0, 7.0], [220.0, 6.0], [221.0, 6.0], [222.0, 6.0], [223.0, 6.0], [224.0, 6.0], [225.0, 6.0], [226.0, 6.0], [227.0, 6.0], [228.0, 6.0], [229.0, 6.0], [230.0, 6.0], [231.0, 6.0], [232.0, 6.0], [233.0, 6.0], [234.0, 6.0], [235.0, 6.0], [236.0, 6.0], [237.0, 6.0], [238.0, 6.0], [239.0, 6.0], [240.0, 6.0], [241.0, 6.0], [242.0, 6.0], [243.0, 6.0], [244.0, 6.0], [245.0, 6.0], [246.0, 6.0], [247.0, 6.0], [248.0, 6.0], [249.0, 6.0], [250.0, 6.0], [251.0, 6.0], [252.0, 6.0], [253.0, 6.0], [254.0, 6.0], [255.0, 6.0], [256.0, 6.0], [257.0, 6.0], [258.0, 5.0], [259.0, 5.0], [260.0, 5.0], [261.0, 5.0], [262.0, 5.0], [263.0, 5.0], [264.0, 5.0], [265.0, 5.0], [266.0, 5.0], [267.0, 5.0], [268.0, 5.0], [269.0, 5.0], [270.0, 5.0], [271.0, 5.0], [272.0, 5.0], [273.0, 5.0], [274.0, 5.0], [275.0, 5.0], [276.0, 5.0], [277.0, 5.0], [278.0, 5.0], [279.0, 5.0], [280.0, 5.0], [281.0, 5.0], [282.0, 5.0], [283.0, 5.0], [284.0, 5.0], [285.0, 5.0], [286.0, 5.0], [287.0, 5.0], [288.0, 5.0], [289.0, 5.0], [290.0, 5.0], [291.0, 5.0], [292.0, 5.0], [293.0, 5.0], [294.0, 5.0], [295.0, 5.0], [296.0, 5.0], [297.0, 5.0], [298.0, 5.0], [299.0, 5.0], [300.0, 5.0], [301.0, 5.0], [302.0, 5.0], [303.0, 5.0], [304.0, 5.0], [305.0, 5.0], [306.0, 5.0], [307.0, 5.0], [308.0, 5.0], [309.0, 5.0], [310.0, 5.0], [311.0, 5.0], [312.0, 5.0], [313.0, 5.0], [314.0, 5.0], [315.0, 5.0], [316.0, 5.0], [317.0, 5.0], [318.0, 4.0], [319.0, 4.0], [320.0, 4.0], [321.0, 4.0], [322.0, 4.0], [323.0, 4.0], [324.0, 4.0], [325.0, 4.0], [326.0, 4.0], [327.0, 4.0], [328.0, 4.0], [329.0, 4.0], [330.0, 4.0], [331.0, 4.0], [332.0, 4.0], [333.0, 4.0], [334.0, 4.0], [335.0, 4.0], [336.0, 4.0], [337.0, 4.0], [338.0, 4.0], [339.0, 4.0], [340.0, 4.0], [341.0, 4.0], [342.0, 4.0], [343.0, 4.0], [344.0, 4.0], [345.0, 4.0], [346.0, 4.0], [347.0, 4.0], [348.0, 4.0], [349.0, 4.0], [350.0, 4.0], [351.0, 4.0], [352.0, 4.0], [353.0, 4.0], [354.0, 4.0], [355.0, 4.0], [356.0, 4.0], [357.0, 4.0], [358.0, 4.0], [359.0, 4.0], [360.0, 4.0], [361.0, 4.0], [362.0, 4.0], [363.0, 4.0], [364.0, 4.0], [365.0, 4.0], [366.0, 4.0], [367.0, 4.0], [368.0, 4.0], [369.0, 4.0], [370.0, 4.0], [371.0, 4.0], [372.0, 4.0], [373.0, 4.0], [374.0, 4.0], [375.0, 4.0], [376.0, 4.0], [377.0, 4.0], [378.0, 4.0], [379.0, 4.0], [380.0, 4.0], [381.0, 4.0], [382.0, 4.0], [383.0, 4.0], [384.0, 4.0], [385.0, 4.0], [386.0, 4.0], [387.0, 4.0], [388.0, 4.0], [389.0, 4.0], [390.0, 4.0], [391.0, 4.0], [392.0, 4.0], [393.0, 4.0], [394.0, 4.0], [395.0, 4.0], [396.0, 4.0], [397.0, 4.0], [398.0, 4.0], [399.0, 3.0], [400.0, 3.0], [401.0, 3.0], [402.0, 3.0], [403.0, 3.0], [404.0, 3.0], [405.0, 3.0], [406.0, 3.0], [407.0, 3.0], [408.0, 3.0], [409.0, 3.0], [410.0, 3.0], [411.0, 3.0], [412.0, 3.0], [413.0, 3.0], [414.0, 3.0], [415.0, 3.0], [416.0, 3.0], [417.0, 3.0], [418.0, 3.0], [419.0, 3.0], [420.0, 3.0], [421.0, 3.0], [422.0, 3.0], [423.0, 3.0], [424.0, 3.0], [425.0, 3.0], [426.0, 3.0], [427.0, 3.0], [428.0, 3.0], [429.0, 3.0], [430.0, 3.0], [431.0, 3.0], [432.0, 3.0], [433.0, 3.0], [434.0, 3.0], [435.0, 3.0], [436.0, 3.0], [437.0, 3.0], [438.0, 3.0], [439.0, 3.0], [440.0, 3.0], [441.0, 3.0], [442.0, 3.0], [443.0, 3.0], [444.0, 3.0], [445.0, 3.0], [446.0, 3.0], [447.0, 3.0], [448.0, 3.0], [449.0, 3.0], [450.0, 3.0], [451.0, 3.0], [452.0, 3.0], [453.0, 3.0], [454.0, 3.0], [455.0, 3.0], [456.0, 3.0], [457.0, 3.0], [458.0, 3.0], [459.0, 3.0], [460.0, 3.0], [461.0, 3.0], [462.0, 3.0], [463.0, 3.0], [464.0, 3.0], [465.0, 3.0], [466.0, 3.0], [467.0, 3.0], [468.0, 3.0], [469.0, 3.0], [470.0, 3.0], [471.0, 3.0], [472.0, 3.0], [473.0, 3.0], [474.0, 3.0], [475.0, 3.0], [476.0, 3.0], [477.0, 3.0], [478.0, 3.0], [479.0, 3.0], [480.0, 3.0], [481.0, 3.0], [482.0, 3.0], [483.0, 3.0], [484.0, 3.0], [485.0, 3.0], [486.0, 3.0], [487.0, 3.0], [488.0, 3.0], [489.0, 3.0], [490.0, 3.0], [491.0, 3.0], [492.0, 3.0], [493.0, 3.0], [494.0, 3.0], [495.0, 3.0], [496.0, 3.0], [497.0, 3.0], [498.0, 3.0], [499.0, 3.0], [500.0, 3.0], [501.0, 3.0], [502.0, 3.0], [503.0, 3.0], [504.0, 3.0], [505.0, 3.0], [506.0, 3.0], [507.0, 3.0], [508.0, 3.0], [509.0, 3.0], [510.0, 3.0], [511.0, 3.0], [512.0, 3.0], [513.0, 3.0], [514.0, 3.0], [515.0, 3.0], [516.0, 3.0], [517.0, 3.0], [518.0, 3.0], [519.0, 3.0], [520.0, 3.0], [521.0, 3.0], [522.0, 3.0], [523.0, 3.0], [524.0, 3.0], [525.0, 3.0], [526.0, 3.0], [527.0, 3.0], [528.0, 3.0], [529.0, 3.0], [530.0, 3.0], [531.0, 3.0], [532.0, 3.0], [533.0, 3.0], [534.0, 3.0], [535.0, 3.0], [536.0, 3.0], [537.0, 3.0], [538.0, 3.0], [539.0, 3.0], [540.0, 3.0], [541.0, 3.0], [542.0, 3.0], [543.0, 3.0], [544.0, 3.0], [545.0, 3.0], [546.0, 3.0], [547.0, 3.0], [548.0, 3.0], [549.0, 3.0], [550.0, 3.0], [551.0, 3.0], [552.0, 3.0], [553.0, 3.0], [554.0, 3.0], [555.0, 3.0], [556.0, 3.0], [557.0, 3.0], [558.0, 3.0], [559.0, 3.0], [560.0, 3.0], [561.0, 3.0], [562.0, 3.0], [563.0, 3.0], [564.0, 3.0], [565.0, 3.0], [566.0, 2.0], [567.0, 2.0], [568.0, 2.0], [569.0, 2.0], [570.0, 2.0], [571.0, 2.0], [572.0, 2.0], [573.0, 2.0], [574.0, 2.0], [575.0, 2.0], [576.0, 2.0], [577.0, 2.0], [578.0, 2.0], [579.0, 2.0], [580.0, 2.0], [581.0, 2.0], [582.0, 2.0], [583.0, 2.0], [584.0, 2.0], [585.0, 2.0], [586.0, 2.0], [587.0, 2.0], [588.0, 2.0], [589.0, 2.0], [590.0, 2.0], [591.0, 2.0], [592.0, 2.0], [593.0, 2.0], [594.0, 2.0], [595.0, 2.0], [596.0, 2.0], [597.0, 2.0], [598.0, 2.0], [599.0, 2.0], [600.0, 2.0], [601.0, 2.0], [602.0, 2.0], [603.0, 2.0], [604.0, 2.0], [605.0, 2.0], [606.0, 2.0], [607.0, 2.0], [608.0, 2.0], [609.0, 2.0], [610.0, 2.0], [611.0, 2.0], [612.0, 2.0], [613.0, 2.0], [614.0, 2.0], [615.0, 2.0], [616.0, 2.0], [617.0, 2.0], [618.0, 2.0], [619.0, 2.0], [620.0, 2.0], [621.0, 2.0], [622.0, 2.0], [623.0, 2.0], [624.0, 2.0], [625.0, 2.0], [626.0, 2.0], [627.0, 2.0], [628.0, 2.0], [629.0, 2.0], [630.0, 2.0], [631.0, 2.0], [632.0, 2.0], [633.0, 2.0], [634.0, 2.0], [635.0, 2.0], [636.0, 2.0], [637.0, 2.0], [638.0, 2.0], [639.0, 2.0], [640.0, 2.0], [641.0, 2.0], [642.0, 2.0], [643.0, 2.0], [644.0, 2.0], [645.0, 2.0], [646.0, 2.0], [647.0, 2.0], [648.0, 2.0], [649.0, 2.0], [650.0, 2.0], [651.0, 2.0], [652.0, 2.0], [653.0, 2.0], [654.0, 2.0], [655.0, 2.0], [656.0, 2.0], [657.0, 2.0], [658.0, 2.0], [659.0, 2.0], [660.0, 2.0], [661.0, 2.0], [662.0, 2.0], [663.0, 2.0], [664.0, 2.0], [665.0, 2.0], [666.0, 2.0], [667.0, 2.0], [668.0, 2.0], [669.0, 2.0], [670.0, 2.0], [671.0, 2.0], [672.0, 2.0], [673.0, 2.0], [674.0, 2.0], [675.0, 2.0], [676.0, 2.0], [677.0, 2.0], [678.0, 2.0], [679.0, 2.0], [680.0, 2.0], [681.0, 2.0], [682.0, 2.0], [683.0, 2.0], [684.0, 2.0], [685.0, 2.0], [686.0, 2.0], [687.0, 2.0], [688.0, 2.0], [689.0, 2.0], [690.0, 2.0], [691.0, 2.0], [692.0, 2.0], [693.0, 2.0], [694.0, 2.0], [695.0, 2.0], [696.0, 2.0], [697.0, 2.0], [698.0, 2.0], [699.0, 2.0], [700.0, 2.0], [701.0, 2.0], [702.0, 2.0], [703.0, 2.0], [704.0, 2.0], [705.0, 2.0], [706.0, 2.0], [707.0, 2.0], [708.0, 2.0], [709.0, 2.0], [710.0, 2.0], [711.0, 2.0], [712.0, 2.0], [713.0, 2.0], [714.0, 2.0], [715.0, 2.0], [716.0, 2.0], [717.0, 2.0], [718.0, 2.0], [719.0, 2.0], [720.0, 2.0], [721.0, 2.0], [722.0, 2.0], [723.0, 2.0], [724.0, 2.0], [725.0, 2.0], [726.0, 2.0], [727.0, 2.0], [728.0, 2.0], [729.0, 2.0], [730.0, 2.0], [731.0, 2.0], [732.0, 2.0], [733.0, 2.0], [734.0, 2.0], [735.0, 2.0], [736.0, 2.0], [737.0, 2.0], [738.0, 2.0], [739.0, 2.0], [740.0, 2.0], [741.0, 2.0], [742.0, 2.0], [743.0, 2.0], [744.0, 2.0], [745.0, 2.0], [746.0, 2.0], [747.0, 2.0], [748.0, 2.0], [749.0, 2.0], [750.0, 2.0], [751.0, 2.0], [752.0, 2.0], [753.0, 2.0], [754.0, 2.0], [755.0, 2.0], [756.0, 2.0], [757.0, 2.0], [758.0, 2.0], [759.0, 2.0], [760.0, 2.0], [761.0, 2.0], [762.0, 2.0], [763.0, 2.0], [764.0, 2.0], [765.0, 2.0], [766.0, 2.0], [767.0, 2.0], [768.0, 2.0], [769.0, 2.0], [770.0, 2.0], [771.0, 2.0], [772.0, 2.0], [773.0, 2.0], [774.0, 2.0], [775.0, 2.0], [776.0, 2.0], [777.0, 2.0], [778.0, 2.0], [779.0, 2.0], [780.0, 2.0], [781.0, 2.0], [782.0, 2.0], [783.0, 2.0], [784.0, 2.0], [785.0, 2.0], [786.0, 2.0], [787.0, 2.0], [788.0, 2.0], [789.0, 2.0], [790.0, 2.0], [791.0, 2.0], [792.0, 2.0], [793.0, 2.0], [794.0, 2.0], [795.0, 2.0], [796.0, 2.0], [797.0, 2.0], [798.0, 2.0], [799.0, 2.0], [800.0, 2.0], [801.0, 2.0], [802.0, 2.0], [803.0, 2.0], [804.0, 2.0], [805.0, 2.0], [806.0, 2.0], [807.0, 2.0], [808.0, 2.0], [809.0, 2.0], [810.0, 2.0], [811.0, 2.0], [812.0, 2.0], [813.0, 2.0], [814.0, 2.0], [815.0, 2.0], [816.0, 2.0], [817.0, 2.0], [818.0, 2.0], [819.0, 2.0], [820.0, 2.0], [821.0, 2.0], [822.0, 2.0], [823.0, 2.0], [824.0, 2.0], [825.0, 2.0], [826.0, 2.0], [827.0, 2.0], [828.0, 2.0], [829.0, 2.0], [830.0, 2.0], [831.0, 2.0], [832.0, 2.0], [833.0, 2.0], [834.0, 2.0], [835.0, 2.0], [836.0, 2.0], [837.0, 2.0], [838.0, 2.0], [839.0, 2.0], [840.0, 2.0], [841.0, 2.0], [842.0, 2.0], [843.0, 2.0], [844.0, 2.0], [845.0, 2.0], [846.0, 2.0], [847.0, 2.0], [848.0, 2.0], [849.0, 2.0], [850.0, 2.0], [851.0, 2.0], [852.0, 2.0], [853.0, 2.0], [854.0, 2.0], [855.0, 2.0], [856.0, 2.0], [857.0, 2.0], [858.0, 2.0], [859.0, 2.0], [860.0, 2.0], [861.0, 2.0], [862.0, 2.0], [863.0, 2.0], [864.0, 2.0], [865.0, 2.0], [866.0, 2.0], [867.0, 2.0], [868.0, 2.0], [869.0, 2.0], [870.0, 2.0], [871.0, 2.0], [872.0, 2.0], [873.0, 2.0], [874.0, 2.0], [875.0, 2.0], [876.0, 2.0], [877.0, 2.0], [878.0, 2.0], [879.0, 2.0], [880.0, 2.0], [881.0, 2.0], [882.0, 2.0], [883.0, 2.0], [884.0, 2.0], [885.0, 2.0], [886.0, 2.0], [887.0, 2.0], [888.0, 2.0], [889.0, 2.0], [890.0, 2.0], [891.0, 2.0], [892.0, 2.0], [893.0, 2.0], [894.0, 2.0], [895.0, 2.0], [896.0, 2.0], [897.0, 2.0], [898.0, 2.0], [899.0, 2.0], [900.0, 2.0], [901.0, 2.0], [902.0, 2.0], [903.0, 2.0], [904.0, 2.0], [905.0, 2.0], [906.0, 2.0], [907.0, 2.0], [908.0, 2.0], [909.0, 2.0], [910.0, 2.0], [911.0, 2.0], [912.0, 2.0], [913.0, 2.0], [914.0, 2.0], [915.0, 2.0], [916.0, 2.0], [917.0, 2.0], [918.0, 2.0], [919.0, 2.0], [920.0, 2.0], [921.0, 2.0], [922.0, 2.0], [923.0, 2.0], [924.0, 2.0], [925.0, 2.0], [926.0, 2.0], [927.0, 2.0], [928.0, 1.0], [929.0, 1.0], [930.0, 1.0], [931.0, 1.0], [932.0, 1.0], [933.0, 1.0], [934.0, 1.0], [935.0, 1.0], [936.0, 1.0], [937.0, 1.0], [938.0, 1.0], [939.0, 1.0], [940.0, 1.0], [941.0, 1.0], [942.0, 1.0], [943.0, 1.0], [944.0, 1.0], [945.0, 1.0], [946.0, 1.0], [947.0, 1.0], [948.0, 1.0], [949.0, 1.0], [950.0, 1.0], [951.0, 1.0], [952.0, 1.0], [953.0, 1.0], [954.0, 1.0], [955.0, 1.0], [956.0, 1.0], [957.0, 1.0], [958.0, 1.0], [959.0, 1.0], [960.0, 1.0], [961.0, 1.0], [962.0, 1.0], [963.0, 1.0], [964.0, 1.0], [965.0, 1.0], [966.0, 1.0], [967.0, 1.0], [968.0, 1.0], [969.0, 1.0], [970.0, 1.0], [971.0, 1.0], [972.0, 1.0], [973.0, 1.0], [974.0, 1.0], [975.0, 1.0], [976.0, 1.0], [977.0, 1.0], [978.0, 1.0], [979.0, 1.0], [980.0, 1.0], [981.0, 1.0], [982.0, 1.0], [983.0, 1.0], [984.0, 1.0], [985.0, 1.0], [986.0, 1.0], [987.0, 1.0], [988.0, 1.0], [989.0, 1.0], [990.0, 1.0], [991.0, 1.0], [992.0, 1.0], [993.0, 1.0], [994.0, 1.0], [995.0, 1.0], [996.0, 1.0], [997.0, 1.0], [998.0, 1.0], [999.0, 1.0], [1000.0, 1.0], [1001.0, 1.0], [1002.0, 1.0], [1003.0, 1.0], [1004.0, 1.0], [1005.0, 1.0], [1006.0, 1.0], [1007.0, 1.0], [1008.0, 1.0], [1009.0, 1.0], [1010.0, 1.0], [1011.0, 1.0], [1012.0, 1.0], [1013.0, 1.0], [1014.0, 1.0], [1015.0, 1.0], [1016.0, 1.0], [1017.0, 1.0], [1018.0, 1.0], [1019.0, 1.0], [1020.0, 1.0], [1021.0, 1.0], [1022.0, 1.0], [1023.0, 1.0], [1024.0, 1.0], [1025.0, 1.0], [1026.0, 1.0], [1027.0, 1.0], [1028.0, 1.0], [1029.0, 1.0], [1030.0, 1.0], [1031.0, 1.0], [1032.0, 1.0], [1033.0, 1.0], [1034.0, 1.0], [1035.0, 1.0], [1036.0, 1.0], [1037.0, 1.0], [1038.0, 1.0], [1039.0, 1.0], [1040.0, 1.0], [1041.0, 1.0], [1042.0, 1.0], [1043.0, 1.0], [1044.0, 1.0], [1045.0, 1.0], [1046.0, 1.0], [1047.0, 1.0], [1048.0, 1.0], [1049.0, 1.0], [1050.0, 1.0], [1051.0, 1.0], [1052.0, 1.0], [1053.0, 1.0], [1054.0, 1.0], [1055.0, 1.0], [1056.0, 1.0], [1057.0, 1.0], [1058.0, 1.0], [1059.0, 1.0], [1060.0, 1.0], [1061.0, 1.0], [1062.0, 1.0], [1063.0, 1.0], [1064.0, 1.0], [1065.0, 1.0], [1066.0, 1.0], [1067.0, 1.0], [1068.0, 1.0], [1069.0, 1.0], [1070.0, 1.0], [1071.0, 1.0], [1072.0, 1.0], [1073.0, 1.0], [1074.0, 1.0], [1075.0, 1.0], [1076.0, 1.0], [1077.0, 1.0], [1078.0, 1.0], [1079.0, 1.0], [1080.0, 1.0], [1081.0, 1.0], [1082.0, 1.0], [1083.0, 1.0], [1084.0, 1.0], [1085.0, 1.0], [1086.0, 1.0], [1087.0, 1.0], [1088.0, 1.0], [1089.0, 1.0], [1090.0, 1.0], [1091.0, 1.0], [1092.0, 1.0], [1093.0, 1.0], [1094.0, 1.0], [1095.0, 1.0], [1096.0, 1.0], [1097.0, 1.0], [1098.0, 1.0], [1099.0, 1.0], [1100.0, 1.0], [1101.0, 1.0], [1102.0, 1.0], [1103.0, 1.0], [1104.0, 1.0], [1105.0, 1.0], [1106.0, 1.0], [1107.0, 1.0], [1108.0, 1.0], [1109.0, 1.0], [1110.0, 1.0], [1111.0, 1.0], [1112.0, 1.0], [1113.0, 1.0], [1114.0, 1.0], [1115.0, 1.0], [1116.0, 1.0], [1117.0, 1.0], [1118.0, 1.0], [1119.0, 1.0], [1120.0, 1.0], [1121.0, 1.0], [1122.0, 1.0], [1123.0, 1.0], [1124.0, 1.0], [1125.0, 1.0], [1126.0, 1.0], [1127.0, 1.0], [1128.0, 1.0], [1129.0, 1.0], [1130.0, 1.0], [1131.0, 1.0], [1132.0, 1.0], [1133.0, 1.0], [1134.0, 1.0], [1135.0, 1.0], [1136.0, 1.0], [1137.0, 1.0], [1138.0, 1.0], [1139.0, 1.0], [1140.0, 1.0], [1141.0, 1.0], [1142.0, 1.0], [1143.0, 1.0], [1144.0, 1.0], [1145.0, 1.0], [1146.0, 1.0], [1147.0, 1.0], [1148.0, 1.0], [1149.0, 1.0], [1150.0, 1.0], [1151.0, 1.0], [1152.0, 1.0], [1153.0, 1.0], [1154.0, 1.0], [1155.0, 1.0], [1156.0, 1.0], [1157.0, 1.0], [1158.0, 1.0], [1159.0, 1.0], [1160.0, 1.0], [1161.0, 1.0], [1162.0, 1.0], [1163.0, 1.0], [1164.0, 1.0], [1165.0, 1.0], [1166.0, 1.0], [1167.0, 1.0], [1168.0, 1.0], [1169.0, 1.0], [1170.0, 1.0], [1171.0, 1.0], [1172.0, 1.0], [1173.0, 1.0], [1174.0, 1.0], [1175.0, 1.0], [1176.0, 1.0], [1177.0, 1.0], [1178.0, 1.0], [1179.0, 1.0], [1180.0, 1.0], [1181.0, 1.0], [1182.0, 1.0], [1183.0, 1.0], [1184.0, 1.0], [1185.0, 1.0], [1186.0, 1.0], [1187.0, 1.0], [1188.0, 1.0], [1189.0, 1.0], [1190.0, 1.0], [1191.0, 1.0], [1192.0, 1.0], [1193.0, 1.0], [1194.0, 1.0], [1195.0, 1.0], [1196.0, 1.0], [1197.0, 1.0], [1198.0, 1.0], [1199.0, 1.0], [1200.0, 1.0], [1201.0, 1.0], [1202.0, 1.0], [1203.0, 1.0], [1204.0, 1.0], [1205.0, 1.0], [1206.0, 1.0], [1207.0, 1.0], [1208.0, 1.0], [1209.0, 1.0], [1210.0, 1.0], [1211.0, 1.0], [1212.0, 1.0], [1213.0, 1.0], [1214.0, 1.0], [1215.0, 1.0], [1216.0, 1.0], [1217.0, 1.0], [1218.0, 1.0], [1219.0, 1.0], [1220.0, 1.0], [1221.0, 1.0], [1222.0, 1.0], [1223.0, 1.0], [1224.0, 1.0], [1225.0, 1.0], [1226.0, 1.0], [1227.0, 1.0], [1228.0, 1.0], [1229.0, 1.0], [1230.0, 1.0], [1231.0, 1.0], [1232.0, 1.0], [1233.0, 1.0], [1234.0, 1.0], [1235.0, 1.0], [1236.0, 1.0], [1237.0, 1.0], [1238.0, 1.0], [1239.0, 1.0], [1240.0, 1.0], [1241.0, 1.0], [1242.0, 1.0], [1243.0, 1.0], [1244.0, 1.0], [1245.0, 1.0], [1246.0, 1.0], [1247.0, 1.0], [1248.0, 1.0], [1249.0, 1.0], [1250.0, 1.0], [1251.0, 1.0], [1252.0, 1.0], [1253.0, 1.0], [1254.0, 1.0], [1255.0, 1.0], [1256.0, 1.0], [1257.0, 1.0], [1258.0, 1.0], [1259.0, 1.0], [1260.0, 1.0], [1261.0, 1.0], [1262.0, 1.0], [1263.0, 1.0], [1264.0, 1.0], [1265.0, 1.0], [1266.0, 1.0], [1267.0, 1.0], [1268.0, 1.0], [1269.0, 1.0], [1270.0, 1.0], [1271.0, 1.0], [1272.0, 1.0], [1273.0, 1.0], [1274.0, 1.0], [1275.0, 1.0], [1276.0, 1.0], [1277.0, 1.0], [1278.0, 1.0], [1279.0, 1.0], [1280.0, 1.0], [1281.0, 1.0], [1282.0, 1.0], [1283.0, 1.0], [1284.0, 1.0], [1285.0, 1.0], [1286.0, 1.0], [1287.0, 1.0], [1288.0, 1.0], [1289.0, 1.0], [1290.0, 1.0], [1291.0, 1.0], [1292.0, 1.0], [1293.0, 1.0], [1294.0, 1.0], [1295.0, 1.0], [1296.0, 1.0], [1297.0, 1.0], [1298.0, 1.0], [1299.0, 1.0], [1300.0, 1.0], [1301.0, 1.0], [1302.0, 1.0], [1303.0, 1.0], [1304.0, 1.0], [1305.0, 1.0], [1306.0, 1.0], [1307.0, 1.0], [1308.0, 1.0], [1309.0, 1.0], [1310.0, 1.0], [1311.0, 1.0], [1312.0, 1.0], [1313.0, 1.0], [1314.0, 1.0], [1315.0, 1.0], [1316.0, 1.0], [1317.0, 1.0], [1318.0, 1.0], [1319.0, 1.0], [1320.0, 1.0], [1321.0, 1.0], [1322.0, 1.0], [1323.0, 1.0], [1324.0, 1.0], [1325.0, 1.0], [1326.0, 1.0], [1327.0, 1.0], [1328.0, 1.0], [1329.0, 1.0], [1330.0, 1.0], [1331.0, 1.0], [1332.0, 1.0], [1333.0, 1.0], [1334.0, 1.0], [1335.0, 1.0], [1336.0, 1.0], [1337.0, 1.0], [1338.0, 1.0], [1339.0, 1.0], [1340.0, 1.0], [1341.0, 1.0], [1342.0, 1.0], [1343.0, 1.0], [1344.0, 1.0], [1345.0, 1.0], [1346.0, 1.0], [1347.0, 1.0], [1348.0, 1.0], [1349.0, 1.0], [1350.0, 1.0], [1351.0, 1.0], [1352.0, 1.0], [1353.0, 1.0], [1354.0, 1.0], [1355.0, 1.0], [1356.0, 1.0], [1357.0, 1.0], [1358.0, 1.0], [1359.0, 1.0], [1360.0, 1.0], [1361.0, 1.0], [1362.0, 1.0], [1363.0, 1.0], [1364.0, 1.0], [1365.0, 1.0], [1366.0, 1.0], [1367.0, 1.0], [1368.0, 1.0], [1369.0, 1.0], [1370.0, 1.0], [1371.0, 1.0], [1372.0, 1.0], [1373.0, 1.0], [1374.0, 1.0], [1375.0, 1.0], [1376.0, 1.0], [1377.0, 1.0], [1378.0, 1.0], [1379.0, 1.0], [1380.0, 1.0], [1381.0, 1.0], [1382.0, 1.0], [1383.0, 1.0], [1384.0, 1.0], [1385.0, 1.0], [1386.0, 1.0], [1387.0, 1.0], [1388.0, 1.0], [1389.0, 1.0], [1390.0, 1.0], [1391.0, 1.0], [1392.0, 1.0], [1393.0, 1.0], [1394.0, 1.0], [1395.0, 1.0], [1396.0, 1.0], [1397.0, 1.0], [1398.0, 1.0], [1399.0, 1.0], [1400.0, 1.0], [1401.0, 1.0], [1402.0, 1.0], [1403.0, 1.0], [1404.0, 1.0], [1405.0, 1.0], [1406.0, 1.0], [1407.0, 1.0], [1408.0, 1.0], [1409.0, 1.0], [1410.0, 1.0], [1411.0, 1.0], [1412.0, 1.0], [1413.0, 1.0], [1414.0, 1.0], [1415.0, 1.0], [1416.0, 1.0], [1417.0, 1.0], [1418.0, 1.0], [1419.0, 1.0], [1420.0, 1.0], [1421.0, 1.0], [1422.0, 1.0], [1423.0, 1.0], [1424.0, 1.0], [1425.0, 1.0], [1426.0, 1.0], [1427.0, 1.0], [1428.0, 1.0], [1429.0, 1.0], [1430.0, 1.0], [1431.0, 1.0], [1432.0, 1.0], [1433.0, 1.0], [1434.0, 1.0], [1435.0, 1.0], [1436.0, 1.0], [1437.0, 1.0], [1438.0, 1.0], [1439.0, 1.0], [1440.0, 1.0], [1441.0, 1.0], [1442.0, 1.0], [1443.0, 1.0], [1444.0, 1.0], [1445.0, 1.0], [1446.0, 1.0], [1447.0, 1.0], [1448.0, 1.0], [1449.0, 1.0], [1450.0, 1.0], [1451.0, 1.0], [1452.0, 1.0], [1453.0, 1.0], [1454.0, 1.0], [1455.0, 1.0], [1456.0, 1.0], [1457.0, 1.0], [1458.0, 1.0], [1459.0, 1.0], [1460.0, 1.0], [1461.0, 1.0], [1462.0, 1.0], [1463.0, 1.0], [1464.0, 1.0], [1465.0, 1.0], [1466.0, 1.0], [1467.0, 1.0], [1468.0, 1.0], [1469.0, 1.0], [1470.0, 1.0], [1471.0, 1.0], [1472.0, 1.0], [1473.0, 1.0], [1474.0, 1.0], [1475.0, 1.0], [1476.0, 1.0], [1477.0, 1.0], [1478.0, 1.0], [1479.0, 1.0], [1480.0, 1.0], [1481.0, 1.0], [1482.0, 1.0], [1483.0, 1.0], [1484.0, 1.0], [1485.0, 1.0], [1486.0, 1.0], [1487.0, 1.0], [1488.0, 1.0], [1489.0, 1.0], [1490.0, 1.0], [1491.0, 1.0], [1492.0, 1.0], [1493.0, 1.0], [1494.0, 1.0], [1495.0, 1.0], [1496.0, 1.0], [1497.0, 1.0], [1498.0, 1.0], [1499.0, 1.0], [1500.0, 1.0], [1501.0, 1.0], [1502.0, 1.0], [1503.0, 1.0], [1504.0, 1.0], [1505.0, 1.0], [1506.0, 1.0], [1507.0, 1.0], [1508.0, 1.0], [1509.0, 1.0], [1510.0, 1.0], [1511.0, 1.0], [1512.0, 1.0], [1513.0, 1.0], [1514.0, 1.0], [1515.0, 1.0], [1516.0, 1.0], [1517.0, 1.0], [1518.0, 1.0], [1519.0, 1.0], [1520.0, 1.0], [1521.0, 1.0], [1522.0, 1.0], [1523.0, 1.0], [1524.0, 1.0], [1525.0, 1.0], [1526.0, 1.0], [1527.0, 1.0], [1528.0, 1.0], [1529.0, 1.0], [1530.0, 1.0], [1531.0, 1.0], [1532.0, 1.0], [1533.0, 1.0], [1534.0, 1.0], [1535.0, 1.0], [1536.0, 1.0], [1537.0, 1.0], [1538.0, 1.0], [1539.0, 1.0], [1540.0, 1.0], [1541.0, 1.0], [1542.0, 1.0], [1543.0, 1.0], [1544.0, 1.0], [1545.0, 1.0], [1546.0, 1.0], [1547.0, 1.0], [1548.0, 1.0], [1549.0, 1.0], [1550.0, 1.0], [1551.0, 1.0], [1552.0, 1.0], [1553.0, 1.0], [1554.0, 1.0], [1555.0, 1.0], [1556.0, 1.0], [1557.0, 1.0], [1558.0, 1.0], [1559.0, 1.0], [1560.0, 1.0], [1561.0, 1.0], [1562.0, 1.0], [1563.0, 1.0], [1564.0, 1.0], [1565.0, 1.0], [1566.0, 1.0], [1567.0, 1.0], [1568.0, 1.0], [1569.0, 1.0], [1570.0, 1.0], [1571.0, 1.0], [1572.0, 1.0], [1573.0, 1.0], [1574.0, 1.0], [1575.0, 1.0], [1576.0, 1.0], [1577.0, 1.0], [1578.0, 1.0], [1579.0, 1.0], [1580.0, 1.0], [1581.0, 1.0], [1582.0, 1.0], [1583.0, 1.0], [1584.0, 1.0], [1585.0, 1.0], [1586.0, 1.0], [1587.0, 1.0], [1588.0, 1.0], [1589.0, 1.0], [1590.0, 1.0], [1591.0, 1.0], [1592.0, 1.0], [1593.0, 1.0], [1594.0, 1.0], [1595.0, 1.0], [1596.0, 1.0], [1597.0, 1.0], [1598.0, 1.0], [1599.0, 1.0], [1600.0, 1.0], [1601.0, 1.0], [1602.0, 1.0], [1603.0, 1.0], [1604.0, 1.0], [1605.0, 1.0], [1606.0, 1.0], [1607.0, 1.0], [1608.0, 1.0], [1609.0, 1.0], [1610.0, 1.0], [1611.0, 1.0], [1612.0, 1.0], [1613.0, 1.0], [1614.0, 1.0], [1615.0, 1.0], [1616.0, 1.0], [1617.0, 1.0], [1618.0, 1.0], [1619.0, 1.0], [1620.0, 1.0], [1621.0, 1.0], [1622.0, 1.0], [1623.0, 1.0], [1624.0, 1.0], [1625.0, 1.0], [1626.0, 1.0], [1627.0, 1.0], [1628.0, 1.0], [1629.0, 1.0], [1630.0, 1.0], [1631.0, 1.0], [1632.0, 1.0], [1633.0, 1.0], [1634.0, 1.0], [1635.0, 1.0], [1636.0, 1.0], [1637.0, 1.0], [1638.0, 1.0], [1639.0, 1.0], [1640.0, 1.0], [1641.0, 1.0], [1642.0, 1.0], [1643.0, 1.0], [1644.0, 1.0], [1645.0, 1.0], [1646.0, 1.0], [1647.0, 1.0], [1648.0, 1.0], [1649.0, 1.0], [1650.0, 1.0], [1651.0, 1.0], [1652.0, 1.0], [1653.0, 1.0], [1654.0, 1.0], [1655.0, 1.0], [1656.0, 1.0], [1657.0, 1.0], [1658.0, 1.0], [1659.0, 1.0], [1660.0, 1.0], [1661.0, 1.0], [1662.0, 1.0], [1663.0, 1.0], [1664.0, 1.0], [1665.0, 1.0], [1666.0, 1.0], [1667.0, 1.0], [1668.0, 1.0], [1669.0, 1.0], [1670.0, 1.0], [1671.0, 1.0], [1672.0, 1.0], [1673.0, 1.0], [1674.0, 1.0], [1675.0, 1.0], [1676.0, 1.0], [1677.0, 1.0], [1678.0, 1.0], [1679.0, 1.0], [1680.0, 1.0], [1681.0, 1.0], [1682.0, 1.0], [1683.0, 1.0], [1684.0, 1.0], [1685.0, 1.0], [1686.0, 1.0], [1687.0, 1.0], [1688.0, 1.0], [1689.0, 1.0], [1690.0, 1.0], [1691.0, 1.0], [1692.0, 1.0], [1693.0, 1.0], [1694.0, 1.0], [1695.0, 1.0], [1696.0, 1.0], [1697.0, 1.0], [1698.0, 1.0], [1699.0, 1.0], [1700.0, 1.0], [1701.0, 1.0], [1702.0, 1.0], [1703.0, 1.0], [1704.0, 1.0], [1705.0, 1.0], [1706.0, 1.0], [1707.0, 1.0], [1708.0, 1.0], [1709.0, 1.0], [1710.0, 1.0], [1711.0, 1.0], [1712.0, 1.0], [1713.0, 1.0], [1714.0, 1.0], [1715.0, 1.0], [1716.0, 1.0], [1717.0, 1.0], [1718.0, 1.0], [1719.0, 1.0], [1720.0, 1.0], [1721.0, 1.0], [1722.0, 1.0], [1723.0, 1.0], [1724.0, 1.0], [1725.0, 1.0], [1726.0, 1.0], [1727.0, 1.0], [1728.0, 1.0], [1729.0, 1.0], [1730.0, 1.0], [1731.0, 1.0], [1732.0, 1.0], [1733.0, 1.0], [1734.0, 1.0], [1735.0, 1.0], [1736.0, 1.0], [1737.0, 1.0], [1738.0, 1.0], [1739.0, 1.0], [1740.0, 1.0], [1741.0, 1.0], [1742.0, 1.0], [1743.0, 1.0], [1744.0, 1.0], [1745.0, 1.0], [1746.0, 1.0], [1747.0, 1.0], [1748.0, 1.0], [1749.0, 1.0], [1750.0, 1.0], [1751.0, 1.0], [1752.0, 1.0], [1753.0, 1.0], [1754.0, 1.0], [1755.0, 1.0], [1756.0, 1.0], [1757.0, 1.0], [1758.0, 1.0], [1759.0, 1.0], [1760.0, 1.0], [1761.0, 1.0], [1762.0, 1.0], [1763.0, 1.0], [1764.0, 1.0], [1765.0, 1.0], [1766.0, 1.0], [1767.0, 1.0], [1768.0, 1.0], [1769.0, 1.0], [1770.0, 1.0], [1771.0, 1.0], [1772.0, 1.0], [1773.0, 1.0], [1774.0, 1.0], [1775.0, 1.0], [1776.0, 1.0], [1777.0, 1.0], [1778.0, 1.0], [1779.0, 1.0], [1780.0, 1.0], [1781.0, 1.0], [1782.0, 1.0], [1783.0, 1.0], [1784.0, 1.0], [1785.0, 1.0], [1786.0, 1.0], [1787.0, 1.0], [1788.0, 1.0], [1789.0, 1.0], [1790.0, 1.0], [1791.0, 1.0], [1792.0, 1.0], [1793.0, 1.0], [1794.0, 1.0], [1795.0, 1.0], [1796.0, 1.0], [1797.0, 1.0], [1798.0, 1.0], [1799.0, 1.0], [1800.0, 1.0], [1801.0, 1.0], [1802.0, 1.0], [1803.0, 1.0], [1804.0, 1.0], [1805.0, 1.0], [1806.0, 1.0], [1807.0, 1.0], [1808.0, 1.0], [1809.0, 1.0], [1810.0, 1.0], [1811.0, 1.0], [1812.0, 1.0], [1813.0, 1.0], [1814.0, 1.0], [1815.0, 1.0], [1816.0, 1.0], [1817.0, 1.0], [1818.0, 1.0], [1819.0, 1.0], [1820.0, 1.0], [1821.0, 1.0], [1822.0, 1.0], [1823.0, 1.0], [1824.0, 1.0], [1825.0, 1.0], [1826.0, 1.0], [1827.0, 1.0], [1828.0, 1.0], [1829.0, 1.0], [1830.0, 1.0], [1831.0, 1.0], [1832.0, 1.0], [1833.0, 1.0], [1834.0, 1.0], [1835.0, 1.0], [1836.0, 1.0], [1837.0, 1.0], [1838.0, 1.0], [1839.0, 1.0], [1840.0, 1.0], [1841.0, 1.0], [1842.0, 1.0], [1843.0, 1.0], [1844.0, 1.0], [1845.0, 1.0], [1846.0, 1.0], [1847.0, 1.0], [1848.0, 1.0], [1849.0, 1.0], [1850.0, 1.0], [1851.0, 1.0], [1852.0, 1.0], [1853.0, 1.0], [1854.0, 1.0], [1855.0, 1.0], [1856.0, 1.0], [1857.0, 1.0], [1858.0, 1.0], [1859.0, 1.0], [1860.0, 1.0], [1861.0, 1.0], [1862.0, 1.0], [1863.0, 1.0], [1864.0, 1.0], [1865.0, 1.0], [1866.0, 1.0], [1867.0, 1.0], [1868.0, 1.0], [1869.0, 1.0], [1870.0, 1.0], [1871.0, 1.0], [1872.0, 1.0], [1873.0, 1.0], [1874.0, 1.0], [1875.0, 1.0], [1876.0, 1.0], [1877.0, 1.0], [1878.0, 1.0], [1879.0, 1.0], [1880.0, 1.0], [1881.0, 1.0], [1882.0, 1.0], [1883.0, 1.0], [1884.0, 1.0], [1885.0, 1.0], [1886.0, 1.0], [1887.0, 1.0], [1888.0, 1.0], [1889.0, 1.0], [1890.0, 1.0], [1891.0, 1.0], [1892.0, 1.0], [1893.0, 1.0], [1894.0, 1.0], [1895.0, 1.0], [1896.0, 1.0], [1897.0, 1.0], [1898.0, 1.0], [1899.0, 1.0], [1900.0, 1.0], [1901.0, 1.0], [1902.0, 1.0], [1903.0, 1.0], [1904.0, 1.0], [1905.0, 1.0], [1906.0, 1.0], [1907.0, 1.0], [1908.0, 1.0], [1909.0, 1.0], [1910.0, 1.0], [1911.0, 1.0], [1912.0, 1.0], [1913.0, 1.0], [1914.0, 1.0], [1915.0, 1.0], [1916.0, 1.0], [1917.0, 1.0], [1918.0, 1.0], [1919.0, 1.0], [1920.0, 1.0], [1921.0, 1.0], [1922.0, 1.0], [1923.0, 1.0], [1924.0, 1.0], [1925.0, 1.0], [1926.0, 1.0], [1927.0, 1.0], [1928.0, 1.0], [1929.0, 1.0], [1930.0, 1.0], [1931.0, 1.0], [1932.0, 1.0], [1933.0, 1.0], [1934.0, 1.0], [1935.0, 1.0], [1936.0, 1.0], [1937.0, 1.0], [1938.0, 1.0], [1939.0, 1.0], [1940.0, 1.0], [1941.0, 1.0], [1942.0, 1.0], [1943.0, 1.0], [1944.0, 1.0], [1945.0, 1.0], [1946.0, 1.0], [1947.0, 1.0], [1948.0, 1.0], [1949.0, 1.0], [1950.0, 1.0], [1951.0, 1.0], [1952.0, 1.0], [1953.0, 1.0], [1954.0, 1.0], [1955.0, 1.0], [1956.0, 1.0], [1957.0, 1.0], [1958.0, 1.0], [1959.0, 1.0], [1960.0, 1.0], [1961.0, 1.0], [1962.0, 1.0], [1963.0, 1.0], [1964.0, 1.0], [1965.0, 1.0], [1966.0, 1.0], [1967.0, 1.0], [1968.0, 1.0], [1969.0, 1.0], [1970.0, 1.0], [1971.0, 1.0], [1972.0, 1.0], [1973.0, 1.0], [1974.0, 1.0], [1975.0, 1.0], [1976.0, 1.0], [1977.0, 1.0], [1978.0, 1.0], [1979.0, 1.0], [1980.0, 1.0], [1981.0, 1.0], [1982.0, 1.0], [1983.0, 1.0], [1984.0, 1.0], [1985.0, 1.0], [1986.0, 1.0], [1987.0, 1.0], [1988.0, 1.0], [1989.0, 1.0], [1990.0, 1.0], [1991.0, 1.0], [1992.0, 1.0], [1993.0, 1.0], [1994.0, 1.0], [1995.0, 1.0], [1996.0, 1.0], [1997.0, 1.0], [1998.0, 1.0], [1999.0, 1.0], [2000.0, 1.0], [2001.0, 1.0], [2002.0, 1.0], [2003.0, 1.0], [2004.0, 1.0], [2005.0, 1.0], [2006.0, 1.0], [2007.0, 1.0], [2008.0, 1.0], [2009.0, 1.0], [2010.0, 1.0], [2011.0, 1.0], [2012.0, 1.0], [2013.0, 1.0], [2014.0, 1.0], [2015.0, 1.0], [2016.0, 1.0], [2017.0, 1.0], [2018.0, 1.0], [2019.0, 1.0], [2020.0, 1.0], [2021.0, 1.0], [2022.0, 1.0], [2023.0, 1.0], [2024.0, 1.0], [2025.0, 1.0], [2026.0, 1.0], [2027.0, 1.0], [2028.0, 1.0], [2029.0, 1.0], [2030.0, 1.0], [2031.0, 1.0], [2032.0, 1.0], [2033.0, 1.0], [2034.0, 1.0], [2035.0, 1.0], [2036.0, 1.0], [2037.0, 1.0], [2038.0, 1.0], [2039.0, 1.0], [2040.0, 1.0], [2041.0, 1.0], [2042.0, 1.0], [2043.0, 1.0], [2044.0, 1.0], [2045.0, 1.0], [2046.0, 1.0], [2047.0, 1.0], [2048.0, 1.0], [2049.0, 1.0], [2050.0, 1.0], [2051.0, 1.0], [2052.0, 1.0], [2053.0, 1.0], [2054.0, 1.0], [2055.0, 1.0], [2056.0, 1.0], [2057.0, 1.0], [2058.0, 1.0], [2059.0, 1.0], [2060.0, 1.0], [2061.0, 1.0], [2062.0, 1.0], [2063.0, 1.0], [2064.0, 1.0], [2065.0, 1.0], [2066.0, 1.0], [2067.0, 1.0], [2068.0, 1.0], [2069.0, 1.0], [2070.0, 1.0], [2071.0, 1.0], [2072.0, 1.0], [2073.0, 1.0], [2074.0, 1.0], [2075.0, 1.0], [2076.0, 1.0], [2077.0, 1.0], [2078.0, 1.0], [2079.0, 1.0], [2080.0, 1.0], [2081.0, 1.0], [2082.0, 1.0], [2083.0, 1.0], [2084.0, 1.0], [2085.0, 1.0], [2086.0, 1.0], [2087.0, 1.0], [2088.0, 1.0], [2089.0, 1.0], [2090.0, 1.0], [2091.0, 1.0], [2092.0, 1.0], [2093.0, 1.0], [2094.0, 1.0], [2095.0, 1.0], [2096.0, 1.0], [2097.0, 1.0], [2098.0, 1.0], [2099.0, 1.0], [2100.0, 1.0], [2101.0, 1.0], [2102.0, 1.0], [2103.0, 1.0], [2104.0, 1.0], [2105.0, 1.0], [2106.0, 1.0], [2107.0, 1.0], [2108.0, 1.0], [2109.0, 1.0], [2110.0, 1.0], [2111.0, 1.0], [2112.0, 1.0], [2113.0, 1.0], [2114.0, 1.0], [2115.0, 1.0], [2116.0, 1.0], [2117.0, 1.0], [2118.0, 1.0], [2119.0, 1.0], [2120.0, 1.0], [2121.0, 1.0], [2122.0, 1.0], [2123.0, 1.0], [2124.0, 1.0], [2125.0, 1.0], [2126.0, 1.0], [2127.0, 1.0], [2128.0, 1.0], [2129.0, 1.0], [2130.0, 1.0], [2131.0, 1.0], [2132.0, 1.0], [2133.0, 1.0], [2134.0, 1.0], [2135.0, 1.0], [2136.0, 1.0], [2137.0, 1.0], [2138.0, 1.0], [2139.0, 1.0], [2140.0, 1.0], [2141.0, 1.0], [2142.0, 1.0], [2143.0, 1.0], [2144.0, 1.0], [2145.0, 1.0], [2146.0, 1.0], [2147.0, 1.0], [2148.0, 1.0], [2149.0, 1.0], [2150.0, 1.0], [2151.0, 1.0], [2152.0, 1.0], [2153.0, 1.0], [2154.0, 1.0], [2155.0, 1.0], [2156.0, 1.0], [2157.0, 1.0], [2158.0, 1.0], [2159.0, 1.0], [2160.0, 1.0], [2161.0, 1.0], [2162.0, 1.0], [2163.0, 1.0], [2164.0, 1.0], [2165.0, 1.0], [2166.0, 1.0], [2167.0, 1.0], [2168.0, 1.0], [2169.0, 1.0], [2170.0, 1.0], [2171.0, 1.0], [2172.0, 1.0], [2173.0, 1.0], [2174.0, 1.0], [2175.0, 1.0], [2176.0, 1.0], [2177.0, 1.0], [2178.0, 1.0], [2179.0, 1.0], [2180.0, 1.0], [2181.0, 1.0], [2182.0, 1.0], [2183.0, 1.0], [2184.0, 1.0], [2185.0, 1.0], [2186.0, 1.0], [2187.0, 1.0], [2188.0, 1.0], [2189.0, 1.0], [2190.0, 1.0], [2191.0, 1.0], [2192.0, 1.0], [2193.0, 1.0], [2194.0, 1.0], [2195.0, 1.0], [2196.0, 1.0], [2197.0, 1.0], [2198.0, 1.0], [2199.0, 1.0], [2200.0, 1.0], [2201.0, 1.0], [2202.0, 1.0], [2203.0, 1.0], [2204.0, 1.0], [2205.0, 1.0], [2206.0, 1.0], [2207.0, 1.0], [2208.0, 1.0], [2209.0, 1.0], [2210.0, 1.0], [2211.0, 1.0], [2212.0, 1.0], [2213.0, 1.0], [2214.0, 1.0], [2215.0, 1.0], [2216.0, 1.0], [2217.0, 1.0], [2218.0, 1.0], [2219.0, 1.0], [2220.0, 1.0], [2221.0, 1.0], [2222.0, 1.0], [2223.0, 1.0], [2224.0, 1.0], [2225.0, 1.0], [2226.0, 1.0], [2227.0, 1.0], [2228.0, 1.0], [2229.0, 1.0], [2230.0, 1.0], [2231.0, 1.0], [2232.0, 1.0], [2233.0, 1.0], [2234.0, 1.0], [2235.0, 1.0], [2236.0, 1.0], [2237.0, 1.0], [2238.0, 1.0], [2239.0, 1.0], [2240.0, 1.0], [2241.0, 1.0], [2242.0, 1.0], [2243.0, 1.0], [2244.0, 1.0], [2245.0, 1.0], [2246.0, 1.0], [2247.0, 1.0], [2248.0, 1.0], [2249.0, 1.0], [2250.0, 1.0], [2251.0, 1.0], [2252.0, 1.0], [2253.0, 1.0], [2254.0, 1.0], [2255.0, 1.0], [2256.0, 1.0], [2257.0, 1.0], [2258.0, 1.0], [2259.0, 1.0], [2260.0, 1.0], [2261.0, 1.0], [2262.0, 1.0], [2263.0, 1.0], [2264.0, 1.0], [2265.0, 1.0], [2266.0, 1.0], [2267.0, 1.0], [2268.0, 1.0], [2269.0, 1.0], [2270.0, 1.0], [2271.0, 1.0], [2272.0, 1.0], [2273.0, 1.0], [2274.0, 1.0], [2275.0, 1.0], [2276.0, 1.0], [2277.0, 1.0], [2278.0, 1.0], [2279.0, 1.0], [2280.0, 1.0], [2281.0, 1.0], [2282.0, 1.0], [2283.0, 1.0], [2284.0, 1.0], [2285.0, 1.0], [2286.0, 1.0], [2287.0, 1.0], [2288.0, 1.0], [2289.0, 1.0], [2290.0, 1.0], [2291.0, 1.0], [2292.0, 1.0], [2293.0, 1.0], [2294.0, 1.0], [2295.0, 1.0], [2296.0, 1.0], [2297.0, 1.0], [2298.0, 1.0], [2299.0, 1.0], [2300.0, 1.0], [2301.0, 1.0], [2302.0, 1.0], [2303.0, 1.0], [2304.0, 1.0], [2305.0, 1.0], [2306.0, 1.0], [2307.0, 1.0], [2308.0, 1.0], [2309.0, 1.0], [2310.0, 1.0], [2311.0, 1.0], [2312.0, 1.0], [2313.0, 1.0], [2314.0, 1.0], [2315.0, 1.0], [2316.0, 1.0], [2317.0, 1.0], [2318.0, 1.0], [2319.0, 1.0], [2320.0, 1.0], [2321.0, 1.0], [2322.0, 1.0], [2323.0, 1.0], [2324.0, 1.0], [2325.0, 1.0], [2326.0, 1.0], [2327.0, 1.0], [2328.0, 1.0], [2329.0, 1.0], [2330.0, 1.0], [2331.0, 1.0], [2332.0, 1.0], [2333.0, 1.0], [2334.0, 1.0], [2335.0, 1.0], [2336.0, 1.0], [2337.0, 1.0], [2338.0, 1.0], [2339.0, 1.0], [2340.0, 1.0], [2341.0, 1.0], [2342.0, 1.0], [2343.0, 1.0], [2344.0, 1.0], [2345.0, 1.0], [2346.0, 1.0], [2347.0, 1.0], [2348.0, 1.0], [2349.0, 1.0], [2350.0, 1.0], [2351.0, 1.0], [2352.0, 1.0], [2353.0, 1.0], [2354.0, 1.0], [2355.0, 1.0], [2356.0, 1.0], [2357.0, 1.0], [2358.0, 1.0], [2359.0, 1.0], [2360.0, 1.0], [2361.0, 1.0], [2362.0, 1.0], [2363.0, 1.0], [2364.0, 1.0], [2365.0, 1.0], [2366.0, 1.0], [2367.0, 1.0], [2368.0, 1.0], [2369.0, 1.0], [2370.0, 1.0], [2371.0, 1.0], [2372.0, 1.0], [2373.0, 1.0], [2374.0, 1.0], [2375.0, 1.0], [2376.0, 1.0], [2377.0, 1.0], [2378.0, 1.0], [2379.0, 1.0], [2380.0, 1.0], [2381.0, 1.0], [2382.0, 1.0], [2383.0, 1.0], [2384.0, 1.0], [2385.0, 1.0], [2386.0, 1.0], [2387.0, 1.0], [2388.0, 1.0], [2389.0, 1.0], [2390.0, 1.0], [2391.0, 1.0], [2392.0, 1.0], [2393.0, 1.0], [2394.0, 1.0], [2395.0, 1.0], [2396.0, 1.0], [2397.0, 1.0], [2398.0, 1.0], [2399.0, 1.0], [2400.0, 1.0], [2401.0, 1.0], [2402.0, 1.0], [2403.0, 1.0], [2404.0, 1.0], [2405.0, 1.0], [2406.0, 1.0], [2407.0, 1.0], [2408.0, 1.0], [2409.0, 1.0], [2410.0, 1.0], [2411.0, 1.0], [2412.0, 1.0], [2413.0, 1.0], [2414.0, 1.0], [2415.0, 1.0], [2416.0, 1.0], [2417.0, 1.0], [2418.0, 1.0], [2419.0, 1.0], [2420.0, 1.0], [2421.0, 1.0], [2422.0, 1.0], [2423.0, 1.0], [2424.0, 1.0], [2425.0, 1.0], [2426.0, 1.0], [2427.0, 1.0], [2428.0, 1.0], [2429.0, 1.0], [2430.0, 1.0], [2431.0, 1.0], [2432.0, 1.0], [2433.0, 1.0], [2434.0, 1.0], [2435.0, 1.0], [2436.0, 1.0], [2437.0, 1.0], [2438.0, 1.0], [2439.0, 1.0], [2440.0, 1.0], [2441.0, 1.0], [2442.0, 1.0], [2443.0, 1.0], [2444.0, 1.0], [2445.0, 1.0], [2446.0, 1.0], [2447.0, 1.0], [2448.0, 1.0], [2449.0, 1.0], [2450.0, 1.0], [2451.0, 1.0], [2452.0, 1.0], [2453.0, 1.0], [2454.0, 1.0], [2455.0, 1.0], [2456.0, 1.0], [2457.0, 1.0], [2458.0, 1.0], [2459.0, 1.0], [2460.0, 1.0], [2461.0, 1.0], [2462.0, 1.0], [2463.0, 1.0], [2464.0, 1.0], [2465.0, 1.0], [2466.0, 1.0], [2467.0, 1.0], [2468.0, 1.0], [2469.0, 1.0], [2470.0, 1.0], [2471.0, 1.0], [2472.0, 1.0], [2473.0, 1.0], [2474.0, 1.0], [2475.0, 1.0], [2476.0, 1.0], [2477.0, 1.0], [2478.0, 1.0], [2479.0, 1.0], [2480.0, 1.0], [2481.0, 1.0], [2482.0, 1.0], [2483.0, 1.0], [2484.0, 1.0], [2485.0, 1.0], [2486.0, 1.0], [2487.0, 1.0], [2488.0, 1.0], [2489.0, 1.0], [2490.0, 1.0], [2491.0, 1.0], [2492.0, 1.0], [2493.0, 1.0], [2494.0, 1.0], [2495.0, 1.0], [2496.0, 1.0], [2497.0, 1.0], [2498.0, 1.0], [2499.0, 1.0], [2500.0, 1.0], [2501.0, 1.0], [2502.0, 1.0], [2503.0, 1.0], [2504.0, 1.0], [2505.0, 1.0], [2506.0, 1.0], [2507.0, 1.0], [2508.0, 1.0], [2509.0, 1.0], [2510.0, 1.0], [2511.0, 1.0], [2512.0, 1.0], [2513.0, 1.0], [2514.0, 1.0], [2515.0, 1.0], [2516.0, 1.0], [2517.0, 1.0], [2518.0, 1.0], [2519.0, 1.0], [2520.0, 1.0], [2521.0, 1.0], [2522.0, 1.0], [2523.0, 1.0], [2524.0, 1.0], [2525.0, 1.0], [2526.0, 1.0], [2527.0, 1.0], [2528.0, 1.0], [2529.0, 1.0], [2530.0, 1.0], [2531.0, 1.0], [2532.0, 1.0], [2533.0, 1.0], [2534.0, 1.0], [2535.0, 1.0], [2536.0, 1.0], [2537.0, 1.0], [2538.0, 1.0], [2539.0, 1.0], [2540.0, 1.0], [2541.0, 1.0], [2542.0, 1.0], [2543.0, 1.0], [2544.0, 1.0], [2545.0, 1.0], [2546.0, 1.0], [2547.0, 1.0], [2548.0, 1.0], [2549.0, 1.0], [2550.0, 1.0], [2551.0, 1.0], [2552.0, 1.0], [2553.0, 1.0], [2554.0, 1.0], [2555.0, 1.0], [2556.0, 1.0]]}, \"axes\": [{\"images\": [], \"bbox\": [0.125, 0.125, 0.775, 0.775], \"xlim\": [0.0, 3000.0], \"id\": \"el25140171029353472\", \"collections\": [], \"xscale\": \"linear\", \"ylim\": [0.0, 1400.0], \"xdomain\": [0.0, 3000.0], \"markers\": [], \"sharex\": [], \"axesbg\": \"#FFFFFF\", \"axesbgalpha\": null, \"ydomain\": [0.0, 1400.0], \"paths\": [], \"zoomable\": true, \"yscale\": \"linear\", \"sharey\": [], \"texts\": [], \"lines\": [{\"linewidth\": 1.0, \"xindex\": 0, \"zorder\": 2, \"coordinates\": \"data\", \"id\": \"el25140170344989752\", \"color\": \"#0000FF\", \"yindex\": 1, \"data\": \"data01\", \"dasharray\": \"10,0\", \"alpha\": 1}], \"axes\": [{\"scale\": \"linear\", \"grid\": {\"gridOn\": false}, \"position\": \"bottom\", \"tickformat\": null, \"nticks\": 7, \"tickvalues\": null, \"fontsize\": 10.0}, {\"scale\": \"linear\", \"grid\": {\"gridOn\": false}, \"position\": \"left\", \"tickformat\": null, \"nticks\": 8, \"tickvalues\": null, \"fontsize\": 10.0}]}], \"width\": 480.0});\n",
       "   }(mpld3);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/mpld3\n",
       "   require.config({paths: {d3: \"https://mpld3.github.io/js/d3.v3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      mpld3_load_lib(\"https://mpld3.github.io/js/mpld3.v0.2.js\", function(){\n",
       "         \n",
       "         mpld3.draw_figure(\"fig_el251401710293530243407788272\", {\"plugins\": [{\"type\": \"reset\"}, {\"button\": true, \"type\": \"zoom\", \"enabled\": false}, {\"button\": true, \"type\": \"boxzoom\", \"enabled\": false}], \"height\": 320.0, \"id\": \"el25140171029353024\", \"data\": {\"data01\": [[1.0, 1255.0], [2.0, 1255.0], [3.0, 509.0], [4.0, 390.0], [5.0, 265.0], [6.0, 243.0], [7.0, 208.0], [8.0, 187.0], [9.0, 146.0], [10.0, 142.0], [11.0, 136.0], [12.0, 113.0], [13.0, 109.0], [14.0, 103.0], [15.0, 98.0], [16.0, 94.0], [17.0, 88.0], [18.0, 82.0], [19.0, 71.0], [20.0, 67.0], [21.0, 65.0], [22.0, 61.0], [23.0, 60.0], [24.0, 58.0], [25.0, 58.0], [26.0, 57.0], [27.0, 56.0], [28.0, 55.0], [29.0, 55.0], [30.0, 53.0], [31.0, 50.0], [32.0, 47.0], [33.0, 45.0], [34.0, 45.0], [35.0, 45.0], [36.0, 44.0], [37.0, 42.0], [38.0, 42.0], [39.0, 41.0], [40.0, 40.0], [41.0, 40.0], [42.0, 40.0], [43.0, 40.0], [44.0, 36.0], [45.0, 34.0], [46.0, 34.0], [47.0, 32.0], [48.0, 32.0], [49.0, 30.0], [50.0, 29.0], [51.0, 29.0], [52.0, 28.0], [53.0, 27.0], [54.0, 26.0], [55.0, 26.0], [56.0, 26.0], [57.0, 25.0], [58.0, 25.0], [59.0, 25.0], [60.0, 25.0], [61.0, 24.0], [62.0, 24.0], [63.0, 23.0], [64.0, 22.0], [65.0, 22.0], [66.0, 21.0], [67.0, 21.0], [68.0, 21.0], [69.0, 20.0], [70.0, 19.0], [71.0, 19.0], [72.0, 19.0], [73.0, 18.0], [74.0, 18.0], [75.0, 18.0], [76.0, 18.0], [77.0, 18.0], [78.0, 17.0], [79.0, 17.0], [80.0, 17.0], [81.0, 17.0], [82.0, 17.0], [83.0, 17.0], [84.0, 16.0], [85.0, 16.0], [86.0, 16.0], [87.0, 16.0], [88.0, 16.0], [89.0, 16.0], [90.0, 16.0], [91.0, 16.0], [92.0, 16.0], [93.0, 16.0], [94.0, 15.0], [95.0, 15.0], [96.0, 15.0], [97.0, 15.0], [98.0, 14.0], [99.0, 14.0], [100.0, 14.0], [101.0, 14.0], [102.0, 14.0], [103.0, 14.0], [104.0, 14.0], [105.0, 13.0], [106.0, 13.0], [107.0, 13.0], [108.0, 13.0], [109.0, 13.0], [110.0, 13.0], [111.0, 13.0], [112.0, 13.0], [113.0, 13.0], [114.0, 12.0], [115.0, 12.0], [116.0, 12.0], [117.0, 12.0], [118.0, 12.0], [119.0, 11.0], [120.0, 11.0], [121.0, 11.0], [122.0, 11.0], [123.0, 11.0], [124.0, 11.0], [125.0, 11.0], [126.0, 11.0], [127.0, 11.0], [128.0, 11.0], [129.0, 11.0], [130.0, 11.0], [131.0, 11.0], [132.0, 11.0], [133.0, 11.0], [134.0, 11.0], [135.0, 11.0], [136.0, 11.0], [137.0, 10.0], [138.0, 10.0], [139.0, 10.0], [140.0, 10.0], [141.0, 10.0], [142.0, 10.0], [143.0, 10.0], [144.0, 10.0], [145.0, 10.0], [146.0, 10.0], [147.0, 10.0], [148.0, 10.0], [149.0, 10.0], [150.0, 10.0], [151.0, 10.0], [152.0, 9.0], [153.0, 9.0], [154.0, 9.0], [155.0, 9.0], [156.0, 9.0], [157.0, 9.0], [158.0, 9.0], [159.0, 9.0], [160.0, 9.0], [161.0, 9.0], [162.0, 9.0], [163.0, 9.0], [164.0, 9.0], [165.0, 9.0], [166.0, 9.0], [167.0, 9.0], [168.0, 9.0], [169.0, 9.0], [170.0, 8.0], [171.0, 8.0], [172.0, 8.0], [173.0, 8.0], [174.0, 8.0], [175.0, 8.0], [176.0, 8.0], [177.0, 8.0], [178.0, 8.0], [179.0, 8.0], [180.0, 8.0], [181.0, 8.0], [182.0, 8.0], [183.0, 8.0], [184.0, 8.0], [185.0, 8.0], [186.0, 8.0], [187.0, 8.0], [188.0, 8.0], [189.0, 7.0], [190.0, 7.0], [191.0, 7.0], [192.0, 7.0], [193.0, 7.0], [194.0, 7.0], [195.0, 7.0], [196.0, 7.0], [197.0, 7.0], [198.0, 7.0], [199.0, 7.0], [200.0, 7.0], [201.0, 7.0], [202.0, 7.0], [203.0, 7.0], [204.0, 7.0], [205.0, 7.0], [206.0, 7.0], [207.0, 7.0], [208.0, 7.0], [209.0, 7.0], [210.0, 7.0], [211.0, 7.0], [212.0, 7.0], [213.0, 7.0], [214.0, 7.0], [215.0, 7.0], [216.0, 7.0], [217.0, 7.0], [218.0, 7.0], [219.0, 7.0], [220.0, 6.0], [221.0, 6.0], [222.0, 6.0], [223.0, 6.0], [224.0, 6.0], [225.0, 6.0], [226.0, 6.0], [227.0, 6.0], [228.0, 6.0], [229.0, 6.0], [230.0, 6.0], [231.0, 6.0], [232.0, 6.0], [233.0, 6.0], [234.0, 6.0], [235.0, 6.0], [236.0, 6.0], [237.0, 6.0], [238.0, 6.0], [239.0, 6.0], [240.0, 6.0], [241.0, 6.0], [242.0, 6.0], [243.0, 6.0], [244.0, 6.0], [245.0, 6.0], [246.0, 6.0], [247.0, 6.0], [248.0, 6.0], [249.0, 6.0], [250.0, 6.0], [251.0, 6.0], [252.0, 6.0], [253.0, 6.0], [254.0, 6.0], [255.0, 6.0], [256.0, 6.0], [257.0, 6.0], [258.0, 5.0], [259.0, 5.0], [260.0, 5.0], [261.0, 5.0], [262.0, 5.0], [263.0, 5.0], [264.0, 5.0], [265.0, 5.0], [266.0, 5.0], [267.0, 5.0], [268.0, 5.0], [269.0, 5.0], [270.0, 5.0], [271.0, 5.0], [272.0, 5.0], [273.0, 5.0], [274.0, 5.0], [275.0, 5.0], [276.0, 5.0], [277.0, 5.0], [278.0, 5.0], [279.0, 5.0], [280.0, 5.0], [281.0, 5.0], [282.0, 5.0], [283.0, 5.0], [284.0, 5.0], [285.0, 5.0], [286.0, 5.0], [287.0, 5.0], [288.0, 5.0], [289.0, 5.0], [290.0, 5.0], [291.0, 5.0], [292.0, 5.0], [293.0, 5.0], [294.0, 5.0], [295.0, 5.0], [296.0, 5.0], [297.0, 5.0], [298.0, 5.0], [299.0, 5.0], [300.0, 5.0], [301.0, 5.0], [302.0, 5.0], [303.0, 5.0], [304.0, 5.0], [305.0, 5.0], [306.0, 5.0], [307.0, 5.0], [308.0, 5.0], [309.0, 5.0], [310.0, 5.0], [311.0, 5.0], [312.0, 5.0], [313.0, 5.0], [314.0, 5.0], [315.0, 5.0], [316.0, 5.0], [317.0, 5.0], [318.0, 4.0], [319.0, 4.0], [320.0, 4.0], [321.0, 4.0], [322.0, 4.0], [323.0, 4.0], [324.0, 4.0], [325.0, 4.0], [326.0, 4.0], [327.0, 4.0], [328.0, 4.0], [329.0, 4.0], [330.0, 4.0], [331.0, 4.0], [332.0, 4.0], [333.0, 4.0], [334.0, 4.0], [335.0, 4.0], [336.0, 4.0], [337.0, 4.0], [338.0, 4.0], [339.0, 4.0], [340.0, 4.0], [341.0, 4.0], [342.0, 4.0], [343.0, 4.0], [344.0, 4.0], [345.0, 4.0], [346.0, 4.0], [347.0, 4.0], [348.0, 4.0], [349.0, 4.0], [350.0, 4.0], [351.0, 4.0], [352.0, 4.0], [353.0, 4.0], [354.0, 4.0], [355.0, 4.0], [356.0, 4.0], [357.0, 4.0], [358.0, 4.0], [359.0, 4.0], [360.0, 4.0], [361.0, 4.0], [362.0, 4.0], [363.0, 4.0], [364.0, 4.0], [365.0, 4.0], [366.0, 4.0], [367.0, 4.0], [368.0, 4.0], [369.0, 4.0], [370.0, 4.0], [371.0, 4.0], [372.0, 4.0], [373.0, 4.0], [374.0, 4.0], [375.0, 4.0], [376.0, 4.0], [377.0, 4.0], [378.0, 4.0], [379.0, 4.0], [380.0, 4.0], [381.0, 4.0], [382.0, 4.0], [383.0, 4.0], [384.0, 4.0], [385.0, 4.0], [386.0, 4.0], [387.0, 4.0], [388.0, 4.0], [389.0, 4.0], [390.0, 4.0], [391.0, 4.0], [392.0, 4.0], [393.0, 4.0], [394.0, 4.0], [395.0, 4.0], [396.0, 4.0], [397.0, 4.0], [398.0, 4.0], [399.0, 3.0], [400.0, 3.0], [401.0, 3.0], [402.0, 3.0], [403.0, 3.0], [404.0, 3.0], [405.0, 3.0], [406.0, 3.0], [407.0, 3.0], [408.0, 3.0], [409.0, 3.0], [410.0, 3.0], [411.0, 3.0], [412.0, 3.0], [413.0, 3.0], [414.0, 3.0], [415.0, 3.0], [416.0, 3.0], [417.0, 3.0], [418.0, 3.0], [419.0, 3.0], [420.0, 3.0], [421.0, 3.0], [422.0, 3.0], [423.0, 3.0], [424.0, 3.0], [425.0, 3.0], [426.0, 3.0], [427.0, 3.0], [428.0, 3.0], [429.0, 3.0], [430.0, 3.0], [431.0, 3.0], [432.0, 3.0], [433.0, 3.0], [434.0, 3.0], [435.0, 3.0], [436.0, 3.0], [437.0, 3.0], [438.0, 3.0], [439.0, 3.0], [440.0, 3.0], [441.0, 3.0], [442.0, 3.0], [443.0, 3.0], [444.0, 3.0], [445.0, 3.0], [446.0, 3.0], [447.0, 3.0], [448.0, 3.0], [449.0, 3.0], [450.0, 3.0], [451.0, 3.0], [452.0, 3.0], [453.0, 3.0], [454.0, 3.0], [455.0, 3.0], [456.0, 3.0], [457.0, 3.0], [458.0, 3.0], [459.0, 3.0], [460.0, 3.0], [461.0, 3.0], [462.0, 3.0], [463.0, 3.0], [464.0, 3.0], [465.0, 3.0], [466.0, 3.0], [467.0, 3.0], [468.0, 3.0], [469.0, 3.0], [470.0, 3.0], [471.0, 3.0], [472.0, 3.0], [473.0, 3.0], [474.0, 3.0], [475.0, 3.0], [476.0, 3.0], [477.0, 3.0], [478.0, 3.0], [479.0, 3.0], [480.0, 3.0], [481.0, 3.0], [482.0, 3.0], [483.0, 3.0], [484.0, 3.0], [485.0, 3.0], [486.0, 3.0], [487.0, 3.0], [488.0, 3.0], [489.0, 3.0], [490.0, 3.0], [491.0, 3.0], [492.0, 3.0], [493.0, 3.0], [494.0, 3.0], [495.0, 3.0], [496.0, 3.0], [497.0, 3.0], [498.0, 3.0], [499.0, 3.0], [500.0, 3.0], [501.0, 3.0], [502.0, 3.0], [503.0, 3.0], [504.0, 3.0], [505.0, 3.0], [506.0, 3.0], [507.0, 3.0], [508.0, 3.0], [509.0, 3.0], [510.0, 3.0], [511.0, 3.0], [512.0, 3.0], [513.0, 3.0], [514.0, 3.0], [515.0, 3.0], [516.0, 3.0], [517.0, 3.0], [518.0, 3.0], [519.0, 3.0], [520.0, 3.0], [521.0, 3.0], [522.0, 3.0], [523.0, 3.0], [524.0, 3.0], [525.0, 3.0], [526.0, 3.0], [527.0, 3.0], [528.0, 3.0], [529.0, 3.0], [530.0, 3.0], [531.0, 3.0], [532.0, 3.0], [533.0, 3.0], [534.0, 3.0], [535.0, 3.0], [536.0, 3.0], [537.0, 3.0], [538.0, 3.0], [539.0, 3.0], [540.0, 3.0], [541.0, 3.0], [542.0, 3.0], [543.0, 3.0], [544.0, 3.0], [545.0, 3.0], [546.0, 3.0], [547.0, 3.0], [548.0, 3.0], [549.0, 3.0], [550.0, 3.0], [551.0, 3.0], [552.0, 3.0], [553.0, 3.0], [554.0, 3.0], [555.0, 3.0], [556.0, 3.0], [557.0, 3.0], [558.0, 3.0], [559.0, 3.0], [560.0, 3.0], [561.0, 3.0], [562.0, 3.0], [563.0, 3.0], [564.0, 3.0], [565.0, 3.0], [566.0, 2.0], [567.0, 2.0], [568.0, 2.0], [569.0, 2.0], [570.0, 2.0], [571.0, 2.0], [572.0, 2.0], [573.0, 2.0], [574.0, 2.0], [575.0, 2.0], [576.0, 2.0], [577.0, 2.0], [578.0, 2.0], [579.0, 2.0], [580.0, 2.0], [581.0, 2.0], [582.0, 2.0], [583.0, 2.0], [584.0, 2.0], [585.0, 2.0], [586.0, 2.0], [587.0, 2.0], [588.0, 2.0], [589.0, 2.0], [590.0, 2.0], [591.0, 2.0], [592.0, 2.0], [593.0, 2.0], [594.0, 2.0], [595.0, 2.0], [596.0, 2.0], [597.0, 2.0], [598.0, 2.0], [599.0, 2.0], [600.0, 2.0], [601.0, 2.0], [602.0, 2.0], [603.0, 2.0], [604.0, 2.0], [605.0, 2.0], [606.0, 2.0], [607.0, 2.0], [608.0, 2.0], [609.0, 2.0], [610.0, 2.0], [611.0, 2.0], [612.0, 2.0], [613.0, 2.0], [614.0, 2.0], [615.0, 2.0], [616.0, 2.0], [617.0, 2.0], [618.0, 2.0], [619.0, 2.0], [620.0, 2.0], [621.0, 2.0], [622.0, 2.0], [623.0, 2.0], [624.0, 2.0], [625.0, 2.0], [626.0, 2.0], [627.0, 2.0], [628.0, 2.0], [629.0, 2.0], [630.0, 2.0], [631.0, 2.0], [632.0, 2.0], [633.0, 2.0], [634.0, 2.0], [635.0, 2.0], [636.0, 2.0], [637.0, 2.0], [638.0, 2.0], [639.0, 2.0], [640.0, 2.0], [641.0, 2.0], [642.0, 2.0], [643.0, 2.0], [644.0, 2.0], [645.0, 2.0], [646.0, 2.0], [647.0, 2.0], [648.0, 2.0], [649.0, 2.0], [650.0, 2.0], [651.0, 2.0], [652.0, 2.0], [653.0, 2.0], [654.0, 2.0], [655.0, 2.0], [656.0, 2.0], [657.0, 2.0], [658.0, 2.0], [659.0, 2.0], [660.0, 2.0], [661.0, 2.0], [662.0, 2.0], [663.0, 2.0], [664.0, 2.0], [665.0, 2.0], [666.0, 2.0], [667.0, 2.0], [668.0, 2.0], [669.0, 2.0], [670.0, 2.0], [671.0, 2.0], [672.0, 2.0], [673.0, 2.0], [674.0, 2.0], [675.0, 2.0], [676.0, 2.0], [677.0, 2.0], [678.0, 2.0], [679.0, 2.0], [680.0, 2.0], [681.0, 2.0], [682.0, 2.0], [683.0, 2.0], [684.0, 2.0], [685.0, 2.0], [686.0, 2.0], [687.0, 2.0], [688.0, 2.0], [689.0, 2.0], [690.0, 2.0], [691.0, 2.0], [692.0, 2.0], [693.0, 2.0], [694.0, 2.0], [695.0, 2.0], [696.0, 2.0], [697.0, 2.0], [698.0, 2.0], [699.0, 2.0], [700.0, 2.0], [701.0, 2.0], [702.0, 2.0], [703.0, 2.0], [704.0, 2.0], [705.0, 2.0], [706.0, 2.0], [707.0, 2.0], [708.0, 2.0], [709.0, 2.0], [710.0, 2.0], [711.0, 2.0], [712.0, 2.0], [713.0, 2.0], [714.0, 2.0], [715.0, 2.0], [716.0, 2.0], [717.0, 2.0], [718.0, 2.0], [719.0, 2.0], [720.0, 2.0], [721.0, 2.0], [722.0, 2.0], [723.0, 2.0], [724.0, 2.0], [725.0, 2.0], [726.0, 2.0], [727.0, 2.0], [728.0, 2.0], [729.0, 2.0], [730.0, 2.0], [731.0, 2.0], [732.0, 2.0], [733.0, 2.0], [734.0, 2.0], [735.0, 2.0], [736.0, 2.0], [737.0, 2.0], [738.0, 2.0], [739.0, 2.0], [740.0, 2.0], [741.0, 2.0], [742.0, 2.0], [743.0, 2.0], [744.0, 2.0], [745.0, 2.0], [746.0, 2.0], [747.0, 2.0], [748.0, 2.0], [749.0, 2.0], [750.0, 2.0], [751.0, 2.0], [752.0, 2.0], [753.0, 2.0], [754.0, 2.0], [755.0, 2.0], [756.0, 2.0], [757.0, 2.0], [758.0, 2.0], [759.0, 2.0], [760.0, 2.0], [761.0, 2.0], [762.0, 2.0], [763.0, 2.0], [764.0, 2.0], [765.0, 2.0], [766.0, 2.0], [767.0, 2.0], [768.0, 2.0], [769.0, 2.0], [770.0, 2.0], [771.0, 2.0], [772.0, 2.0], [773.0, 2.0], [774.0, 2.0], [775.0, 2.0], [776.0, 2.0], [777.0, 2.0], [778.0, 2.0], [779.0, 2.0], [780.0, 2.0], [781.0, 2.0], [782.0, 2.0], [783.0, 2.0], [784.0, 2.0], [785.0, 2.0], [786.0, 2.0], [787.0, 2.0], [788.0, 2.0], [789.0, 2.0], [790.0, 2.0], [791.0, 2.0], [792.0, 2.0], [793.0, 2.0], [794.0, 2.0], [795.0, 2.0], [796.0, 2.0], [797.0, 2.0], [798.0, 2.0], [799.0, 2.0], [800.0, 2.0], [801.0, 2.0], [802.0, 2.0], [803.0, 2.0], [804.0, 2.0], [805.0, 2.0], [806.0, 2.0], [807.0, 2.0], [808.0, 2.0], [809.0, 2.0], [810.0, 2.0], [811.0, 2.0], [812.0, 2.0], [813.0, 2.0], [814.0, 2.0], [815.0, 2.0], [816.0, 2.0], [817.0, 2.0], [818.0, 2.0], [819.0, 2.0], [820.0, 2.0], [821.0, 2.0], [822.0, 2.0], [823.0, 2.0], [824.0, 2.0], [825.0, 2.0], [826.0, 2.0], [827.0, 2.0], [828.0, 2.0], [829.0, 2.0], [830.0, 2.0], [831.0, 2.0], [832.0, 2.0], [833.0, 2.0], [834.0, 2.0], [835.0, 2.0], [836.0, 2.0], [837.0, 2.0], [838.0, 2.0], [839.0, 2.0], [840.0, 2.0], [841.0, 2.0], [842.0, 2.0], [843.0, 2.0], [844.0, 2.0], [845.0, 2.0], [846.0, 2.0], [847.0, 2.0], [848.0, 2.0], [849.0, 2.0], [850.0, 2.0], [851.0, 2.0], [852.0, 2.0], [853.0, 2.0], [854.0, 2.0], [855.0, 2.0], [856.0, 2.0], [857.0, 2.0], [858.0, 2.0], [859.0, 2.0], [860.0, 2.0], [861.0, 2.0], [862.0, 2.0], [863.0, 2.0], [864.0, 2.0], [865.0, 2.0], [866.0, 2.0], [867.0, 2.0], [868.0, 2.0], [869.0, 2.0], [870.0, 2.0], [871.0, 2.0], [872.0, 2.0], [873.0, 2.0], [874.0, 2.0], [875.0, 2.0], [876.0, 2.0], [877.0, 2.0], [878.0, 2.0], [879.0, 2.0], [880.0, 2.0], [881.0, 2.0], [882.0, 2.0], [883.0, 2.0], [884.0, 2.0], [885.0, 2.0], [886.0, 2.0], [887.0, 2.0], [888.0, 2.0], [889.0, 2.0], [890.0, 2.0], [891.0, 2.0], [892.0, 2.0], [893.0, 2.0], [894.0, 2.0], [895.0, 2.0], [896.0, 2.0], [897.0, 2.0], [898.0, 2.0], [899.0, 2.0], [900.0, 2.0], [901.0, 2.0], [902.0, 2.0], [903.0, 2.0], [904.0, 2.0], [905.0, 2.0], [906.0, 2.0], [907.0, 2.0], [908.0, 2.0], [909.0, 2.0], [910.0, 2.0], [911.0, 2.0], [912.0, 2.0], [913.0, 2.0], [914.0, 2.0], [915.0, 2.0], [916.0, 2.0], [917.0, 2.0], [918.0, 2.0], [919.0, 2.0], [920.0, 2.0], [921.0, 2.0], [922.0, 2.0], [923.0, 2.0], [924.0, 2.0], [925.0, 2.0], [926.0, 2.0], [927.0, 2.0], [928.0, 1.0], [929.0, 1.0], [930.0, 1.0], [931.0, 1.0], [932.0, 1.0], [933.0, 1.0], [934.0, 1.0], [935.0, 1.0], [936.0, 1.0], [937.0, 1.0], [938.0, 1.0], [939.0, 1.0], [940.0, 1.0], [941.0, 1.0], [942.0, 1.0], [943.0, 1.0], [944.0, 1.0], [945.0, 1.0], [946.0, 1.0], [947.0, 1.0], [948.0, 1.0], [949.0, 1.0], [950.0, 1.0], [951.0, 1.0], [952.0, 1.0], [953.0, 1.0], [954.0, 1.0], [955.0, 1.0], [956.0, 1.0], [957.0, 1.0], [958.0, 1.0], [959.0, 1.0], [960.0, 1.0], [961.0, 1.0], [962.0, 1.0], [963.0, 1.0], [964.0, 1.0], [965.0, 1.0], [966.0, 1.0], [967.0, 1.0], [968.0, 1.0], [969.0, 1.0], [970.0, 1.0], [971.0, 1.0], [972.0, 1.0], [973.0, 1.0], [974.0, 1.0], [975.0, 1.0], [976.0, 1.0], [977.0, 1.0], [978.0, 1.0], [979.0, 1.0], [980.0, 1.0], [981.0, 1.0], [982.0, 1.0], [983.0, 1.0], [984.0, 1.0], [985.0, 1.0], [986.0, 1.0], [987.0, 1.0], [988.0, 1.0], [989.0, 1.0], [990.0, 1.0], [991.0, 1.0], [992.0, 1.0], [993.0, 1.0], [994.0, 1.0], [995.0, 1.0], [996.0, 1.0], [997.0, 1.0], [998.0, 1.0], [999.0, 1.0], [1000.0, 1.0], [1001.0, 1.0], [1002.0, 1.0], [1003.0, 1.0], [1004.0, 1.0], [1005.0, 1.0], [1006.0, 1.0], [1007.0, 1.0], [1008.0, 1.0], [1009.0, 1.0], [1010.0, 1.0], [1011.0, 1.0], [1012.0, 1.0], [1013.0, 1.0], [1014.0, 1.0], [1015.0, 1.0], [1016.0, 1.0], [1017.0, 1.0], [1018.0, 1.0], [1019.0, 1.0], [1020.0, 1.0], [1021.0, 1.0], [1022.0, 1.0], [1023.0, 1.0], [1024.0, 1.0], [1025.0, 1.0], [1026.0, 1.0], [1027.0, 1.0], [1028.0, 1.0], [1029.0, 1.0], [1030.0, 1.0], [1031.0, 1.0], [1032.0, 1.0], [1033.0, 1.0], [1034.0, 1.0], [1035.0, 1.0], [1036.0, 1.0], [1037.0, 1.0], [1038.0, 1.0], [1039.0, 1.0], [1040.0, 1.0], [1041.0, 1.0], [1042.0, 1.0], [1043.0, 1.0], [1044.0, 1.0], [1045.0, 1.0], [1046.0, 1.0], [1047.0, 1.0], [1048.0, 1.0], [1049.0, 1.0], [1050.0, 1.0], [1051.0, 1.0], [1052.0, 1.0], [1053.0, 1.0], [1054.0, 1.0], [1055.0, 1.0], [1056.0, 1.0], [1057.0, 1.0], [1058.0, 1.0], [1059.0, 1.0], [1060.0, 1.0], [1061.0, 1.0], [1062.0, 1.0], [1063.0, 1.0], [1064.0, 1.0], [1065.0, 1.0], [1066.0, 1.0], [1067.0, 1.0], [1068.0, 1.0], [1069.0, 1.0], [1070.0, 1.0], [1071.0, 1.0], [1072.0, 1.0], [1073.0, 1.0], [1074.0, 1.0], [1075.0, 1.0], [1076.0, 1.0], [1077.0, 1.0], [1078.0, 1.0], [1079.0, 1.0], [1080.0, 1.0], [1081.0, 1.0], [1082.0, 1.0], [1083.0, 1.0], [1084.0, 1.0], [1085.0, 1.0], [1086.0, 1.0], [1087.0, 1.0], [1088.0, 1.0], [1089.0, 1.0], [1090.0, 1.0], [1091.0, 1.0], [1092.0, 1.0], [1093.0, 1.0], [1094.0, 1.0], [1095.0, 1.0], [1096.0, 1.0], [1097.0, 1.0], [1098.0, 1.0], [1099.0, 1.0], [1100.0, 1.0], [1101.0, 1.0], [1102.0, 1.0], [1103.0, 1.0], [1104.0, 1.0], [1105.0, 1.0], [1106.0, 1.0], [1107.0, 1.0], [1108.0, 1.0], [1109.0, 1.0], [1110.0, 1.0], [1111.0, 1.0], [1112.0, 1.0], [1113.0, 1.0], [1114.0, 1.0], [1115.0, 1.0], [1116.0, 1.0], [1117.0, 1.0], [1118.0, 1.0], [1119.0, 1.0], [1120.0, 1.0], [1121.0, 1.0], [1122.0, 1.0], [1123.0, 1.0], [1124.0, 1.0], [1125.0, 1.0], [1126.0, 1.0], [1127.0, 1.0], [1128.0, 1.0], [1129.0, 1.0], [1130.0, 1.0], [1131.0, 1.0], [1132.0, 1.0], [1133.0, 1.0], [1134.0, 1.0], [1135.0, 1.0], [1136.0, 1.0], [1137.0, 1.0], [1138.0, 1.0], [1139.0, 1.0], [1140.0, 1.0], [1141.0, 1.0], [1142.0, 1.0], [1143.0, 1.0], [1144.0, 1.0], [1145.0, 1.0], [1146.0, 1.0], [1147.0, 1.0], [1148.0, 1.0], [1149.0, 1.0], [1150.0, 1.0], [1151.0, 1.0], [1152.0, 1.0], [1153.0, 1.0], [1154.0, 1.0], [1155.0, 1.0], [1156.0, 1.0], [1157.0, 1.0], [1158.0, 1.0], [1159.0, 1.0], [1160.0, 1.0], [1161.0, 1.0], [1162.0, 1.0], [1163.0, 1.0], [1164.0, 1.0], [1165.0, 1.0], [1166.0, 1.0], [1167.0, 1.0], [1168.0, 1.0], [1169.0, 1.0], [1170.0, 1.0], [1171.0, 1.0], [1172.0, 1.0], [1173.0, 1.0], [1174.0, 1.0], [1175.0, 1.0], [1176.0, 1.0], [1177.0, 1.0], [1178.0, 1.0], [1179.0, 1.0], [1180.0, 1.0], [1181.0, 1.0], [1182.0, 1.0], [1183.0, 1.0], [1184.0, 1.0], [1185.0, 1.0], [1186.0, 1.0], [1187.0, 1.0], [1188.0, 1.0], [1189.0, 1.0], [1190.0, 1.0], [1191.0, 1.0], [1192.0, 1.0], [1193.0, 1.0], [1194.0, 1.0], [1195.0, 1.0], [1196.0, 1.0], [1197.0, 1.0], [1198.0, 1.0], [1199.0, 1.0], [1200.0, 1.0], [1201.0, 1.0], [1202.0, 1.0], [1203.0, 1.0], [1204.0, 1.0], [1205.0, 1.0], [1206.0, 1.0], [1207.0, 1.0], [1208.0, 1.0], [1209.0, 1.0], [1210.0, 1.0], [1211.0, 1.0], [1212.0, 1.0], [1213.0, 1.0], [1214.0, 1.0], [1215.0, 1.0], [1216.0, 1.0], [1217.0, 1.0], [1218.0, 1.0], [1219.0, 1.0], [1220.0, 1.0], [1221.0, 1.0], [1222.0, 1.0], [1223.0, 1.0], [1224.0, 1.0], [1225.0, 1.0], [1226.0, 1.0], [1227.0, 1.0], [1228.0, 1.0], [1229.0, 1.0], [1230.0, 1.0], [1231.0, 1.0], [1232.0, 1.0], [1233.0, 1.0], [1234.0, 1.0], [1235.0, 1.0], [1236.0, 1.0], [1237.0, 1.0], [1238.0, 1.0], [1239.0, 1.0], [1240.0, 1.0], [1241.0, 1.0], [1242.0, 1.0], [1243.0, 1.0], [1244.0, 1.0], [1245.0, 1.0], [1246.0, 1.0], [1247.0, 1.0], [1248.0, 1.0], [1249.0, 1.0], [1250.0, 1.0], [1251.0, 1.0], [1252.0, 1.0], [1253.0, 1.0], [1254.0, 1.0], [1255.0, 1.0], [1256.0, 1.0], [1257.0, 1.0], [1258.0, 1.0], [1259.0, 1.0], [1260.0, 1.0], [1261.0, 1.0], [1262.0, 1.0], [1263.0, 1.0], [1264.0, 1.0], [1265.0, 1.0], [1266.0, 1.0], [1267.0, 1.0], [1268.0, 1.0], [1269.0, 1.0], [1270.0, 1.0], [1271.0, 1.0], [1272.0, 1.0], [1273.0, 1.0], [1274.0, 1.0], [1275.0, 1.0], [1276.0, 1.0], [1277.0, 1.0], [1278.0, 1.0], [1279.0, 1.0], [1280.0, 1.0], [1281.0, 1.0], [1282.0, 1.0], [1283.0, 1.0], [1284.0, 1.0], [1285.0, 1.0], [1286.0, 1.0], [1287.0, 1.0], [1288.0, 1.0], [1289.0, 1.0], [1290.0, 1.0], [1291.0, 1.0], [1292.0, 1.0], [1293.0, 1.0], [1294.0, 1.0], [1295.0, 1.0], [1296.0, 1.0], [1297.0, 1.0], [1298.0, 1.0], [1299.0, 1.0], [1300.0, 1.0], [1301.0, 1.0], [1302.0, 1.0], [1303.0, 1.0], [1304.0, 1.0], [1305.0, 1.0], [1306.0, 1.0], [1307.0, 1.0], [1308.0, 1.0], [1309.0, 1.0], [1310.0, 1.0], [1311.0, 1.0], [1312.0, 1.0], [1313.0, 1.0], [1314.0, 1.0], [1315.0, 1.0], [1316.0, 1.0], [1317.0, 1.0], [1318.0, 1.0], [1319.0, 1.0], [1320.0, 1.0], [1321.0, 1.0], [1322.0, 1.0], [1323.0, 1.0], [1324.0, 1.0], [1325.0, 1.0], [1326.0, 1.0], [1327.0, 1.0], [1328.0, 1.0], [1329.0, 1.0], [1330.0, 1.0], [1331.0, 1.0], [1332.0, 1.0], [1333.0, 1.0], [1334.0, 1.0], [1335.0, 1.0], [1336.0, 1.0], [1337.0, 1.0], [1338.0, 1.0], [1339.0, 1.0], [1340.0, 1.0], [1341.0, 1.0], [1342.0, 1.0], [1343.0, 1.0], [1344.0, 1.0], [1345.0, 1.0], [1346.0, 1.0], [1347.0, 1.0], [1348.0, 1.0], [1349.0, 1.0], [1350.0, 1.0], [1351.0, 1.0], [1352.0, 1.0], [1353.0, 1.0], [1354.0, 1.0], [1355.0, 1.0], [1356.0, 1.0], [1357.0, 1.0], [1358.0, 1.0], [1359.0, 1.0], [1360.0, 1.0], [1361.0, 1.0], [1362.0, 1.0], [1363.0, 1.0], [1364.0, 1.0], [1365.0, 1.0], [1366.0, 1.0], [1367.0, 1.0], [1368.0, 1.0], [1369.0, 1.0], [1370.0, 1.0], [1371.0, 1.0], [1372.0, 1.0], [1373.0, 1.0], [1374.0, 1.0], [1375.0, 1.0], [1376.0, 1.0], [1377.0, 1.0], [1378.0, 1.0], [1379.0, 1.0], [1380.0, 1.0], [1381.0, 1.0], [1382.0, 1.0], [1383.0, 1.0], [1384.0, 1.0], [1385.0, 1.0], [1386.0, 1.0], [1387.0, 1.0], [1388.0, 1.0], [1389.0, 1.0], [1390.0, 1.0], [1391.0, 1.0], [1392.0, 1.0], [1393.0, 1.0], [1394.0, 1.0], [1395.0, 1.0], [1396.0, 1.0], [1397.0, 1.0], [1398.0, 1.0], [1399.0, 1.0], [1400.0, 1.0], [1401.0, 1.0], [1402.0, 1.0], [1403.0, 1.0], [1404.0, 1.0], [1405.0, 1.0], [1406.0, 1.0], [1407.0, 1.0], [1408.0, 1.0], [1409.0, 1.0], [1410.0, 1.0], [1411.0, 1.0], [1412.0, 1.0], [1413.0, 1.0], [1414.0, 1.0], [1415.0, 1.0], [1416.0, 1.0], [1417.0, 1.0], [1418.0, 1.0], [1419.0, 1.0], [1420.0, 1.0], [1421.0, 1.0], [1422.0, 1.0], [1423.0, 1.0], [1424.0, 1.0], [1425.0, 1.0], [1426.0, 1.0], [1427.0, 1.0], [1428.0, 1.0], [1429.0, 1.0], [1430.0, 1.0], [1431.0, 1.0], [1432.0, 1.0], [1433.0, 1.0], [1434.0, 1.0], [1435.0, 1.0], [1436.0, 1.0], [1437.0, 1.0], [1438.0, 1.0], [1439.0, 1.0], [1440.0, 1.0], [1441.0, 1.0], [1442.0, 1.0], [1443.0, 1.0], [1444.0, 1.0], [1445.0, 1.0], [1446.0, 1.0], [1447.0, 1.0], [1448.0, 1.0], [1449.0, 1.0], [1450.0, 1.0], [1451.0, 1.0], [1452.0, 1.0], [1453.0, 1.0], [1454.0, 1.0], [1455.0, 1.0], [1456.0, 1.0], [1457.0, 1.0], [1458.0, 1.0], [1459.0, 1.0], [1460.0, 1.0], [1461.0, 1.0], [1462.0, 1.0], [1463.0, 1.0], [1464.0, 1.0], [1465.0, 1.0], [1466.0, 1.0], [1467.0, 1.0], [1468.0, 1.0], [1469.0, 1.0], [1470.0, 1.0], [1471.0, 1.0], [1472.0, 1.0], [1473.0, 1.0], [1474.0, 1.0], [1475.0, 1.0], [1476.0, 1.0], [1477.0, 1.0], [1478.0, 1.0], [1479.0, 1.0], [1480.0, 1.0], [1481.0, 1.0], [1482.0, 1.0], [1483.0, 1.0], [1484.0, 1.0], [1485.0, 1.0], [1486.0, 1.0], [1487.0, 1.0], [1488.0, 1.0], [1489.0, 1.0], [1490.0, 1.0], [1491.0, 1.0], [1492.0, 1.0], [1493.0, 1.0], [1494.0, 1.0], [1495.0, 1.0], [1496.0, 1.0], [1497.0, 1.0], [1498.0, 1.0], [1499.0, 1.0], [1500.0, 1.0], [1501.0, 1.0], [1502.0, 1.0], [1503.0, 1.0], [1504.0, 1.0], [1505.0, 1.0], [1506.0, 1.0], [1507.0, 1.0], [1508.0, 1.0], [1509.0, 1.0], [1510.0, 1.0], [1511.0, 1.0], [1512.0, 1.0], [1513.0, 1.0], [1514.0, 1.0], [1515.0, 1.0], [1516.0, 1.0], [1517.0, 1.0], [1518.0, 1.0], [1519.0, 1.0], [1520.0, 1.0], [1521.0, 1.0], [1522.0, 1.0], [1523.0, 1.0], [1524.0, 1.0], [1525.0, 1.0], [1526.0, 1.0], [1527.0, 1.0], [1528.0, 1.0], [1529.0, 1.0], [1530.0, 1.0], [1531.0, 1.0], [1532.0, 1.0], [1533.0, 1.0], [1534.0, 1.0], [1535.0, 1.0], [1536.0, 1.0], [1537.0, 1.0], [1538.0, 1.0], [1539.0, 1.0], [1540.0, 1.0], [1541.0, 1.0], [1542.0, 1.0], [1543.0, 1.0], [1544.0, 1.0], [1545.0, 1.0], [1546.0, 1.0], [1547.0, 1.0], [1548.0, 1.0], [1549.0, 1.0], [1550.0, 1.0], [1551.0, 1.0], [1552.0, 1.0], [1553.0, 1.0], [1554.0, 1.0], [1555.0, 1.0], [1556.0, 1.0], [1557.0, 1.0], [1558.0, 1.0], [1559.0, 1.0], [1560.0, 1.0], [1561.0, 1.0], [1562.0, 1.0], [1563.0, 1.0], [1564.0, 1.0], [1565.0, 1.0], [1566.0, 1.0], [1567.0, 1.0], [1568.0, 1.0], [1569.0, 1.0], [1570.0, 1.0], [1571.0, 1.0], [1572.0, 1.0], [1573.0, 1.0], [1574.0, 1.0], [1575.0, 1.0], [1576.0, 1.0], [1577.0, 1.0], [1578.0, 1.0], [1579.0, 1.0], [1580.0, 1.0], [1581.0, 1.0], [1582.0, 1.0], [1583.0, 1.0], [1584.0, 1.0], [1585.0, 1.0], [1586.0, 1.0], [1587.0, 1.0], [1588.0, 1.0], [1589.0, 1.0], [1590.0, 1.0], [1591.0, 1.0], [1592.0, 1.0], [1593.0, 1.0], [1594.0, 1.0], [1595.0, 1.0], [1596.0, 1.0], [1597.0, 1.0], [1598.0, 1.0], [1599.0, 1.0], [1600.0, 1.0], [1601.0, 1.0], [1602.0, 1.0], [1603.0, 1.0], [1604.0, 1.0], [1605.0, 1.0], [1606.0, 1.0], [1607.0, 1.0], [1608.0, 1.0], [1609.0, 1.0], [1610.0, 1.0], [1611.0, 1.0], [1612.0, 1.0], [1613.0, 1.0], [1614.0, 1.0], [1615.0, 1.0], [1616.0, 1.0], [1617.0, 1.0], [1618.0, 1.0], [1619.0, 1.0], [1620.0, 1.0], [1621.0, 1.0], [1622.0, 1.0], [1623.0, 1.0], [1624.0, 1.0], [1625.0, 1.0], [1626.0, 1.0], [1627.0, 1.0], [1628.0, 1.0], [1629.0, 1.0], [1630.0, 1.0], [1631.0, 1.0], [1632.0, 1.0], [1633.0, 1.0], [1634.0, 1.0], [1635.0, 1.0], [1636.0, 1.0], [1637.0, 1.0], [1638.0, 1.0], [1639.0, 1.0], [1640.0, 1.0], [1641.0, 1.0], [1642.0, 1.0], [1643.0, 1.0], [1644.0, 1.0], [1645.0, 1.0], [1646.0, 1.0], [1647.0, 1.0], [1648.0, 1.0], [1649.0, 1.0], [1650.0, 1.0], [1651.0, 1.0], [1652.0, 1.0], [1653.0, 1.0], [1654.0, 1.0], [1655.0, 1.0], [1656.0, 1.0], [1657.0, 1.0], [1658.0, 1.0], [1659.0, 1.0], [1660.0, 1.0], [1661.0, 1.0], [1662.0, 1.0], [1663.0, 1.0], [1664.0, 1.0], [1665.0, 1.0], [1666.0, 1.0], [1667.0, 1.0], [1668.0, 1.0], [1669.0, 1.0], [1670.0, 1.0], [1671.0, 1.0], [1672.0, 1.0], [1673.0, 1.0], [1674.0, 1.0], [1675.0, 1.0], [1676.0, 1.0], [1677.0, 1.0], [1678.0, 1.0], [1679.0, 1.0], [1680.0, 1.0], [1681.0, 1.0], [1682.0, 1.0], [1683.0, 1.0], [1684.0, 1.0], [1685.0, 1.0], [1686.0, 1.0], [1687.0, 1.0], [1688.0, 1.0], [1689.0, 1.0], [1690.0, 1.0], [1691.0, 1.0], [1692.0, 1.0], [1693.0, 1.0], [1694.0, 1.0], [1695.0, 1.0], [1696.0, 1.0], [1697.0, 1.0], [1698.0, 1.0], [1699.0, 1.0], [1700.0, 1.0], [1701.0, 1.0], [1702.0, 1.0], [1703.0, 1.0], [1704.0, 1.0], [1705.0, 1.0], [1706.0, 1.0], [1707.0, 1.0], [1708.0, 1.0], [1709.0, 1.0], [1710.0, 1.0], [1711.0, 1.0], [1712.0, 1.0], [1713.0, 1.0], [1714.0, 1.0], [1715.0, 1.0], [1716.0, 1.0], [1717.0, 1.0], [1718.0, 1.0], [1719.0, 1.0], [1720.0, 1.0], [1721.0, 1.0], [1722.0, 1.0], [1723.0, 1.0], [1724.0, 1.0], [1725.0, 1.0], [1726.0, 1.0], [1727.0, 1.0], [1728.0, 1.0], [1729.0, 1.0], [1730.0, 1.0], [1731.0, 1.0], [1732.0, 1.0], [1733.0, 1.0], [1734.0, 1.0], [1735.0, 1.0], [1736.0, 1.0], [1737.0, 1.0], [1738.0, 1.0], [1739.0, 1.0], [1740.0, 1.0], [1741.0, 1.0], [1742.0, 1.0], [1743.0, 1.0], [1744.0, 1.0], [1745.0, 1.0], [1746.0, 1.0], [1747.0, 1.0], [1748.0, 1.0], [1749.0, 1.0], [1750.0, 1.0], [1751.0, 1.0], [1752.0, 1.0], [1753.0, 1.0], [1754.0, 1.0], [1755.0, 1.0], [1756.0, 1.0], [1757.0, 1.0], [1758.0, 1.0], [1759.0, 1.0], [1760.0, 1.0], [1761.0, 1.0], [1762.0, 1.0], [1763.0, 1.0], [1764.0, 1.0], [1765.0, 1.0], [1766.0, 1.0], [1767.0, 1.0], [1768.0, 1.0], [1769.0, 1.0], [1770.0, 1.0], [1771.0, 1.0], [1772.0, 1.0], [1773.0, 1.0], [1774.0, 1.0], [1775.0, 1.0], [1776.0, 1.0], [1777.0, 1.0], [1778.0, 1.0], [1779.0, 1.0], [1780.0, 1.0], [1781.0, 1.0], [1782.0, 1.0], [1783.0, 1.0], [1784.0, 1.0], [1785.0, 1.0], [1786.0, 1.0], [1787.0, 1.0], [1788.0, 1.0], [1789.0, 1.0], [1790.0, 1.0], [1791.0, 1.0], [1792.0, 1.0], [1793.0, 1.0], [1794.0, 1.0], [1795.0, 1.0], [1796.0, 1.0], [1797.0, 1.0], [1798.0, 1.0], [1799.0, 1.0], [1800.0, 1.0], [1801.0, 1.0], [1802.0, 1.0], [1803.0, 1.0], [1804.0, 1.0], [1805.0, 1.0], [1806.0, 1.0], [1807.0, 1.0], [1808.0, 1.0], [1809.0, 1.0], [1810.0, 1.0], [1811.0, 1.0], [1812.0, 1.0], [1813.0, 1.0], [1814.0, 1.0], [1815.0, 1.0], [1816.0, 1.0], [1817.0, 1.0], [1818.0, 1.0], [1819.0, 1.0], [1820.0, 1.0], [1821.0, 1.0], [1822.0, 1.0], [1823.0, 1.0], [1824.0, 1.0], [1825.0, 1.0], [1826.0, 1.0], [1827.0, 1.0], [1828.0, 1.0], [1829.0, 1.0], [1830.0, 1.0], [1831.0, 1.0], [1832.0, 1.0], [1833.0, 1.0], [1834.0, 1.0], [1835.0, 1.0], [1836.0, 1.0], [1837.0, 1.0], [1838.0, 1.0], [1839.0, 1.0], [1840.0, 1.0], [1841.0, 1.0], [1842.0, 1.0], [1843.0, 1.0], [1844.0, 1.0], [1845.0, 1.0], [1846.0, 1.0], [1847.0, 1.0], [1848.0, 1.0], [1849.0, 1.0], [1850.0, 1.0], [1851.0, 1.0], [1852.0, 1.0], [1853.0, 1.0], [1854.0, 1.0], [1855.0, 1.0], [1856.0, 1.0], [1857.0, 1.0], [1858.0, 1.0], [1859.0, 1.0], [1860.0, 1.0], [1861.0, 1.0], [1862.0, 1.0], [1863.0, 1.0], [1864.0, 1.0], [1865.0, 1.0], [1866.0, 1.0], [1867.0, 1.0], [1868.0, 1.0], [1869.0, 1.0], [1870.0, 1.0], [1871.0, 1.0], [1872.0, 1.0], [1873.0, 1.0], [1874.0, 1.0], [1875.0, 1.0], [1876.0, 1.0], [1877.0, 1.0], [1878.0, 1.0], [1879.0, 1.0], [1880.0, 1.0], [1881.0, 1.0], [1882.0, 1.0], [1883.0, 1.0], [1884.0, 1.0], [1885.0, 1.0], [1886.0, 1.0], [1887.0, 1.0], [1888.0, 1.0], [1889.0, 1.0], [1890.0, 1.0], [1891.0, 1.0], [1892.0, 1.0], [1893.0, 1.0], [1894.0, 1.0], [1895.0, 1.0], [1896.0, 1.0], [1897.0, 1.0], [1898.0, 1.0], [1899.0, 1.0], [1900.0, 1.0], [1901.0, 1.0], [1902.0, 1.0], [1903.0, 1.0], [1904.0, 1.0], [1905.0, 1.0], [1906.0, 1.0], [1907.0, 1.0], [1908.0, 1.0], [1909.0, 1.0], [1910.0, 1.0], [1911.0, 1.0], [1912.0, 1.0], [1913.0, 1.0], [1914.0, 1.0], [1915.0, 1.0], [1916.0, 1.0], [1917.0, 1.0], [1918.0, 1.0], [1919.0, 1.0], [1920.0, 1.0], [1921.0, 1.0], [1922.0, 1.0], [1923.0, 1.0], [1924.0, 1.0], [1925.0, 1.0], [1926.0, 1.0], [1927.0, 1.0], [1928.0, 1.0], [1929.0, 1.0], [1930.0, 1.0], [1931.0, 1.0], [1932.0, 1.0], [1933.0, 1.0], [1934.0, 1.0], [1935.0, 1.0], [1936.0, 1.0], [1937.0, 1.0], [1938.0, 1.0], [1939.0, 1.0], [1940.0, 1.0], [1941.0, 1.0], [1942.0, 1.0], [1943.0, 1.0], [1944.0, 1.0], [1945.0, 1.0], [1946.0, 1.0], [1947.0, 1.0], [1948.0, 1.0], [1949.0, 1.0], [1950.0, 1.0], [1951.0, 1.0], [1952.0, 1.0], [1953.0, 1.0], [1954.0, 1.0], [1955.0, 1.0], [1956.0, 1.0], [1957.0, 1.0], [1958.0, 1.0], [1959.0, 1.0], [1960.0, 1.0], [1961.0, 1.0], [1962.0, 1.0], [1963.0, 1.0], [1964.0, 1.0], [1965.0, 1.0], [1966.0, 1.0], [1967.0, 1.0], [1968.0, 1.0], [1969.0, 1.0], [1970.0, 1.0], [1971.0, 1.0], [1972.0, 1.0], [1973.0, 1.0], [1974.0, 1.0], [1975.0, 1.0], [1976.0, 1.0], [1977.0, 1.0], [1978.0, 1.0], [1979.0, 1.0], [1980.0, 1.0], [1981.0, 1.0], [1982.0, 1.0], [1983.0, 1.0], [1984.0, 1.0], [1985.0, 1.0], [1986.0, 1.0], [1987.0, 1.0], [1988.0, 1.0], [1989.0, 1.0], [1990.0, 1.0], [1991.0, 1.0], [1992.0, 1.0], [1993.0, 1.0], [1994.0, 1.0], [1995.0, 1.0], [1996.0, 1.0], [1997.0, 1.0], [1998.0, 1.0], [1999.0, 1.0], [2000.0, 1.0], [2001.0, 1.0], [2002.0, 1.0], [2003.0, 1.0], [2004.0, 1.0], [2005.0, 1.0], [2006.0, 1.0], [2007.0, 1.0], [2008.0, 1.0], [2009.0, 1.0], [2010.0, 1.0], [2011.0, 1.0], [2012.0, 1.0], [2013.0, 1.0], [2014.0, 1.0], [2015.0, 1.0], [2016.0, 1.0], [2017.0, 1.0], [2018.0, 1.0], [2019.0, 1.0], [2020.0, 1.0], [2021.0, 1.0], [2022.0, 1.0], [2023.0, 1.0], [2024.0, 1.0], [2025.0, 1.0], [2026.0, 1.0], [2027.0, 1.0], [2028.0, 1.0], [2029.0, 1.0], [2030.0, 1.0], [2031.0, 1.0], [2032.0, 1.0], [2033.0, 1.0], [2034.0, 1.0], [2035.0, 1.0], [2036.0, 1.0], [2037.0, 1.0], [2038.0, 1.0], [2039.0, 1.0], [2040.0, 1.0], [2041.0, 1.0], [2042.0, 1.0], [2043.0, 1.0], [2044.0, 1.0], [2045.0, 1.0], [2046.0, 1.0], [2047.0, 1.0], [2048.0, 1.0], [2049.0, 1.0], [2050.0, 1.0], [2051.0, 1.0], [2052.0, 1.0], [2053.0, 1.0], [2054.0, 1.0], [2055.0, 1.0], [2056.0, 1.0], [2057.0, 1.0], [2058.0, 1.0], [2059.0, 1.0], [2060.0, 1.0], [2061.0, 1.0], [2062.0, 1.0], [2063.0, 1.0], [2064.0, 1.0], [2065.0, 1.0], [2066.0, 1.0], [2067.0, 1.0], [2068.0, 1.0], [2069.0, 1.0], [2070.0, 1.0], [2071.0, 1.0], [2072.0, 1.0], [2073.0, 1.0], [2074.0, 1.0], [2075.0, 1.0], [2076.0, 1.0], [2077.0, 1.0], [2078.0, 1.0], [2079.0, 1.0], [2080.0, 1.0], [2081.0, 1.0], [2082.0, 1.0], [2083.0, 1.0], [2084.0, 1.0], [2085.0, 1.0], [2086.0, 1.0], [2087.0, 1.0], [2088.0, 1.0], [2089.0, 1.0], [2090.0, 1.0], [2091.0, 1.0], [2092.0, 1.0], [2093.0, 1.0], [2094.0, 1.0], [2095.0, 1.0], [2096.0, 1.0], [2097.0, 1.0], [2098.0, 1.0], [2099.0, 1.0], [2100.0, 1.0], [2101.0, 1.0], [2102.0, 1.0], [2103.0, 1.0], [2104.0, 1.0], [2105.0, 1.0], [2106.0, 1.0], [2107.0, 1.0], [2108.0, 1.0], [2109.0, 1.0], [2110.0, 1.0], [2111.0, 1.0], [2112.0, 1.0], [2113.0, 1.0], [2114.0, 1.0], [2115.0, 1.0], [2116.0, 1.0], [2117.0, 1.0], [2118.0, 1.0], [2119.0, 1.0], [2120.0, 1.0], [2121.0, 1.0], [2122.0, 1.0], [2123.0, 1.0], [2124.0, 1.0], [2125.0, 1.0], [2126.0, 1.0], [2127.0, 1.0], [2128.0, 1.0], [2129.0, 1.0], [2130.0, 1.0], [2131.0, 1.0], [2132.0, 1.0], [2133.0, 1.0], [2134.0, 1.0], [2135.0, 1.0], [2136.0, 1.0], [2137.0, 1.0], [2138.0, 1.0], [2139.0, 1.0], [2140.0, 1.0], [2141.0, 1.0], [2142.0, 1.0], [2143.0, 1.0], [2144.0, 1.0], [2145.0, 1.0], [2146.0, 1.0], [2147.0, 1.0], [2148.0, 1.0], [2149.0, 1.0], [2150.0, 1.0], [2151.0, 1.0], [2152.0, 1.0], [2153.0, 1.0], [2154.0, 1.0], [2155.0, 1.0], [2156.0, 1.0], [2157.0, 1.0], [2158.0, 1.0], [2159.0, 1.0], [2160.0, 1.0], [2161.0, 1.0], [2162.0, 1.0], [2163.0, 1.0], [2164.0, 1.0], [2165.0, 1.0], [2166.0, 1.0], [2167.0, 1.0], [2168.0, 1.0], [2169.0, 1.0], [2170.0, 1.0], [2171.0, 1.0], [2172.0, 1.0], [2173.0, 1.0], [2174.0, 1.0], [2175.0, 1.0], [2176.0, 1.0], [2177.0, 1.0], [2178.0, 1.0], [2179.0, 1.0], [2180.0, 1.0], [2181.0, 1.0], [2182.0, 1.0], [2183.0, 1.0], [2184.0, 1.0], [2185.0, 1.0], [2186.0, 1.0], [2187.0, 1.0], [2188.0, 1.0], [2189.0, 1.0], [2190.0, 1.0], [2191.0, 1.0], [2192.0, 1.0], [2193.0, 1.0], [2194.0, 1.0], [2195.0, 1.0], [2196.0, 1.0], [2197.0, 1.0], [2198.0, 1.0], [2199.0, 1.0], [2200.0, 1.0], [2201.0, 1.0], [2202.0, 1.0], [2203.0, 1.0], [2204.0, 1.0], [2205.0, 1.0], [2206.0, 1.0], [2207.0, 1.0], [2208.0, 1.0], [2209.0, 1.0], [2210.0, 1.0], [2211.0, 1.0], [2212.0, 1.0], [2213.0, 1.0], [2214.0, 1.0], [2215.0, 1.0], [2216.0, 1.0], [2217.0, 1.0], [2218.0, 1.0], [2219.0, 1.0], [2220.0, 1.0], [2221.0, 1.0], [2222.0, 1.0], [2223.0, 1.0], [2224.0, 1.0], [2225.0, 1.0], [2226.0, 1.0], [2227.0, 1.0], [2228.0, 1.0], [2229.0, 1.0], [2230.0, 1.0], [2231.0, 1.0], [2232.0, 1.0], [2233.0, 1.0], [2234.0, 1.0], [2235.0, 1.0], [2236.0, 1.0], [2237.0, 1.0], [2238.0, 1.0], [2239.0, 1.0], [2240.0, 1.0], [2241.0, 1.0], [2242.0, 1.0], [2243.0, 1.0], [2244.0, 1.0], [2245.0, 1.0], [2246.0, 1.0], [2247.0, 1.0], [2248.0, 1.0], [2249.0, 1.0], [2250.0, 1.0], [2251.0, 1.0], [2252.0, 1.0], [2253.0, 1.0], [2254.0, 1.0], [2255.0, 1.0], [2256.0, 1.0], [2257.0, 1.0], [2258.0, 1.0], [2259.0, 1.0], [2260.0, 1.0], [2261.0, 1.0], [2262.0, 1.0], [2263.0, 1.0], [2264.0, 1.0], [2265.0, 1.0], [2266.0, 1.0], [2267.0, 1.0], [2268.0, 1.0], [2269.0, 1.0], [2270.0, 1.0], [2271.0, 1.0], [2272.0, 1.0], [2273.0, 1.0], [2274.0, 1.0], [2275.0, 1.0], [2276.0, 1.0], [2277.0, 1.0], [2278.0, 1.0], [2279.0, 1.0], [2280.0, 1.0], [2281.0, 1.0], [2282.0, 1.0], [2283.0, 1.0], [2284.0, 1.0], [2285.0, 1.0], [2286.0, 1.0], [2287.0, 1.0], [2288.0, 1.0], [2289.0, 1.0], [2290.0, 1.0], [2291.0, 1.0], [2292.0, 1.0], [2293.0, 1.0], [2294.0, 1.0], [2295.0, 1.0], [2296.0, 1.0], [2297.0, 1.0], [2298.0, 1.0], [2299.0, 1.0], [2300.0, 1.0], [2301.0, 1.0], [2302.0, 1.0], [2303.0, 1.0], [2304.0, 1.0], [2305.0, 1.0], [2306.0, 1.0], [2307.0, 1.0], [2308.0, 1.0], [2309.0, 1.0], [2310.0, 1.0], [2311.0, 1.0], [2312.0, 1.0], [2313.0, 1.0], [2314.0, 1.0], [2315.0, 1.0], [2316.0, 1.0], [2317.0, 1.0], [2318.0, 1.0], [2319.0, 1.0], [2320.0, 1.0], [2321.0, 1.0], [2322.0, 1.0], [2323.0, 1.0], [2324.0, 1.0], [2325.0, 1.0], [2326.0, 1.0], [2327.0, 1.0], [2328.0, 1.0], [2329.0, 1.0], [2330.0, 1.0], [2331.0, 1.0], [2332.0, 1.0], [2333.0, 1.0], [2334.0, 1.0], [2335.0, 1.0], [2336.0, 1.0], [2337.0, 1.0], [2338.0, 1.0], [2339.0, 1.0], [2340.0, 1.0], [2341.0, 1.0], [2342.0, 1.0], [2343.0, 1.0], [2344.0, 1.0], [2345.0, 1.0], [2346.0, 1.0], [2347.0, 1.0], [2348.0, 1.0], [2349.0, 1.0], [2350.0, 1.0], [2351.0, 1.0], [2352.0, 1.0], [2353.0, 1.0], [2354.0, 1.0], [2355.0, 1.0], [2356.0, 1.0], [2357.0, 1.0], [2358.0, 1.0], [2359.0, 1.0], [2360.0, 1.0], [2361.0, 1.0], [2362.0, 1.0], [2363.0, 1.0], [2364.0, 1.0], [2365.0, 1.0], [2366.0, 1.0], [2367.0, 1.0], [2368.0, 1.0], [2369.0, 1.0], [2370.0, 1.0], [2371.0, 1.0], [2372.0, 1.0], [2373.0, 1.0], [2374.0, 1.0], [2375.0, 1.0], [2376.0, 1.0], [2377.0, 1.0], [2378.0, 1.0], [2379.0, 1.0], [2380.0, 1.0], [2381.0, 1.0], [2382.0, 1.0], [2383.0, 1.0], [2384.0, 1.0], [2385.0, 1.0], [2386.0, 1.0], [2387.0, 1.0], [2388.0, 1.0], [2389.0, 1.0], [2390.0, 1.0], [2391.0, 1.0], [2392.0, 1.0], [2393.0, 1.0], [2394.0, 1.0], [2395.0, 1.0], [2396.0, 1.0], [2397.0, 1.0], [2398.0, 1.0], [2399.0, 1.0], [2400.0, 1.0], [2401.0, 1.0], [2402.0, 1.0], [2403.0, 1.0], [2404.0, 1.0], [2405.0, 1.0], [2406.0, 1.0], [2407.0, 1.0], [2408.0, 1.0], [2409.0, 1.0], [2410.0, 1.0], [2411.0, 1.0], [2412.0, 1.0], [2413.0, 1.0], [2414.0, 1.0], [2415.0, 1.0], [2416.0, 1.0], [2417.0, 1.0], [2418.0, 1.0], [2419.0, 1.0], [2420.0, 1.0], [2421.0, 1.0], [2422.0, 1.0], [2423.0, 1.0], [2424.0, 1.0], [2425.0, 1.0], [2426.0, 1.0], [2427.0, 1.0], [2428.0, 1.0], [2429.0, 1.0], [2430.0, 1.0], [2431.0, 1.0], [2432.0, 1.0], [2433.0, 1.0], [2434.0, 1.0], [2435.0, 1.0], [2436.0, 1.0], [2437.0, 1.0], [2438.0, 1.0], [2439.0, 1.0], [2440.0, 1.0], [2441.0, 1.0], [2442.0, 1.0], [2443.0, 1.0], [2444.0, 1.0], [2445.0, 1.0], [2446.0, 1.0], [2447.0, 1.0], [2448.0, 1.0], [2449.0, 1.0], [2450.0, 1.0], [2451.0, 1.0], [2452.0, 1.0], [2453.0, 1.0], [2454.0, 1.0], [2455.0, 1.0], [2456.0, 1.0], [2457.0, 1.0], [2458.0, 1.0], [2459.0, 1.0], [2460.0, 1.0], [2461.0, 1.0], [2462.0, 1.0], [2463.0, 1.0], [2464.0, 1.0], [2465.0, 1.0], [2466.0, 1.0], [2467.0, 1.0], [2468.0, 1.0], [2469.0, 1.0], [2470.0, 1.0], [2471.0, 1.0], [2472.0, 1.0], [2473.0, 1.0], [2474.0, 1.0], [2475.0, 1.0], [2476.0, 1.0], [2477.0, 1.0], [2478.0, 1.0], [2479.0, 1.0], [2480.0, 1.0], [2481.0, 1.0], [2482.0, 1.0], [2483.0, 1.0], [2484.0, 1.0], [2485.0, 1.0], [2486.0, 1.0], [2487.0, 1.0], [2488.0, 1.0], [2489.0, 1.0], [2490.0, 1.0], [2491.0, 1.0], [2492.0, 1.0], [2493.0, 1.0], [2494.0, 1.0], [2495.0, 1.0], [2496.0, 1.0], [2497.0, 1.0], [2498.0, 1.0], [2499.0, 1.0], [2500.0, 1.0], [2501.0, 1.0], [2502.0, 1.0], [2503.0, 1.0], [2504.0, 1.0], [2505.0, 1.0], [2506.0, 1.0], [2507.0, 1.0], [2508.0, 1.0], [2509.0, 1.0], [2510.0, 1.0], [2511.0, 1.0], [2512.0, 1.0], [2513.0, 1.0], [2514.0, 1.0], [2515.0, 1.0], [2516.0, 1.0], [2517.0, 1.0], [2518.0, 1.0], [2519.0, 1.0], [2520.0, 1.0], [2521.0, 1.0], [2522.0, 1.0], [2523.0, 1.0], [2524.0, 1.0], [2525.0, 1.0], [2526.0, 1.0], [2527.0, 1.0], [2528.0, 1.0], [2529.0, 1.0], [2530.0, 1.0], [2531.0, 1.0], [2532.0, 1.0], [2533.0, 1.0], [2534.0, 1.0], [2535.0, 1.0], [2536.0, 1.0], [2537.0, 1.0], [2538.0, 1.0], [2539.0, 1.0], [2540.0, 1.0], [2541.0, 1.0], [2542.0, 1.0], [2543.0, 1.0], [2544.0, 1.0], [2545.0, 1.0], [2546.0, 1.0], [2547.0, 1.0], [2548.0, 1.0], [2549.0, 1.0], [2550.0, 1.0], [2551.0, 1.0], [2552.0, 1.0], [2553.0, 1.0], [2554.0, 1.0], [2555.0, 1.0], [2556.0, 1.0]]}, \"axes\": [{\"images\": [], \"bbox\": [0.125, 0.125, 0.775, 0.775], \"xlim\": [0.0, 3000.0], \"id\": \"el25140171029353472\", \"collections\": [], \"xscale\": \"linear\", \"ylim\": [0.0, 1400.0], \"xdomain\": [0.0, 3000.0], \"markers\": [], \"sharex\": [], \"axesbg\": \"#FFFFFF\", \"axesbgalpha\": null, \"ydomain\": [0.0, 1400.0], \"paths\": [], \"zoomable\": true, \"yscale\": \"linear\", \"sharey\": [], \"texts\": [], \"lines\": [{\"linewidth\": 1.0, \"xindex\": 0, \"zorder\": 2, \"coordinates\": \"data\", \"id\": \"el25140170344989752\", \"color\": \"#0000FF\", \"yindex\": 1, \"data\": \"data01\", \"dasharray\": \"10,0\", \"alpha\": 1}], \"axes\": [{\"scale\": \"linear\", \"grid\": {\"gridOn\": false}, \"position\": \"bottom\", \"tickformat\": null, \"nticks\": 7, \"tickvalues\": null, \"fontsize\": 10.0}, {\"scale\": \"linear\", \"grid\": {\"gridOn\": false}, \"position\": \"left\", \"tickformat\": null, \"nticks\": 8, \"tickvalues\": null, \"fontsize\": 10.0}]}], \"width\": 480.0});\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & mpld3\n",
       "    mpld3_load_lib(\"https://mpld3.github.io/js/d3.v3.min.js\", function(){\n",
       "         mpld3_load_lib(\"https://mpld3.github.io/js/mpld3.v0.2.js\", function(){\n",
       "                 \n",
       "                 mpld3.draw_figure(\"fig_el251401710293530243407788272\", {\"plugins\": [{\"type\": \"reset\"}, {\"button\": true, \"type\": \"zoom\", \"enabled\": false}, {\"button\": true, \"type\": \"boxzoom\", \"enabled\": false}], \"height\": 320.0, \"id\": \"el25140171029353024\", \"data\": {\"data01\": [[1.0, 1255.0], [2.0, 1255.0], [3.0, 509.0], [4.0, 390.0], [5.0, 265.0], [6.0, 243.0], [7.0, 208.0], [8.0, 187.0], [9.0, 146.0], [10.0, 142.0], [11.0, 136.0], [12.0, 113.0], [13.0, 109.0], [14.0, 103.0], [15.0, 98.0], [16.0, 94.0], [17.0, 88.0], [18.0, 82.0], [19.0, 71.0], [20.0, 67.0], [21.0, 65.0], [22.0, 61.0], [23.0, 60.0], [24.0, 58.0], [25.0, 58.0], [26.0, 57.0], [27.0, 56.0], [28.0, 55.0], [29.0, 55.0], [30.0, 53.0], [31.0, 50.0], [32.0, 47.0], [33.0, 45.0], [34.0, 45.0], [35.0, 45.0], [36.0, 44.0], [37.0, 42.0], [38.0, 42.0], [39.0, 41.0], [40.0, 40.0], [41.0, 40.0], [42.0, 40.0], [43.0, 40.0], [44.0, 36.0], [45.0, 34.0], [46.0, 34.0], [47.0, 32.0], [48.0, 32.0], [49.0, 30.0], [50.0, 29.0], [51.0, 29.0], [52.0, 28.0], [53.0, 27.0], [54.0, 26.0], [55.0, 26.0], [56.0, 26.0], [57.0, 25.0], [58.0, 25.0], [59.0, 25.0], [60.0, 25.0], [61.0, 24.0], [62.0, 24.0], [63.0, 23.0], [64.0, 22.0], [65.0, 22.0], [66.0, 21.0], [67.0, 21.0], [68.0, 21.0], [69.0, 20.0], [70.0, 19.0], [71.0, 19.0], [72.0, 19.0], [73.0, 18.0], [74.0, 18.0], [75.0, 18.0], [76.0, 18.0], [77.0, 18.0], [78.0, 17.0], [79.0, 17.0], [80.0, 17.0], [81.0, 17.0], [82.0, 17.0], [83.0, 17.0], [84.0, 16.0], [85.0, 16.0], [86.0, 16.0], [87.0, 16.0], [88.0, 16.0], [89.0, 16.0], [90.0, 16.0], [91.0, 16.0], [92.0, 16.0], [93.0, 16.0], [94.0, 15.0], [95.0, 15.0], [96.0, 15.0], [97.0, 15.0], [98.0, 14.0], [99.0, 14.0], [100.0, 14.0], [101.0, 14.0], [102.0, 14.0], [103.0, 14.0], [104.0, 14.0], [105.0, 13.0], [106.0, 13.0], [107.0, 13.0], [108.0, 13.0], [109.0, 13.0], [110.0, 13.0], [111.0, 13.0], [112.0, 13.0], [113.0, 13.0], [114.0, 12.0], [115.0, 12.0], [116.0, 12.0], [117.0, 12.0], [118.0, 12.0], [119.0, 11.0], [120.0, 11.0], [121.0, 11.0], [122.0, 11.0], [123.0, 11.0], [124.0, 11.0], [125.0, 11.0], [126.0, 11.0], [127.0, 11.0], [128.0, 11.0], [129.0, 11.0], [130.0, 11.0], [131.0, 11.0], [132.0, 11.0], [133.0, 11.0], [134.0, 11.0], [135.0, 11.0], [136.0, 11.0], [137.0, 10.0], [138.0, 10.0], [139.0, 10.0], [140.0, 10.0], [141.0, 10.0], [142.0, 10.0], [143.0, 10.0], [144.0, 10.0], [145.0, 10.0], [146.0, 10.0], [147.0, 10.0], [148.0, 10.0], [149.0, 10.0], [150.0, 10.0], [151.0, 10.0], [152.0, 9.0], [153.0, 9.0], [154.0, 9.0], [155.0, 9.0], [156.0, 9.0], [157.0, 9.0], [158.0, 9.0], [159.0, 9.0], [160.0, 9.0], [161.0, 9.0], [162.0, 9.0], [163.0, 9.0], [164.0, 9.0], [165.0, 9.0], [166.0, 9.0], [167.0, 9.0], [168.0, 9.0], [169.0, 9.0], [170.0, 8.0], [171.0, 8.0], [172.0, 8.0], [173.0, 8.0], [174.0, 8.0], [175.0, 8.0], [176.0, 8.0], [177.0, 8.0], [178.0, 8.0], [179.0, 8.0], [180.0, 8.0], [181.0, 8.0], [182.0, 8.0], [183.0, 8.0], [184.0, 8.0], [185.0, 8.0], [186.0, 8.0], [187.0, 8.0], [188.0, 8.0], [189.0, 7.0], [190.0, 7.0], [191.0, 7.0], [192.0, 7.0], [193.0, 7.0], [194.0, 7.0], [195.0, 7.0], [196.0, 7.0], [197.0, 7.0], [198.0, 7.0], [199.0, 7.0], [200.0, 7.0], [201.0, 7.0], [202.0, 7.0], [203.0, 7.0], [204.0, 7.0], [205.0, 7.0], [206.0, 7.0], [207.0, 7.0], [208.0, 7.0], [209.0, 7.0], [210.0, 7.0], [211.0, 7.0], [212.0, 7.0], [213.0, 7.0], [214.0, 7.0], [215.0, 7.0], [216.0, 7.0], [217.0, 7.0], [218.0, 7.0], [219.0, 7.0], [220.0, 6.0], [221.0, 6.0], [222.0, 6.0], [223.0, 6.0], [224.0, 6.0], [225.0, 6.0], [226.0, 6.0], [227.0, 6.0], [228.0, 6.0], [229.0, 6.0], [230.0, 6.0], [231.0, 6.0], [232.0, 6.0], [233.0, 6.0], [234.0, 6.0], [235.0, 6.0], [236.0, 6.0], [237.0, 6.0], [238.0, 6.0], [239.0, 6.0], [240.0, 6.0], [241.0, 6.0], [242.0, 6.0], [243.0, 6.0], [244.0, 6.0], [245.0, 6.0], [246.0, 6.0], [247.0, 6.0], [248.0, 6.0], [249.0, 6.0], [250.0, 6.0], [251.0, 6.0], [252.0, 6.0], [253.0, 6.0], [254.0, 6.0], [255.0, 6.0], [256.0, 6.0], [257.0, 6.0], [258.0, 5.0], [259.0, 5.0], [260.0, 5.0], [261.0, 5.0], [262.0, 5.0], [263.0, 5.0], [264.0, 5.0], [265.0, 5.0], [266.0, 5.0], [267.0, 5.0], [268.0, 5.0], [269.0, 5.0], [270.0, 5.0], [271.0, 5.0], [272.0, 5.0], [273.0, 5.0], [274.0, 5.0], [275.0, 5.0], [276.0, 5.0], [277.0, 5.0], [278.0, 5.0], [279.0, 5.0], [280.0, 5.0], [281.0, 5.0], [282.0, 5.0], [283.0, 5.0], [284.0, 5.0], [285.0, 5.0], [286.0, 5.0], [287.0, 5.0], [288.0, 5.0], [289.0, 5.0], [290.0, 5.0], [291.0, 5.0], [292.0, 5.0], [293.0, 5.0], [294.0, 5.0], [295.0, 5.0], [296.0, 5.0], [297.0, 5.0], [298.0, 5.0], [299.0, 5.0], [300.0, 5.0], [301.0, 5.0], [302.0, 5.0], [303.0, 5.0], [304.0, 5.0], [305.0, 5.0], [306.0, 5.0], [307.0, 5.0], [308.0, 5.0], [309.0, 5.0], [310.0, 5.0], [311.0, 5.0], [312.0, 5.0], [313.0, 5.0], [314.0, 5.0], [315.0, 5.0], [316.0, 5.0], [317.0, 5.0], [318.0, 4.0], [319.0, 4.0], [320.0, 4.0], [321.0, 4.0], [322.0, 4.0], [323.0, 4.0], [324.0, 4.0], [325.0, 4.0], [326.0, 4.0], [327.0, 4.0], [328.0, 4.0], [329.0, 4.0], [330.0, 4.0], [331.0, 4.0], [332.0, 4.0], [333.0, 4.0], [334.0, 4.0], [335.0, 4.0], [336.0, 4.0], [337.0, 4.0], [338.0, 4.0], [339.0, 4.0], [340.0, 4.0], [341.0, 4.0], [342.0, 4.0], [343.0, 4.0], [344.0, 4.0], [345.0, 4.0], [346.0, 4.0], [347.0, 4.0], [348.0, 4.0], [349.0, 4.0], [350.0, 4.0], [351.0, 4.0], [352.0, 4.0], [353.0, 4.0], [354.0, 4.0], [355.0, 4.0], [356.0, 4.0], [357.0, 4.0], [358.0, 4.0], [359.0, 4.0], [360.0, 4.0], [361.0, 4.0], [362.0, 4.0], [363.0, 4.0], [364.0, 4.0], [365.0, 4.0], [366.0, 4.0], [367.0, 4.0], [368.0, 4.0], [369.0, 4.0], [370.0, 4.0], [371.0, 4.0], [372.0, 4.0], [373.0, 4.0], [374.0, 4.0], [375.0, 4.0], [376.0, 4.0], [377.0, 4.0], [378.0, 4.0], [379.0, 4.0], [380.0, 4.0], [381.0, 4.0], [382.0, 4.0], [383.0, 4.0], [384.0, 4.0], [385.0, 4.0], [386.0, 4.0], [387.0, 4.0], [388.0, 4.0], [389.0, 4.0], [390.0, 4.0], [391.0, 4.0], [392.0, 4.0], [393.0, 4.0], [394.0, 4.0], [395.0, 4.0], [396.0, 4.0], [397.0, 4.0], [398.0, 4.0], [399.0, 3.0], [400.0, 3.0], [401.0, 3.0], [402.0, 3.0], [403.0, 3.0], [404.0, 3.0], [405.0, 3.0], [406.0, 3.0], [407.0, 3.0], [408.0, 3.0], [409.0, 3.0], [410.0, 3.0], [411.0, 3.0], [412.0, 3.0], [413.0, 3.0], [414.0, 3.0], [415.0, 3.0], [416.0, 3.0], [417.0, 3.0], [418.0, 3.0], [419.0, 3.0], [420.0, 3.0], [421.0, 3.0], [422.0, 3.0], [423.0, 3.0], [424.0, 3.0], [425.0, 3.0], [426.0, 3.0], [427.0, 3.0], [428.0, 3.0], [429.0, 3.0], [430.0, 3.0], [431.0, 3.0], [432.0, 3.0], [433.0, 3.0], [434.0, 3.0], [435.0, 3.0], [436.0, 3.0], [437.0, 3.0], [438.0, 3.0], [439.0, 3.0], [440.0, 3.0], [441.0, 3.0], [442.0, 3.0], [443.0, 3.0], [444.0, 3.0], [445.0, 3.0], [446.0, 3.0], [447.0, 3.0], [448.0, 3.0], [449.0, 3.0], [450.0, 3.0], [451.0, 3.0], [452.0, 3.0], [453.0, 3.0], [454.0, 3.0], [455.0, 3.0], [456.0, 3.0], [457.0, 3.0], [458.0, 3.0], [459.0, 3.0], [460.0, 3.0], [461.0, 3.0], [462.0, 3.0], [463.0, 3.0], [464.0, 3.0], [465.0, 3.0], [466.0, 3.0], [467.0, 3.0], [468.0, 3.0], [469.0, 3.0], [470.0, 3.0], [471.0, 3.0], [472.0, 3.0], [473.0, 3.0], [474.0, 3.0], [475.0, 3.0], [476.0, 3.0], [477.0, 3.0], [478.0, 3.0], [479.0, 3.0], [480.0, 3.0], [481.0, 3.0], [482.0, 3.0], [483.0, 3.0], [484.0, 3.0], [485.0, 3.0], [486.0, 3.0], [487.0, 3.0], [488.0, 3.0], [489.0, 3.0], [490.0, 3.0], [491.0, 3.0], [492.0, 3.0], [493.0, 3.0], [494.0, 3.0], [495.0, 3.0], [496.0, 3.0], [497.0, 3.0], [498.0, 3.0], [499.0, 3.0], [500.0, 3.0], [501.0, 3.0], [502.0, 3.0], [503.0, 3.0], [504.0, 3.0], [505.0, 3.0], [506.0, 3.0], [507.0, 3.0], [508.0, 3.0], [509.0, 3.0], [510.0, 3.0], [511.0, 3.0], [512.0, 3.0], [513.0, 3.0], [514.0, 3.0], [515.0, 3.0], [516.0, 3.0], [517.0, 3.0], [518.0, 3.0], [519.0, 3.0], [520.0, 3.0], [521.0, 3.0], [522.0, 3.0], [523.0, 3.0], [524.0, 3.0], [525.0, 3.0], [526.0, 3.0], [527.0, 3.0], [528.0, 3.0], [529.0, 3.0], [530.0, 3.0], [531.0, 3.0], [532.0, 3.0], [533.0, 3.0], [534.0, 3.0], [535.0, 3.0], [536.0, 3.0], [537.0, 3.0], [538.0, 3.0], [539.0, 3.0], [540.0, 3.0], [541.0, 3.0], [542.0, 3.0], [543.0, 3.0], [544.0, 3.0], [545.0, 3.0], [546.0, 3.0], [547.0, 3.0], [548.0, 3.0], [549.0, 3.0], [550.0, 3.0], [551.0, 3.0], [552.0, 3.0], [553.0, 3.0], [554.0, 3.0], [555.0, 3.0], [556.0, 3.0], [557.0, 3.0], [558.0, 3.0], [559.0, 3.0], [560.0, 3.0], [561.0, 3.0], [562.0, 3.0], [563.0, 3.0], [564.0, 3.0], [565.0, 3.0], [566.0, 2.0], [567.0, 2.0], [568.0, 2.0], [569.0, 2.0], [570.0, 2.0], [571.0, 2.0], [572.0, 2.0], [573.0, 2.0], [574.0, 2.0], [575.0, 2.0], [576.0, 2.0], [577.0, 2.0], [578.0, 2.0], [579.0, 2.0], [580.0, 2.0], [581.0, 2.0], [582.0, 2.0], [583.0, 2.0], [584.0, 2.0], [585.0, 2.0], [586.0, 2.0], [587.0, 2.0], [588.0, 2.0], [589.0, 2.0], [590.0, 2.0], [591.0, 2.0], [592.0, 2.0], [593.0, 2.0], [594.0, 2.0], [595.0, 2.0], [596.0, 2.0], [597.0, 2.0], [598.0, 2.0], [599.0, 2.0], [600.0, 2.0], [601.0, 2.0], [602.0, 2.0], [603.0, 2.0], [604.0, 2.0], [605.0, 2.0], [606.0, 2.0], [607.0, 2.0], [608.0, 2.0], [609.0, 2.0], [610.0, 2.0], [611.0, 2.0], [612.0, 2.0], [613.0, 2.0], [614.0, 2.0], [615.0, 2.0], [616.0, 2.0], [617.0, 2.0], [618.0, 2.0], [619.0, 2.0], [620.0, 2.0], [621.0, 2.0], [622.0, 2.0], [623.0, 2.0], [624.0, 2.0], [625.0, 2.0], [626.0, 2.0], [627.0, 2.0], [628.0, 2.0], [629.0, 2.0], [630.0, 2.0], [631.0, 2.0], [632.0, 2.0], [633.0, 2.0], [634.0, 2.0], [635.0, 2.0], [636.0, 2.0], [637.0, 2.0], [638.0, 2.0], [639.0, 2.0], [640.0, 2.0], [641.0, 2.0], [642.0, 2.0], [643.0, 2.0], [644.0, 2.0], [645.0, 2.0], [646.0, 2.0], [647.0, 2.0], [648.0, 2.0], [649.0, 2.0], [650.0, 2.0], [651.0, 2.0], [652.0, 2.0], [653.0, 2.0], [654.0, 2.0], [655.0, 2.0], [656.0, 2.0], [657.0, 2.0], [658.0, 2.0], [659.0, 2.0], [660.0, 2.0], [661.0, 2.0], [662.0, 2.0], [663.0, 2.0], [664.0, 2.0], [665.0, 2.0], [666.0, 2.0], [667.0, 2.0], [668.0, 2.0], [669.0, 2.0], [670.0, 2.0], [671.0, 2.0], [672.0, 2.0], [673.0, 2.0], [674.0, 2.0], [675.0, 2.0], [676.0, 2.0], [677.0, 2.0], [678.0, 2.0], [679.0, 2.0], [680.0, 2.0], [681.0, 2.0], [682.0, 2.0], [683.0, 2.0], [684.0, 2.0], [685.0, 2.0], [686.0, 2.0], [687.0, 2.0], [688.0, 2.0], [689.0, 2.0], [690.0, 2.0], [691.0, 2.0], [692.0, 2.0], [693.0, 2.0], [694.0, 2.0], [695.0, 2.0], [696.0, 2.0], [697.0, 2.0], [698.0, 2.0], [699.0, 2.0], [700.0, 2.0], [701.0, 2.0], [702.0, 2.0], [703.0, 2.0], [704.0, 2.0], [705.0, 2.0], [706.0, 2.0], [707.0, 2.0], [708.0, 2.0], [709.0, 2.0], [710.0, 2.0], [711.0, 2.0], [712.0, 2.0], [713.0, 2.0], [714.0, 2.0], [715.0, 2.0], [716.0, 2.0], [717.0, 2.0], [718.0, 2.0], [719.0, 2.0], [720.0, 2.0], [721.0, 2.0], [722.0, 2.0], [723.0, 2.0], [724.0, 2.0], [725.0, 2.0], [726.0, 2.0], [727.0, 2.0], [728.0, 2.0], [729.0, 2.0], [730.0, 2.0], [731.0, 2.0], [732.0, 2.0], [733.0, 2.0], [734.0, 2.0], [735.0, 2.0], [736.0, 2.0], [737.0, 2.0], [738.0, 2.0], [739.0, 2.0], [740.0, 2.0], [741.0, 2.0], [742.0, 2.0], [743.0, 2.0], [744.0, 2.0], [745.0, 2.0], [746.0, 2.0], [747.0, 2.0], [748.0, 2.0], [749.0, 2.0], [750.0, 2.0], [751.0, 2.0], [752.0, 2.0], [753.0, 2.0], [754.0, 2.0], [755.0, 2.0], [756.0, 2.0], [757.0, 2.0], [758.0, 2.0], [759.0, 2.0], [760.0, 2.0], [761.0, 2.0], [762.0, 2.0], [763.0, 2.0], [764.0, 2.0], [765.0, 2.0], [766.0, 2.0], [767.0, 2.0], [768.0, 2.0], [769.0, 2.0], [770.0, 2.0], [771.0, 2.0], [772.0, 2.0], [773.0, 2.0], [774.0, 2.0], [775.0, 2.0], [776.0, 2.0], [777.0, 2.0], [778.0, 2.0], [779.0, 2.0], [780.0, 2.0], [781.0, 2.0], [782.0, 2.0], [783.0, 2.0], [784.0, 2.0], [785.0, 2.0], [786.0, 2.0], [787.0, 2.0], [788.0, 2.0], [789.0, 2.0], [790.0, 2.0], [791.0, 2.0], [792.0, 2.0], [793.0, 2.0], [794.0, 2.0], [795.0, 2.0], [796.0, 2.0], [797.0, 2.0], [798.0, 2.0], [799.0, 2.0], [800.0, 2.0], [801.0, 2.0], [802.0, 2.0], [803.0, 2.0], [804.0, 2.0], [805.0, 2.0], [806.0, 2.0], [807.0, 2.0], [808.0, 2.0], [809.0, 2.0], [810.0, 2.0], [811.0, 2.0], [812.0, 2.0], [813.0, 2.0], [814.0, 2.0], [815.0, 2.0], [816.0, 2.0], [817.0, 2.0], [818.0, 2.0], [819.0, 2.0], [820.0, 2.0], [821.0, 2.0], [822.0, 2.0], [823.0, 2.0], [824.0, 2.0], [825.0, 2.0], [826.0, 2.0], [827.0, 2.0], [828.0, 2.0], [829.0, 2.0], [830.0, 2.0], [831.0, 2.0], [832.0, 2.0], [833.0, 2.0], [834.0, 2.0], [835.0, 2.0], [836.0, 2.0], [837.0, 2.0], [838.0, 2.0], [839.0, 2.0], [840.0, 2.0], [841.0, 2.0], [842.0, 2.0], [843.0, 2.0], [844.0, 2.0], [845.0, 2.0], [846.0, 2.0], [847.0, 2.0], [848.0, 2.0], [849.0, 2.0], [850.0, 2.0], [851.0, 2.0], [852.0, 2.0], [853.0, 2.0], [854.0, 2.0], [855.0, 2.0], [856.0, 2.0], [857.0, 2.0], [858.0, 2.0], [859.0, 2.0], [860.0, 2.0], [861.0, 2.0], [862.0, 2.0], [863.0, 2.0], [864.0, 2.0], [865.0, 2.0], [866.0, 2.0], [867.0, 2.0], [868.0, 2.0], [869.0, 2.0], [870.0, 2.0], [871.0, 2.0], [872.0, 2.0], [873.0, 2.0], [874.0, 2.0], [875.0, 2.0], [876.0, 2.0], [877.0, 2.0], [878.0, 2.0], [879.0, 2.0], [880.0, 2.0], [881.0, 2.0], [882.0, 2.0], [883.0, 2.0], [884.0, 2.0], [885.0, 2.0], [886.0, 2.0], [887.0, 2.0], [888.0, 2.0], [889.0, 2.0], [890.0, 2.0], [891.0, 2.0], [892.0, 2.0], [893.0, 2.0], [894.0, 2.0], [895.0, 2.0], [896.0, 2.0], [897.0, 2.0], [898.0, 2.0], [899.0, 2.0], [900.0, 2.0], [901.0, 2.0], [902.0, 2.0], [903.0, 2.0], [904.0, 2.0], [905.0, 2.0], [906.0, 2.0], [907.0, 2.0], [908.0, 2.0], [909.0, 2.0], [910.0, 2.0], [911.0, 2.0], [912.0, 2.0], [913.0, 2.0], [914.0, 2.0], [915.0, 2.0], [916.0, 2.0], [917.0, 2.0], [918.0, 2.0], [919.0, 2.0], [920.0, 2.0], [921.0, 2.0], [922.0, 2.0], [923.0, 2.0], [924.0, 2.0], [925.0, 2.0], [926.0, 2.0], [927.0, 2.0], [928.0, 1.0], [929.0, 1.0], [930.0, 1.0], [931.0, 1.0], [932.0, 1.0], [933.0, 1.0], [934.0, 1.0], [935.0, 1.0], [936.0, 1.0], [937.0, 1.0], [938.0, 1.0], [939.0, 1.0], [940.0, 1.0], [941.0, 1.0], [942.0, 1.0], [943.0, 1.0], [944.0, 1.0], [945.0, 1.0], [946.0, 1.0], [947.0, 1.0], [948.0, 1.0], [949.0, 1.0], [950.0, 1.0], [951.0, 1.0], [952.0, 1.0], [953.0, 1.0], [954.0, 1.0], [955.0, 1.0], [956.0, 1.0], [957.0, 1.0], [958.0, 1.0], [959.0, 1.0], [960.0, 1.0], [961.0, 1.0], [962.0, 1.0], [963.0, 1.0], [964.0, 1.0], [965.0, 1.0], [966.0, 1.0], [967.0, 1.0], [968.0, 1.0], [969.0, 1.0], [970.0, 1.0], [971.0, 1.0], [972.0, 1.0], [973.0, 1.0], [974.0, 1.0], [975.0, 1.0], [976.0, 1.0], [977.0, 1.0], [978.0, 1.0], [979.0, 1.0], [980.0, 1.0], [981.0, 1.0], [982.0, 1.0], [983.0, 1.0], [984.0, 1.0], [985.0, 1.0], [986.0, 1.0], [987.0, 1.0], [988.0, 1.0], [989.0, 1.0], [990.0, 1.0], [991.0, 1.0], [992.0, 1.0], [993.0, 1.0], [994.0, 1.0], [995.0, 1.0], [996.0, 1.0], [997.0, 1.0], [998.0, 1.0], [999.0, 1.0], [1000.0, 1.0], [1001.0, 1.0], [1002.0, 1.0], [1003.0, 1.0], [1004.0, 1.0], [1005.0, 1.0], [1006.0, 1.0], [1007.0, 1.0], [1008.0, 1.0], [1009.0, 1.0], [1010.0, 1.0], [1011.0, 1.0], [1012.0, 1.0], [1013.0, 1.0], [1014.0, 1.0], [1015.0, 1.0], [1016.0, 1.0], [1017.0, 1.0], [1018.0, 1.0], [1019.0, 1.0], [1020.0, 1.0], [1021.0, 1.0], [1022.0, 1.0], [1023.0, 1.0], [1024.0, 1.0], [1025.0, 1.0], [1026.0, 1.0], [1027.0, 1.0], [1028.0, 1.0], [1029.0, 1.0], [1030.0, 1.0], [1031.0, 1.0], [1032.0, 1.0], [1033.0, 1.0], [1034.0, 1.0], [1035.0, 1.0], [1036.0, 1.0], [1037.0, 1.0], [1038.0, 1.0], [1039.0, 1.0], [1040.0, 1.0], [1041.0, 1.0], [1042.0, 1.0], [1043.0, 1.0], [1044.0, 1.0], [1045.0, 1.0], [1046.0, 1.0], [1047.0, 1.0], [1048.0, 1.0], [1049.0, 1.0], [1050.0, 1.0], [1051.0, 1.0], [1052.0, 1.0], [1053.0, 1.0], [1054.0, 1.0], [1055.0, 1.0], [1056.0, 1.0], [1057.0, 1.0], [1058.0, 1.0], [1059.0, 1.0], [1060.0, 1.0], [1061.0, 1.0], [1062.0, 1.0], [1063.0, 1.0], [1064.0, 1.0], [1065.0, 1.0], [1066.0, 1.0], [1067.0, 1.0], [1068.0, 1.0], [1069.0, 1.0], [1070.0, 1.0], [1071.0, 1.0], [1072.0, 1.0], [1073.0, 1.0], [1074.0, 1.0], [1075.0, 1.0], [1076.0, 1.0], [1077.0, 1.0], [1078.0, 1.0], [1079.0, 1.0], [1080.0, 1.0], [1081.0, 1.0], [1082.0, 1.0], [1083.0, 1.0], [1084.0, 1.0], [1085.0, 1.0], [1086.0, 1.0], [1087.0, 1.0], [1088.0, 1.0], [1089.0, 1.0], [1090.0, 1.0], [1091.0, 1.0], [1092.0, 1.0], [1093.0, 1.0], [1094.0, 1.0], [1095.0, 1.0], [1096.0, 1.0], [1097.0, 1.0], [1098.0, 1.0], [1099.0, 1.0], [1100.0, 1.0], [1101.0, 1.0], [1102.0, 1.0], [1103.0, 1.0], [1104.0, 1.0], [1105.0, 1.0], [1106.0, 1.0], [1107.0, 1.0], [1108.0, 1.0], [1109.0, 1.0], [1110.0, 1.0], [1111.0, 1.0], [1112.0, 1.0], [1113.0, 1.0], [1114.0, 1.0], [1115.0, 1.0], [1116.0, 1.0], [1117.0, 1.0], [1118.0, 1.0], [1119.0, 1.0], [1120.0, 1.0], [1121.0, 1.0], [1122.0, 1.0], [1123.0, 1.0], [1124.0, 1.0], [1125.0, 1.0], [1126.0, 1.0], [1127.0, 1.0], [1128.0, 1.0], [1129.0, 1.0], [1130.0, 1.0], [1131.0, 1.0], [1132.0, 1.0], [1133.0, 1.0], [1134.0, 1.0], [1135.0, 1.0], [1136.0, 1.0], [1137.0, 1.0], [1138.0, 1.0], [1139.0, 1.0], [1140.0, 1.0], [1141.0, 1.0], [1142.0, 1.0], [1143.0, 1.0], [1144.0, 1.0], [1145.0, 1.0], [1146.0, 1.0], [1147.0, 1.0], [1148.0, 1.0], [1149.0, 1.0], [1150.0, 1.0], [1151.0, 1.0], [1152.0, 1.0], [1153.0, 1.0], [1154.0, 1.0], [1155.0, 1.0], [1156.0, 1.0], [1157.0, 1.0], [1158.0, 1.0], [1159.0, 1.0], [1160.0, 1.0], [1161.0, 1.0], [1162.0, 1.0], [1163.0, 1.0], [1164.0, 1.0], [1165.0, 1.0], [1166.0, 1.0], [1167.0, 1.0], [1168.0, 1.0], [1169.0, 1.0], [1170.0, 1.0], [1171.0, 1.0], [1172.0, 1.0], [1173.0, 1.0], [1174.0, 1.0], [1175.0, 1.0], [1176.0, 1.0], [1177.0, 1.0], [1178.0, 1.0], [1179.0, 1.0], [1180.0, 1.0], [1181.0, 1.0], [1182.0, 1.0], [1183.0, 1.0], [1184.0, 1.0], [1185.0, 1.0], [1186.0, 1.0], [1187.0, 1.0], [1188.0, 1.0], [1189.0, 1.0], [1190.0, 1.0], [1191.0, 1.0], [1192.0, 1.0], [1193.0, 1.0], [1194.0, 1.0], [1195.0, 1.0], [1196.0, 1.0], [1197.0, 1.0], [1198.0, 1.0], [1199.0, 1.0], [1200.0, 1.0], [1201.0, 1.0], [1202.0, 1.0], [1203.0, 1.0], [1204.0, 1.0], [1205.0, 1.0], [1206.0, 1.0], [1207.0, 1.0], [1208.0, 1.0], [1209.0, 1.0], [1210.0, 1.0], [1211.0, 1.0], [1212.0, 1.0], [1213.0, 1.0], [1214.0, 1.0], [1215.0, 1.0], [1216.0, 1.0], [1217.0, 1.0], [1218.0, 1.0], [1219.0, 1.0], [1220.0, 1.0], [1221.0, 1.0], [1222.0, 1.0], [1223.0, 1.0], [1224.0, 1.0], [1225.0, 1.0], [1226.0, 1.0], [1227.0, 1.0], [1228.0, 1.0], [1229.0, 1.0], [1230.0, 1.0], [1231.0, 1.0], [1232.0, 1.0], [1233.0, 1.0], [1234.0, 1.0], [1235.0, 1.0], [1236.0, 1.0], [1237.0, 1.0], [1238.0, 1.0], [1239.0, 1.0], [1240.0, 1.0], [1241.0, 1.0], [1242.0, 1.0], [1243.0, 1.0], [1244.0, 1.0], [1245.0, 1.0], [1246.0, 1.0], [1247.0, 1.0], [1248.0, 1.0], [1249.0, 1.0], [1250.0, 1.0], [1251.0, 1.0], [1252.0, 1.0], [1253.0, 1.0], [1254.0, 1.0], [1255.0, 1.0], [1256.0, 1.0], [1257.0, 1.0], [1258.0, 1.0], [1259.0, 1.0], [1260.0, 1.0], [1261.0, 1.0], [1262.0, 1.0], [1263.0, 1.0], [1264.0, 1.0], [1265.0, 1.0], [1266.0, 1.0], [1267.0, 1.0], [1268.0, 1.0], [1269.0, 1.0], [1270.0, 1.0], [1271.0, 1.0], [1272.0, 1.0], [1273.0, 1.0], [1274.0, 1.0], [1275.0, 1.0], [1276.0, 1.0], [1277.0, 1.0], [1278.0, 1.0], [1279.0, 1.0], [1280.0, 1.0], [1281.0, 1.0], [1282.0, 1.0], [1283.0, 1.0], [1284.0, 1.0], [1285.0, 1.0], [1286.0, 1.0], [1287.0, 1.0], [1288.0, 1.0], [1289.0, 1.0], [1290.0, 1.0], [1291.0, 1.0], [1292.0, 1.0], [1293.0, 1.0], [1294.0, 1.0], [1295.0, 1.0], [1296.0, 1.0], [1297.0, 1.0], [1298.0, 1.0], [1299.0, 1.0], [1300.0, 1.0], [1301.0, 1.0], [1302.0, 1.0], [1303.0, 1.0], [1304.0, 1.0], [1305.0, 1.0], [1306.0, 1.0], [1307.0, 1.0], [1308.0, 1.0], [1309.0, 1.0], [1310.0, 1.0], [1311.0, 1.0], [1312.0, 1.0], [1313.0, 1.0], [1314.0, 1.0], [1315.0, 1.0], [1316.0, 1.0], [1317.0, 1.0], [1318.0, 1.0], [1319.0, 1.0], [1320.0, 1.0], [1321.0, 1.0], [1322.0, 1.0], [1323.0, 1.0], [1324.0, 1.0], [1325.0, 1.0], [1326.0, 1.0], [1327.0, 1.0], [1328.0, 1.0], [1329.0, 1.0], [1330.0, 1.0], [1331.0, 1.0], [1332.0, 1.0], [1333.0, 1.0], [1334.0, 1.0], [1335.0, 1.0], [1336.0, 1.0], [1337.0, 1.0], [1338.0, 1.0], [1339.0, 1.0], [1340.0, 1.0], [1341.0, 1.0], [1342.0, 1.0], [1343.0, 1.0], [1344.0, 1.0], [1345.0, 1.0], [1346.0, 1.0], [1347.0, 1.0], [1348.0, 1.0], [1349.0, 1.0], [1350.0, 1.0], [1351.0, 1.0], [1352.0, 1.0], [1353.0, 1.0], [1354.0, 1.0], [1355.0, 1.0], [1356.0, 1.0], [1357.0, 1.0], [1358.0, 1.0], [1359.0, 1.0], [1360.0, 1.0], [1361.0, 1.0], [1362.0, 1.0], [1363.0, 1.0], [1364.0, 1.0], [1365.0, 1.0], [1366.0, 1.0], [1367.0, 1.0], [1368.0, 1.0], [1369.0, 1.0], [1370.0, 1.0], [1371.0, 1.0], [1372.0, 1.0], [1373.0, 1.0], [1374.0, 1.0], [1375.0, 1.0], [1376.0, 1.0], [1377.0, 1.0], [1378.0, 1.0], [1379.0, 1.0], [1380.0, 1.0], [1381.0, 1.0], [1382.0, 1.0], [1383.0, 1.0], [1384.0, 1.0], [1385.0, 1.0], [1386.0, 1.0], [1387.0, 1.0], [1388.0, 1.0], [1389.0, 1.0], [1390.0, 1.0], [1391.0, 1.0], [1392.0, 1.0], [1393.0, 1.0], [1394.0, 1.0], [1395.0, 1.0], [1396.0, 1.0], [1397.0, 1.0], [1398.0, 1.0], [1399.0, 1.0], [1400.0, 1.0], [1401.0, 1.0], [1402.0, 1.0], [1403.0, 1.0], [1404.0, 1.0], [1405.0, 1.0], [1406.0, 1.0], [1407.0, 1.0], [1408.0, 1.0], [1409.0, 1.0], [1410.0, 1.0], [1411.0, 1.0], [1412.0, 1.0], [1413.0, 1.0], [1414.0, 1.0], [1415.0, 1.0], [1416.0, 1.0], [1417.0, 1.0], [1418.0, 1.0], [1419.0, 1.0], [1420.0, 1.0], [1421.0, 1.0], [1422.0, 1.0], [1423.0, 1.0], [1424.0, 1.0], [1425.0, 1.0], [1426.0, 1.0], [1427.0, 1.0], [1428.0, 1.0], [1429.0, 1.0], [1430.0, 1.0], [1431.0, 1.0], [1432.0, 1.0], [1433.0, 1.0], [1434.0, 1.0], [1435.0, 1.0], [1436.0, 1.0], [1437.0, 1.0], [1438.0, 1.0], [1439.0, 1.0], [1440.0, 1.0], [1441.0, 1.0], [1442.0, 1.0], [1443.0, 1.0], [1444.0, 1.0], [1445.0, 1.0], [1446.0, 1.0], [1447.0, 1.0], [1448.0, 1.0], [1449.0, 1.0], [1450.0, 1.0], [1451.0, 1.0], [1452.0, 1.0], [1453.0, 1.0], [1454.0, 1.0], [1455.0, 1.0], [1456.0, 1.0], [1457.0, 1.0], [1458.0, 1.0], [1459.0, 1.0], [1460.0, 1.0], [1461.0, 1.0], [1462.0, 1.0], [1463.0, 1.0], [1464.0, 1.0], [1465.0, 1.0], [1466.0, 1.0], [1467.0, 1.0], [1468.0, 1.0], [1469.0, 1.0], [1470.0, 1.0], [1471.0, 1.0], [1472.0, 1.0], [1473.0, 1.0], [1474.0, 1.0], [1475.0, 1.0], [1476.0, 1.0], [1477.0, 1.0], [1478.0, 1.0], [1479.0, 1.0], [1480.0, 1.0], [1481.0, 1.0], [1482.0, 1.0], [1483.0, 1.0], [1484.0, 1.0], [1485.0, 1.0], [1486.0, 1.0], [1487.0, 1.0], [1488.0, 1.0], [1489.0, 1.0], [1490.0, 1.0], [1491.0, 1.0], [1492.0, 1.0], [1493.0, 1.0], [1494.0, 1.0], [1495.0, 1.0], [1496.0, 1.0], [1497.0, 1.0], [1498.0, 1.0], [1499.0, 1.0], [1500.0, 1.0], [1501.0, 1.0], [1502.0, 1.0], [1503.0, 1.0], [1504.0, 1.0], [1505.0, 1.0], [1506.0, 1.0], [1507.0, 1.0], [1508.0, 1.0], [1509.0, 1.0], [1510.0, 1.0], [1511.0, 1.0], [1512.0, 1.0], [1513.0, 1.0], [1514.0, 1.0], [1515.0, 1.0], [1516.0, 1.0], [1517.0, 1.0], [1518.0, 1.0], [1519.0, 1.0], [1520.0, 1.0], [1521.0, 1.0], [1522.0, 1.0], [1523.0, 1.0], [1524.0, 1.0], [1525.0, 1.0], [1526.0, 1.0], [1527.0, 1.0], [1528.0, 1.0], [1529.0, 1.0], [1530.0, 1.0], [1531.0, 1.0], [1532.0, 1.0], [1533.0, 1.0], [1534.0, 1.0], [1535.0, 1.0], [1536.0, 1.0], [1537.0, 1.0], [1538.0, 1.0], [1539.0, 1.0], [1540.0, 1.0], [1541.0, 1.0], [1542.0, 1.0], [1543.0, 1.0], [1544.0, 1.0], [1545.0, 1.0], [1546.0, 1.0], [1547.0, 1.0], [1548.0, 1.0], [1549.0, 1.0], [1550.0, 1.0], [1551.0, 1.0], [1552.0, 1.0], [1553.0, 1.0], [1554.0, 1.0], [1555.0, 1.0], [1556.0, 1.0], [1557.0, 1.0], [1558.0, 1.0], [1559.0, 1.0], [1560.0, 1.0], [1561.0, 1.0], [1562.0, 1.0], [1563.0, 1.0], [1564.0, 1.0], [1565.0, 1.0], [1566.0, 1.0], [1567.0, 1.0], [1568.0, 1.0], [1569.0, 1.0], [1570.0, 1.0], [1571.0, 1.0], [1572.0, 1.0], [1573.0, 1.0], [1574.0, 1.0], [1575.0, 1.0], [1576.0, 1.0], [1577.0, 1.0], [1578.0, 1.0], [1579.0, 1.0], [1580.0, 1.0], [1581.0, 1.0], [1582.0, 1.0], [1583.0, 1.0], [1584.0, 1.0], [1585.0, 1.0], [1586.0, 1.0], [1587.0, 1.0], [1588.0, 1.0], [1589.0, 1.0], [1590.0, 1.0], [1591.0, 1.0], [1592.0, 1.0], [1593.0, 1.0], [1594.0, 1.0], [1595.0, 1.0], [1596.0, 1.0], [1597.0, 1.0], [1598.0, 1.0], [1599.0, 1.0], [1600.0, 1.0], [1601.0, 1.0], [1602.0, 1.0], [1603.0, 1.0], [1604.0, 1.0], [1605.0, 1.0], [1606.0, 1.0], [1607.0, 1.0], [1608.0, 1.0], [1609.0, 1.0], [1610.0, 1.0], [1611.0, 1.0], [1612.0, 1.0], [1613.0, 1.0], [1614.0, 1.0], [1615.0, 1.0], [1616.0, 1.0], [1617.0, 1.0], [1618.0, 1.0], [1619.0, 1.0], [1620.0, 1.0], [1621.0, 1.0], [1622.0, 1.0], [1623.0, 1.0], [1624.0, 1.0], [1625.0, 1.0], [1626.0, 1.0], [1627.0, 1.0], [1628.0, 1.0], [1629.0, 1.0], [1630.0, 1.0], [1631.0, 1.0], [1632.0, 1.0], [1633.0, 1.0], [1634.0, 1.0], [1635.0, 1.0], [1636.0, 1.0], [1637.0, 1.0], [1638.0, 1.0], [1639.0, 1.0], [1640.0, 1.0], [1641.0, 1.0], [1642.0, 1.0], [1643.0, 1.0], [1644.0, 1.0], [1645.0, 1.0], [1646.0, 1.0], [1647.0, 1.0], [1648.0, 1.0], [1649.0, 1.0], [1650.0, 1.0], [1651.0, 1.0], [1652.0, 1.0], [1653.0, 1.0], [1654.0, 1.0], [1655.0, 1.0], [1656.0, 1.0], [1657.0, 1.0], [1658.0, 1.0], [1659.0, 1.0], [1660.0, 1.0], [1661.0, 1.0], [1662.0, 1.0], [1663.0, 1.0], [1664.0, 1.0], [1665.0, 1.0], [1666.0, 1.0], [1667.0, 1.0], [1668.0, 1.0], [1669.0, 1.0], [1670.0, 1.0], [1671.0, 1.0], [1672.0, 1.0], [1673.0, 1.0], [1674.0, 1.0], [1675.0, 1.0], [1676.0, 1.0], [1677.0, 1.0], [1678.0, 1.0], [1679.0, 1.0], [1680.0, 1.0], [1681.0, 1.0], [1682.0, 1.0], [1683.0, 1.0], [1684.0, 1.0], [1685.0, 1.0], [1686.0, 1.0], [1687.0, 1.0], [1688.0, 1.0], [1689.0, 1.0], [1690.0, 1.0], [1691.0, 1.0], [1692.0, 1.0], [1693.0, 1.0], [1694.0, 1.0], [1695.0, 1.0], [1696.0, 1.0], [1697.0, 1.0], [1698.0, 1.0], [1699.0, 1.0], [1700.0, 1.0], [1701.0, 1.0], [1702.0, 1.0], [1703.0, 1.0], [1704.0, 1.0], [1705.0, 1.0], [1706.0, 1.0], [1707.0, 1.0], [1708.0, 1.0], [1709.0, 1.0], [1710.0, 1.0], [1711.0, 1.0], [1712.0, 1.0], [1713.0, 1.0], [1714.0, 1.0], [1715.0, 1.0], [1716.0, 1.0], [1717.0, 1.0], [1718.0, 1.0], [1719.0, 1.0], [1720.0, 1.0], [1721.0, 1.0], [1722.0, 1.0], [1723.0, 1.0], [1724.0, 1.0], [1725.0, 1.0], [1726.0, 1.0], [1727.0, 1.0], [1728.0, 1.0], [1729.0, 1.0], [1730.0, 1.0], [1731.0, 1.0], [1732.0, 1.0], [1733.0, 1.0], [1734.0, 1.0], [1735.0, 1.0], [1736.0, 1.0], [1737.0, 1.0], [1738.0, 1.0], [1739.0, 1.0], [1740.0, 1.0], [1741.0, 1.0], [1742.0, 1.0], [1743.0, 1.0], [1744.0, 1.0], [1745.0, 1.0], [1746.0, 1.0], [1747.0, 1.0], [1748.0, 1.0], [1749.0, 1.0], [1750.0, 1.0], [1751.0, 1.0], [1752.0, 1.0], [1753.0, 1.0], [1754.0, 1.0], [1755.0, 1.0], [1756.0, 1.0], [1757.0, 1.0], [1758.0, 1.0], [1759.0, 1.0], [1760.0, 1.0], [1761.0, 1.0], [1762.0, 1.0], [1763.0, 1.0], [1764.0, 1.0], [1765.0, 1.0], [1766.0, 1.0], [1767.0, 1.0], [1768.0, 1.0], [1769.0, 1.0], [1770.0, 1.0], [1771.0, 1.0], [1772.0, 1.0], [1773.0, 1.0], [1774.0, 1.0], [1775.0, 1.0], [1776.0, 1.0], [1777.0, 1.0], [1778.0, 1.0], [1779.0, 1.0], [1780.0, 1.0], [1781.0, 1.0], [1782.0, 1.0], [1783.0, 1.0], [1784.0, 1.0], [1785.0, 1.0], [1786.0, 1.0], [1787.0, 1.0], [1788.0, 1.0], [1789.0, 1.0], [1790.0, 1.0], [1791.0, 1.0], [1792.0, 1.0], [1793.0, 1.0], [1794.0, 1.0], [1795.0, 1.0], [1796.0, 1.0], [1797.0, 1.0], [1798.0, 1.0], [1799.0, 1.0], [1800.0, 1.0], [1801.0, 1.0], [1802.0, 1.0], [1803.0, 1.0], [1804.0, 1.0], [1805.0, 1.0], [1806.0, 1.0], [1807.0, 1.0], [1808.0, 1.0], [1809.0, 1.0], [1810.0, 1.0], [1811.0, 1.0], [1812.0, 1.0], [1813.0, 1.0], [1814.0, 1.0], [1815.0, 1.0], [1816.0, 1.0], [1817.0, 1.0], [1818.0, 1.0], [1819.0, 1.0], [1820.0, 1.0], [1821.0, 1.0], [1822.0, 1.0], [1823.0, 1.0], [1824.0, 1.0], [1825.0, 1.0], [1826.0, 1.0], [1827.0, 1.0], [1828.0, 1.0], [1829.0, 1.0], [1830.0, 1.0], [1831.0, 1.0], [1832.0, 1.0], [1833.0, 1.0], [1834.0, 1.0], [1835.0, 1.0], [1836.0, 1.0], [1837.0, 1.0], [1838.0, 1.0], [1839.0, 1.0], [1840.0, 1.0], [1841.0, 1.0], [1842.0, 1.0], [1843.0, 1.0], [1844.0, 1.0], [1845.0, 1.0], [1846.0, 1.0], [1847.0, 1.0], [1848.0, 1.0], [1849.0, 1.0], [1850.0, 1.0], [1851.0, 1.0], [1852.0, 1.0], [1853.0, 1.0], [1854.0, 1.0], [1855.0, 1.0], [1856.0, 1.0], [1857.0, 1.0], [1858.0, 1.0], [1859.0, 1.0], [1860.0, 1.0], [1861.0, 1.0], [1862.0, 1.0], [1863.0, 1.0], [1864.0, 1.0], [1865.0, 1.0], [1866.0, 1.0], [1867.0, 1.0], [1868.0, 1.0], [1869.0, 1.0], [1870.0, 1.0], [1871.0, 1.0], [1872.0, 1.0], [1873.0, 1.0], [1874.0, 1.0], [1875.0, 1.0], [1876.0, 1.0], [1877.0, 1.0], [1878.0, 1.0], [1879.0, 1.0], [1880.0, 1.0], [1881.0, 1.0], [1882.0, 1.0], [1883.0, 1.0], [1884.0, 1.0], [1885.0, 1.0], [1886.0, 1.0], [1887.0, 1.0], [1888.0, 1.0], [1889.0, 1.0], [1890.0, 1.0], [1891.0, 1.0], [1892.0, 1.0], [1893.0, 1.0], [1894.0, 1.0], [1895.0, 1.0], [1896.0, 1.0], [1897.0, 1.0], [1898.0, 1.0], [1899.0, 1.0], [1900.0, 1.0], [1901.0, 1.0], [1902.0, 1.0], [1903.0, 1.0], [1904.0, 1.0], [1905.0, 1.0], [1906.0, 1.0], [1907.0, 1.0], [1908.0, 1.0], [1909.0, 1.0], [1910.0, 1.0], [1911.0, 1.0], [1912.0, 1.0], [1913.0, 1.0], [1914.0, 1.0], [1915.0, 1.0], [1916.0, 1.0], [1917.0, 1.0], [1918.0, 1.0], [1919.0, 1.0], [1920.0, 1.0], [1921.0, 1.0], [1922.0, 1.0], [1923.0, 1.0], [1924.0, 1.0], [1925.0, 1.0], [1926.0, 1.0], [1927.0, 1.0], [1928.0, 1.0], [1929.0, 1.0], [1930.0, 1.0], [1931.0, 1.0], [1932.0, 1.0], [1933.0, 1.0], [1934.0, 1.0], [1935.0, 1.0], [1936.0, 1.0], [1937.0, 1.0], [1938.0, 1.0], [1939.0, 1.0], [1940.0, 1.0], [1941.0, 1.0], [1942.0, 1.0], [1943.0, 1.0], [1944.0, 1.0], [1945.0, 1.0], [1946.0, 1.0], [1947.0, 1.0], [1948.0, 1.0], [1949.0, 1.0], [1950.0, 1.0], [1951.0, 1.0], [1952.0, 1.0], [1953.0, 1.0], [1954.0, 1.0], [1955.0, 1.0], [1956.0, 1.0], [1957.0, 1.0], [1958.0, 1.0], [1959.0, 1.0], [1960.0, 1.0], [1961.0, 1.0], [1962.0, 1.0], [1963.0, 1.0], [1964.0, 1.0], [1965.0, 1.0], [1966.0, 1.0], [1967.0, 1.0], [1968.0, 1.0], [1969.0, 1.0], [1970.0, 1.0], [1971.0, 1.0], [1972.0, 1.0], [1973.0, 1.0], [1974.0, 1.0], [1975.0, 1.0], [1976.0, 1.0], [1977.0, 1.0], [1978.0, 1.0], [1979.0, 1.0], [1980.0, 1.0], [1981.0, 1.0], [1982.0, 1.0], [1983.0, 1.0], [1984.0, 1.0], [1985.0, 1.0], [1986.0, 1.0], [1987.0, 1.0], [1988.0, 1.0], [1989.0, 1.0], [1990.0, 1.0], [1991.0, 1.0], [1992.0, 1.0], [1993.0, 1.0], [1994.0, 1.0], [1995.0, 1.0], [1996.0, 1.0], [1997.0, 1.0], [1998.0, 1.0], [1999.0, 1.0], [2000.0, 1.0], [2001.0, 1.0], [2002.0, 1.0], [2003.0, 1.0], [2004.0, 1.0], [2005.0, 1.0], [2006.0, 1.0], [2007.0, 1.0], [2008.0, 1.0], [2009.0, 1.0], [2010.0, 1.0], [2011.0, 1.0], [2012.0, 1.0], [2013.0, 1.0], [2014.0, 1.0], [2015.0, 1.0], [2016.0, 1.0], [2017.0, 1.0], [2018.0, 1.0], [2019.0, 1.0], [2020.0, 1.0], [2021.0, 1.0], [2022.0, 1.0], [2023.0, 1.0], [2024.0, 1.0], [2025.0, 1.0], [2026.0, 1.0], [2027.0, 1.0], [2028.0, 1.0], [2029.0, 1.0], [2030.0, 1.0], [2031.0, 1.0], [2032.0, 1.0], [2033.0, 1.0], [2034.0, 1.0], [2035.0, 1.0], [2036.0, 1.0], [2037.0, 1.0], [2038.0, 1.0], [2039.0, 1.0], [2040.0, 1.0], [2041.0, 1.0], [2042.0, 1.0], [2043.0, 1.0], [2044.0, 1.0], [2045.0, 1.0], [2046.0, 1.0], [2047.0, 1.0], [2048.0, 1.0], [2049.0, 1.0], [2050.0, 1.0], [2051.0, 1.0], [2052.0, 1.0], [2053.0, 1.0], [2054.0, 1.0], [2055.0, 1.0], [2056.0, 1.0], [2057.0, 1.0], [2058.0, 1.0], [2059.0, 1.0], [2060.0, 1.0], [2061.0, 1.0], [2062.0, 1.0], [2063.0, 1.0], [2064.0, 1.0], [2065.0, 1.0], [2066.0, 1.0], [2067.0, 1.0], [2068.0, 1.0], [2069.0, 1.0], [2070.0, 1.0], [2071.0, 1.0], [2072.0, 1.0], [2073.0, 1.0], [2074.0, 1.0], [2075.0, 1.0], [2076.0, 1.0], [2077.0, 1.0], [2078.0, 1.0], [2079.0, 1.0], [2080.0, 1.0], [2081.0, 1.0], [2082.0, 1.0], [2083.0, 1.0], [2084.0, 1.0], [2085.0, 1.0], [2086.0, 1.0], [2087.0, 1.0], [2088.0, 1.0], [2089.0, 1.0], [2090.0, 1.0], [2091.0, 1.0], [2092.0, 1.0], [2093.0, 1.0], [2094.0, 1.0], [2095.0, 1.0], [2096.0, 1.0], [2097.0, 1.0], [2098.0, 1.0], [2099.0, 1.0], [2100.0, 1.0], [2101.0, 1.0], [2102.0, 1.0], [2103.0, 1.0], [2104.0, 1.0], [2105.0, 1.0], [2106.0, 1.0], [2107.0, 1.0], [2108.0, 1.0], [2109.0, 1.0], [2110.0, 1.0], [2111.0, 1.0], [2112.0, 1.0], [2113.0, 1.0], [2114.0, 1.0], [2115.0, 1.0], [2116.0, 1.0], [2117.0, 1.0], [2118.0, 1.0], [2119.0, 1.0], [2120.0, 1.0], [2121.0, 1.0], [2122.0, 1.0], [2123.0, 1.0], [2124.0, 1.0], [2125.0, 1.0], [2126.0, 1.0], [2127.0, 1.0], [2128.0, 1.0], [2129.0, 1.0], [2130.0, 1.0], [2131.0, 1.0], [2132.0, 1.0], [2133.0, 1.0], [2134.0, 1.0], [2135.0, 1.0], [2136.0, 1.0], [2137.0, 1.0], [2138.0, 1.0], [2139.0, 1.0], [2140.0, 1.0], [2141.0, 1.0], [2142.0, 1.0], [2143.0, 1.0], [2144.0, 1.0], [2145.0, 1.0], [2146.0, 1.0], [2147.0, 1.0], [2148.0, 1.0], [2149.0, 1.0], [2150.0, 1.0], [2151.0, 1.0], [2152.0, 1.0], [2153.0, 1.0], [2154.0, 1.0], [2155.0, 1.0], [2156.0, 1.0], [2157.0, 1.0], [2158.0, 1.0], [2159.0, 1.0], [2160.0, 1.0], [2161.0, 1.0], [2162.0, 1.0], [2163.0, 1.0], [2164.0, 1.0], [2165.0, 1.0], [2166.0, 1.0], [2167.0, 1.0], [2168.0, 1.0], [2169.0, 1.0], [2170.0, 1.0], [2171.0, 1.0], [2172.0, 1.0], [2173.0, 1.0], [2174.0, 1.0], [2175.0, 1.0], [2176.0, 1.0], [2177.0, 1.0], [2178.0, 1.0], [2179.0, 1.0], [2180.0, 1.0], [2181.0, 1.0], [2182.0, 1.0], [2183.0, 1.0], [2184.0, 1.0], [2185.0, 1.0], [2186.0, 1.0], [2187.0, 1.0], [2188.0, 1.0], [2189.0, 1.0], [2190.0, 1.0], [2191.0, 1.0], [2192.0, 1.0], [2193.0, 1.0], [2194.0, 1.0], [2195.0, 1.0], [2196.0, 1.0], [2197.0, 1.0], [2198.0, 1.0], [2199.0, 1.0], [2200.0, 1.0], [2201.0, 1.0], [2202.0, 1.0], [2203.0, 1.0], [2204.0, 1.0], [2205.0, 1.0], [2206.0, 1.0], [2207.0, 1.0], [2208.0, 1.0], [2209.0, 1.0], [2210.0, 1.0], [2211.0, 1.0], [2212.0, 1.0], [2213.0, 1.0], [2214.0, 1.0], [2215.0, 1.0], [2216.0, 1.0], [2217.0, 1.0], [2218.0, 1.0], [2219.0, 1.0], [2220.0, 1.0], [2221.0, 1.0], [2222.0, 1.0], [2223.0, 1.0], [2224.0, 1.0], [2225.0, 1.0], [2226.0, 1.0], [2227.0, 1.0], [2228.0, 1.0], [2229.0, 1.0], [2230.0, 1.0], [2231.0, 1.0], [2232.0, 1.0], [2233.0, 1.0], [2234.0, 1.0], [2235.0, 1.0], [2236.0, 1.0], [2237.0, 1.0], [2238.0, 1.0], [2239.0, 1.0], [2240.0, 1.0], [2241.0, 1.0], [2242.0, 1.0], [2243.0, 1.0], [2244.0, 1.0], [2245.0, 1.0], [2246.0, 1.0], [2247.0, 1.0], [2248.0, 1.0], [2249.0, 1.0], [2250.0, 1.0], [2251.0, 1.0], [2252.0, 1.0], [2253.0, 1.0], [2254.0, 1.0], [2255.0, 1.0], [2256.0, 1.0], [2257.0, 1.0], [2258.0, 1.0], [2259.0, 1.0], [2260.0, 1.0], [2261.0, 1.0], [2262.0, 1.0], [2263.0, 1.0], [2264.0, 1.0], [2265.0, 1.0], [2266.0, 1.0], [2267.0, 1.0], [2268.0, 1.0], [2269.0, 1.0], [2270.0, 1.0], [2271.0, 1.0], [2272.0, 1.0], [2273.0, 1.0], [2274.0, 1.0], [2275.0, 1.0], [2276.0, 1.0], [2277.0, 1.0], [2278.0, 1.0], [2279.0, 1.0], [2280.0, 1.0], [2281.0, 1.0], [2282.0, 1.0], [2283.0, 1.0], [2284.0, 1.0], [2285.0, 1.0], [2286.0, 1.0], [2287.0, 1.0], [2288.0, 1.0], [2289.0, 1.0], [2290.0, 1.0], [2291.0, 1.0], [2292.0, 1.0], [2293.0, 1.0], [2294.0, 1.0], [2295.0, 1.0], [2296.0, 1.0], [2297.0, 1.0], [2298.0, 1.0], [2299.0, 1.0], [2300.0, 1.0], [2301.0, 1.0], [2302.0, 1.0], [2303.0, 1.0], [2304.0, 1.0], [2305.0, 1.0], [2306.0, 1.0], [2307.0, 1.0], [2308.0, 1.0], [2309.0, 1.0], [2310.0, 1.0], [2311.0, 1.0], [2312.0, 1.0], [2313.0, 1.0], [2314.0, 1.0], [2315.0, 1.0], [2316.0, 1.0], [2317.0, 1.0], [2318.0, 1.0], [2319.0, 1.0], [2320.0, 1.0], [2321.0, 1.0], [2322.0, 1.0], [2323.0, 1.0], [2324.0, 1.0], [2325.0, 1.0], [2326.0, 1.0], [2327.0, 1.0], [2328.0, 1.0], [2329.0, 1.0], [2330.0, 1.0], [2331.0, 1.0], [2332.0, 1.0], [2333.0, 1.0], [2334.0, 1.0], [2335.0, 1.0], [2336.0, 1.0], [2337.0, 1.0], [2338.0, 1.0], [2339.0, 1.0], [2340.0, 1.0], [2341.0, 1.0], [2342.0, 1.0], [2343.0, 1.0], [2344.0, 1.0], [2345.0, 1.0], [2346.0, 1.0], [2347.0, 1.0], [2348.0, 1.0], [2349.0, 1.0], [2350.0, 1.0], [2351.0, 1.0], [2352.0, 1.0], [2353.0, 1.0], [2354.0, 1.0], [2355.0, 1.0], [2356.0, 1.0], [2357.0, 1.0], [2358.0, 1.0], [2359.0, 1.0], [2360.0, 1.0], [2361.0, 1.0], [2362.0, 1.0], [2363.0, 1.0], [2364.0, 1.0], [2365.0, 1.0], [2366.0, 1.0], [2367.0, 1.0], [2368.0, 1.0], [2369.0, 1.0], [2370.0, 1.0], [2371.0, 1.0], [2372.0, 1.0], [2373.0, 1.0], [2374.0, 1.0], [2375.0, 1.0], [2376.0, 1.0], [2377.0, 1.0], [2378.0, 1.0], [2379.0, 1.0], [2380.0, 1.0], [2381.0, 1.0], [2382.0, 1.0], [2383.0, 1.0], [2384.0, 1.0], [2385.0, 1.0], [2386.0, 1.0], [2387.0, 1.0], [2388.0, 1.0], [2389.0, 1.0], [2390.0, 1.0], [2391.0, 1.0], [2392.0, 1.0], [2393.0, 1.0], [2394.0, 1.0], [2395.0, 1.0], [2396.0, 1.0], [2397.0, 1.0], [2398.0, 1.0], [2399.0, 1.0], [2400.0, 1.0], [2401.0, 1.0], [2402.0, 1.0], [2403.0, 1.0], [2404.0, 1.0], [2405.0, 1.0], [2406.0, 1.0], [2407.0, 1.0], [2408.0, 1.0], [2409.0, 1.0], [2410.0, 1.0], [2411.0, 1.0], [2412.0, 1.0], [2413.0, 1.0], [2414.0, 1.0], [2415.0, 1.0], [2416.0, 1.0], [2417.0, 1.0], [2418.0, 1.0], [2419.0, 1.0], [2420.0, 1.0], [2421.0, 1.0], [2422.0, 1.0], [2423.0, 1.0], [2424.0, 1.0], [2425.0, 1.0], [2426.0, 1.0], [2427.0, 1.0], [2428.0, 1.0], [2429.0, 1.0], [2430.0, 1.0], [2431.0, 1.0], [2432.0, 1.0], [2433.0, 1.0], [2434.0, 1.0], [2435.0, 1.0], [2436.0, 1.0], [2437.0, 1.0], [2438.0, 1.0], [2439.0, 1.0], [2440.0, 1.0], [2441.0, 1.0], [2442.0, 1.0], [2443.0, 1.0], [2444.0, 1.0], [2445.0, 1.0], [2446.0, 1.0], [2447.0, 1.0], [2448.0, 1.0], [2449.0, 1.0], [2450.0, 1.0], [2451.0, 1.0], [2452.0, 1.0], [2453.0, 1.0], [2454.0, 1.0], [2455.0, 1.0], [2456.0, 1.0], [2457.0, 1.0], [2458.0, 1.0], [2459.0, 1.0], [2460.0, 1.0], [2461.0, 1.0], [2462.0, 1.0], [2463.0, 1.0], [2464.0, 1.0], [2465.0, 1.0], [2466.0, 1.0], [2467.0, 1.0], [2468.0, 1.0], [2469.0, 1.0], [2470.0, 1.0], [2471.0, 1.0], [2472.0, 1.0], [2473.0, 1.0], [2474.0, 1.0], [2475.0, 1.0], [2476.0, 1.0], [2477.0, 1.0], [2478.0, 1.0], [2479.0, 1.0], [2480.0, 1.0], [2481.0, 1.0], [2482.0, 1.0], [2483.0, 1.0], [2484.0, 1.0], [2485.0, 1.0], [2486.0, 1.0], [2487.0, 1.0], [2488.0, 1.0], [2489.0, 1.0], [2490.0, 1.0], [2491.0, 1.0], [2492.0, 1.0], [2493.0, 1.0], [2494.0, 1.0], [2495.0, 1.0], [2496.0, 1.0], [2497.0, 1.0], [2498.0, 1.0], [2499.0, 1.0], [2500.0, 1.0], [2501.0, 1.0], [2502.0, 1.0], [2503.0, 1.0], [2504.0, 1.0], [2505.0, 1.0], [2506.0, 1.0], [2507.0, 1.0], [2508.0, 1.0], [2509.0, 1.0], [2510.0, 1.0], [2511.0, 1.0], [2512.0, 1.0], [2513.0, 1.0], [2514.0, 1.0], [2515.0, 1.0], [2516.0, 1.0], [2517.0, 1.0], [2518.0, 1.0], [2519.0, 1.0], [2520.0, 1.0], [2521.0, 1.0], [2522.0, 1.0], [2523.0, 1.0], [2524.0, 1.0], [2525.0, 1.0], [2526.0, 1.0], [2527.0, 1.0], [2528.0, 1.0], [2529.0, 1.0], [2530.0, 1.0], [2531.0, 1.0], [2532.0, 1.0], [2533.0, 1.0], [2534.0, 1.0], [2535.0, 1.0], [2536.0, 1.0], [2537.0, 1.0], [2538.0, 1.0], [2539.0, 1.0], [2540.0, 1.0], [2541.0, 1.0], [2542.0, 1.0], [2543.0, 1.0], [2544.0, 1.0], [2545.0, 1.0], [2546.0, 1.0], [2547.0, 1.0], [2548.0, 1.0], [2549.0, 1.0], [2550.0, 1.0], [2551.0, 1.0], [2552.0, 1.0], [2553.0, 1.0], [2554.0, 1.0], [2555.0, 1.0], [2556.0, 1.0]]}, \"axes\": [{\"images\": [], \"bbox\": [0.125, 0.125, 0.775, 0.775], \"xlim\": [0.0, 3000.0], \"id\": \"el25140171029353472\", \"collections\": [], \"xscale\": \"linear\", \"ylim\": [0.0, 1400.0], \"xdomain\": [0.0, 3000.0], \"markers\": [], \"sharex\": [], \"axesbg\": \"#FFFFFF\", \"axesbgalpha\": null, \"ydomain\": [0.0, 1400.0], \"paths\": [], \"zoomable\": true, \"yscale\": \"linear\", \"sharey\": [], \"texts\": [], \"lines\": [{\"linewidth\": 1.0, \"xindex\": 0, \"zorder\": 2, \"coordinates\": \"data\", \"id\": \"el25140170344989752\", \"color\": \"#0000FF\", \"yindex\": 1, \"data\": \"data01\", \"dasharray\": \"10,0\", \"alpha\": 1}], \"axes\": [{\"scale\": \"linear\", \"grid\": {\"gridOn\": false}, \"position\": \"bottom\", \"tickformat\": null, \"nticks\": 7, \"tickvalues\": null, \"fontsize\": 10.0}, {\"scale\": \"linear\", \"grid\": {\"gridOn\": false}, \"position\": \"left\", \"tickformat\": null, \"nticks\": 8, \"tickvalues\": null, \"fontsize\": 10.0}]}], \"width\": 480.0});\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.xscale('linear')\n",
    "plt.yscale('linear')\n",
    "plt.plot(ranks, sorted_counts)\n",
    "mpld3.display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In log-space such rank vs frequency graphs resemble linear functions. This observation is known as *Zipf's Law*, and can be formalized as follows. Let \\\\(r\\_w\\\\) be the rank of a word \\\\(w\\\\), and \\\\(f\\_w\\\\) its frequency, then we have:\n",
    "\n",
    "$$\n",
    "  f_w \\propto \\frac{1}{r_w}.\n",
    "$$\n",
    "\n",
    "## Inserting Out-of-Vocabularly Tokens\n",
    "The long tail of infrequent words is a problem for LMs because it means there will always be words with zero counts in your training set. There are various solutions to this problem. For example, when it comes to calculating the LM perplexity we could remove words that do not appear in the training set. This overcomes the problem of infinite perplexity but doesn't solve the actual issue: the LM assigns too low probability to unseen words. Moreover, the problem only gets worse when one considers n-gram models with larger \\\\(n\\\\), because these will encounter many unseen n-grams, which, when removed, will only leave small fractions of the original sentences.\n",
    "\n",
    "The principled solution to this problem is smoothing, and we will discuss it in more detail later. Before we get there we present a simple preprocessing step that generally simplifies the handling of unseen words, and gives rise to a simple smoothing heuristic. Namely, we replace unseen words in the test corpus with an out-of-vocabularly token, say `OOV`. This means that LMs can still work with a fixed vocabularly that consists of all training words, and the `OOV` token. Now we just need a way to estimate the probability of the `OOV` token to avoid the infinite perplexity problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[BAR]',\n",
       " '[OOV]',\n",
       " '[OOV]',\n",
       " 'to',\n",
       " 'the',\n",
       " '[OOV]',\n",
       " '[/BAR]',\n",
       " '[BAR]',\n",
       " '[/BAR]',\n",
       " '[BAR]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OOV = '[OOV]'\n",
    "def replace_OOVs(vocab,data):\n",
    "    \"\"\"\n",
    "    Replace every word not within the vocabulary with the `OOV` symbol.\n",
    "    Args:\n",
    "        vocab: the reference vocabulary.\n",
    "        data: the sequence of tokens to replace words within\n",
    "\n",
    "    Returns:\n",
    "        a version of `data` where each word not in `vocab` is replaced by the `OOV` symbol.\n",
    "    \"\"\"\n",
    "\n",
    "    return [word if word in vocab else OOV for word in data]\n",
    "\n",
    "replace_OOVs(baseline.vocab, test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in practice we can enable language models to operate on any test set vocabulary if we decorate the model with following wrapper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OOVAwareLM(LanguageModel):\n",
    "    \"\"\"\n",
    "    This LM converts out of vocabulary tokens to a special OOV token before their probability is calculated.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_lm, oov_count, oov=OOV):\n",
    "        super().__init__(base_lm.vocab, base_lm.order)\n",
    "        self.base_lm = base_lm\n",
    "        self.oov = oov\n",
    "        self.oov_count = oov_count\n",
    "        \n",
    "    def probability(self, word, *history):\n",
    "        actual_word, norm = (word,1) if word in self.base_lm.vocab else (self.oov,self.oov_count)\n",
    "        return self.base_lm.probality(actual_word, *history) / norm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wrapper takes unseen words (outside of the vocabulary) and maps them to the `OOV` token. If the underlying base model has a probability for this token the `OOVAwareLM` returns that probability divided by the number of out-of-vocabulary words we expect. This number can be set easily when a fixed test corpus is available: it amounts to counting how many words in the test corpus do not appear in the base LM vocabulary.\n",
    "\n",
    "A simple way to (heuristically) estimate the `OOV` probability is to replace the first encounter of each word in the training set with the `OOV` token. Now we can estimate LMs as before, and will automatically get some estimate of the `OOV` probability. The underlying assumption of this heuristic is that the probability of unseen words is identical to the probability of encountering a new word. We illustrate the two operations of this method in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[OOV]', 'AA', '[OOV]', 'BB', 'AA']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inject_OOVs(data):\n",
    "    \"\"\"\n",
    "    Uses a heuristic to inject OOV symbols into a dataset.\n",
    "    Args:\n",
    "        data: the sequence of words to inject OOVs into.\n",
    "\n",
    "    Returns: the new sequence with OOV symbols injected.\n",
    "    \"\"\"\n",
    "\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for word in data:\n",
    "        if word in seen:\n",
    "            result.append(word)\n",
    "        else:\n",
    "            result.append(OOV)\n",
    "            seen.add(word)\n",
    "    return result\n",
    "\n",
    "inject_OOVs([\"AA\",\"AA\",\"BB\",\"BB\",\"AA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply this to our training and test set, and create a new uniform model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "928.0000000011556"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_train = inject_OOVs(train)\n",
    "oov_vocab = set(oov_train)\n",
    "oov_test = replace_OOVs(oov_vocab, test)\n",
    "oov_baseline = UniformLM(oov_vocab)\n",
    "perplexity(oov_baseline,oov_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Language Models\n",
    "The uniform LM is obviously not good at modelling actual language. To improve upon this baseline, we can estimate the conditional n-gram distributions from the training data. To this end let us first introduce one parameter $\\param_{w,h}$ for each word $w$ and history $h$ of length $n - 1$, and define a parametrized language model $p_\\params$: \n",
    "\n",
    "$$\n",
    "\\prob_\\params(w|h) = \\param_{w,h}\n",
    "$$\n",
    "\n",
    "Training an n-gram LM amounts to estimating \\\\(\\params\\\\) from some training set \\\\(\\train=(w_1,\\ldots,w_n)\\\\).\n",
    "One way to do this is to choose the \\\\(\\params\\\\) that maximizes the log-likelihood of \\\\(\\train\\\\):\n",
    "$$\n",
    "\\params^* = \\argmax_\\params \\log p_\\params(\\train)\n",
    "$$\n",
    "\n",
    "As it turns out, this maximum-log-likelihood estimate (MLE) can calculated in closed form, simply by counting:\n",
    "$$\n",
    "\\param^*_{w,h} = \\frac{\\counts{\\train}{w,h}}{\\counts{\\train}{h}} \n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\counts{D}{e} = \\text{Count of event } e \\text{ in }  D \n",
    "$$\n",
    "\n",
    "Here the event $h$ means seeing the history $h$, and $w,h$ seeing the history $h$ followed by word $w$.  \n",
    "\n",
    "Many LM variants can be implemented simply by estimating the counts in the nominator and denominator differently. We therefore introduce an interface for such count-based LMs. This will help us later to implement LM variants by modifying the counts of a base-LM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CountLM(LanguageModel):\n",
    "    \"\"\"\n",
    "    A Language Model that uses counts of events and histories to calculate probabilities of words in context.\n",
    "    \"\"\"\n",
    "    @abc.abstractmethod\n",
    "    def counts(self, word_and_history):\n",
    "        pass\n",
    "    @abc.abstractmethod\n",
    "    def norm(self, history):\n",
    "        pass\n",
    "    \n",
    "    def probability(self, word, *history):\n",
    "        sub_history = tuple(history[-(self.order-1):]) if self.order > 1 else () \n",
    "        return self.counts((word,) + sub_history) / self.norm(sub_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use this to code up a generic NGram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramLM(CountLM):\n",
    "    def __init__(self, train, order):\n",
    "        \"\"\"\n",
    "        Create an NGram language model.\n",
    "        Args:\n",
    "            train: list of training tokens.\n",
    "            order: order of the LM.\n",
    "        \"\"\"\n",
    "        super().__init__(set(train), order)\n",
    "        self._counts = collections.defaultdict(float)\n",
    "        self._norm = collections.defaultdict(float)\n",
    "        for i in range(self.order, len(train)):\n",
    "            history = tuple(train[i - self.order + 1 : i])\n",
    "            word = train[i]\n",
    "            self._counts[(word,) + history] += 1.0\n",
    "            self._norm[history] += 1.0\n",
    "    def counts(self, word_and_history):\n",
    "        return self._counts[word_and_history]\n",
    "    def norm(self, history):\n",
    "        return self._norm[history]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train a unigram model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2f839432e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unigram = NGramLM(oov_train,1)\n",
    "def plot_probabilities(lm, context = (), how_many = 10):    \n",
    "    probs = sorted([(word,lm.probability(word,*context)) for word in lm.vocab], key=lambda x:x[1], reverse=True)[:how_many]\n",
    "    util.plot_bar_graph([prob for _,prob in probs], [word for word, _ in probs])\n",
    "plot_probabilities(unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unigram LM has substantially reduced (and hence better) perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.11302463241343"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(unigram,oov_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also look at the language the unigram LM generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[BAR]',\n",
       " '[/BAR]',\n",
       " '[OOV]',\n",
       " '[OOV]',\n",
       " 'There',\n",
       " 'any',\n",
       " \"'\",\n",
       " 'But',\n",
       " 'maybe',\n",
       " '[BAR]']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(unigram, [], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram LM\n",
    "\n",
    "The unigram model ignores any correlation between consecutive words in a sentence. The next best model to overcome this shortcoming is a bigram model. This model conditions the probability of the current word on the previous word. Let us construct such model from the training data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxhLoSQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2f8208b7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigram = NGramLM(oov_train,2)\n",
    "plot_probabilities(bigram, ('I',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a more peaked distribution conditioned on \"I\" than in the case of the unigram model. Let us see how the bigram LM generates language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[BAR] Brooklyn New York to himself [/BAR] [BAR] The real [/BAR] [BAR] [OOV] country [/BAR] [BAR] Verse 2 [/BAR] [BAR] Are you ' s up What the [OOV] [OOV] [/BAR] [BAR]\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(sample(bigram, ['[BAR]'], 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the bigram model improve perplexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(bigram,oov_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the bigram model has the problem we tried to avoid using the OOV preprocessing method above. The problem is that there are contexts in which the OOV word (and other words) hasn't been seen, and hence it receives 0 probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.probability(\"[OOV]\",\"money\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing\n",
    "\n",
    "The general problem is that maximum likelhood estimates will always underestimate the true probability of some words, and in turn overestimate the (context-dependent) probabilities of other words. To overcome this issue we aim to _smooth_ the probabilities and move mass from seen events to unseen events.\n",
    "\n",
    "### Laplace Smoothing\n",
    "\n",
    "The easiest way to overcome the problem of zero probabilities is to simply add pseudo counts to each event in the dataset (in a Bayesian setting this amounts to a maximum posteriori estimate under a dirichlet prior on parameters).\n",
    "\n",
    "$$\n",
    "\\param^{\\alpha}_{w,h} = \\frac{\\counts{\\train}{h,w} + \\alpha}{\\counts{\\train}{h} + \\alpha \\lvert V \\rvert } \n",
    "$$\n",
    "\n",
    "Let us implement this in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010660980810234541"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LaplaceLM(CountLM):\n",
    "    def __init__(self, base_lm, alpha):\n",
    "        super().__init__(base_lm.vocab, base_lm.order)\n",
    "        self.base_lm = base_lm\n",
    "        self.alpha = alpha\n",
    "    def counts(self, word_and_history):\n",
    "        return self.base_lm.counts(word_and_history) + self.alpha\n",
    "    def norm(self, history):\n",
    "        return self.base_lm.norm(history) + self.alpha * len(self.base_lm.vocab)\n",
    "\n",
    "laplace_bigram = LaplaceLM(bigram, 0.1) \n",
    "laplace_bigram.probability(\"[OOV]\",\"money\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give a better perplexity value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.51634219903197"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(laplace_bigram,oov_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted counts\n",
    "It is often useful to think of smoothing algorithms as un-smoothed Maximum-Likelhood estimators that work with *adjusted* n-gram counts in the numerator, and fixed history counts in the denominator. This allows us to see how counts from high-frequency words are reduced, and counts of unseen words increased. If these changes are too big, the smoothing method is likely not very effective.\n",
    "\n",
    "Let us reformulate the laplace LM using adjusted counts. Note that we since we have histories with count 0, we do need to increase the original denominator by a small \\\\(\\epsilon\\\\) to avoid division by zero. \n",
    "$$\n",
    "\\begin{split}\n",
    "\\counts{\\train,\\alpha}{h,w} &= \\param^{\\alpha}_{w,h} \\cdot (\\counts{\\train}{h} +  \\epsilon)\\\\\\\\\n",
    "\\counts{\\train,\\alpha}{h} &= \\counts{\\train}{h} + \\epsilon\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585.0, 564.5934362811013)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AdjustedLaplaceLM(CountLM):\n",
    "    def __init__(self, base_lm, alpha):\n",
    "        super().__init__(base_lm.vocab, base_lm.order)\n",
    "        self.base_lm = base_lm\n",
    "        self.alpha = alpha\n",
    "        self.eps = 0.000001\n",
    "    def counts(self, word_and_history):\n",
    "        history = word_and_history[1:]\n",
    "        word = word_and_history[0]\n",
    "        return 0.0 if word not in self.vocab else \\\n",
    "               (self.base_lm.counts(word_and_history) + self.alpha) / \\\n",
    "               (self.base_lm.norm(history) + self.alpha * len(self.base_lm.vocab)) * \\\n",
    "               (self.base_lm.norm(history) + self.eps)\n",
    "    def norm(self, history):\n",
    "        return self.base_lm.norm(history) + self.eps\n",
    "\n",
    "adjusted_laplace_bigram = AdjustedLaplaceLM(bigram, 0.1)\n",
    "bigram.counts((OOV,OOV)), adjusted_laplace_bigram.counts((OOV,OOV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that for high frequency words the absolute counts are altered quite substantially. This is unfortunate because for high frequency words we would expect the counts to be relatively accurate. Can we test more generally wether our adjusted counts are sensible?\n",
    "\n",
    "One option is to compare the adjusted counts to average counts in a held-out set. For example, for words of count 0 in the training set, how does their average count in the held-out set compare to their adjusted count in the smoothed model? To test this we need some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_counts(train_lm, test_lm, vocab):\n",
    "    \"\"\"\n",
    "    Calculate a dictionary from counts in the training-LM to counts in the test-LM. \n",
    "    \"\"\"\n",
    "    avg_test_counts = collections.defaultdict(float)\n",
    "    norm = collections.defaultdict(float)\n",
    "    for ngram in util.cross_product([list(train_lm.vocab)] * train_lm.order):\n",
    "        train_count = train_lm.counts(ngram)\n",
    "        test_count = test_lm.counts(ngram)\n",
    "        avg_test_counts[train_count] += test_count\n",
    "        norm[train_count] += 1.0\n",
    "    for c in avg_test_counts.keys():\n",
    "        avg_test_counts[c] /= norm[c]\n",
    "    return avg_test_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate a table of training counts, test counts, and smoothed counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0.0041     0.0059\n",
      "1     0.4125     0.3034\n",
      "2     1.0836     0.7959\n",
      "3     1.7626     1.2479\n",
      "4     2.9123     1.7678\n",
      "5     3.6724     2.4055\n"
     ]
    }
   ],
   "source": [
    "test_bigram = NGramLM(oov_test, 2)\n",
    "joint_vocab = set(oov_test + oov_train)\n",
    "avg_test_counts = avg_counts(bigram, test_bigram, joint_vocab)\n",
    "avg_laplace_counts = avg_counts(bigram, AdjustedLaplaceLM(bigram, 0.1), joint_vocab)\n",
    "for count in range(0, 6):\n",
    "    print(\"{} {:10.4f} {:10.4f}\".format(count, avg_test_counts[count], avg_laplace_counts[count]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "For a given context the smoothing methods discussed above shift mass uniformly across the words that haven't been seen in this context. This makes sense when the words are not in the vocabularly. However, when words are in the vocabularly but just have not been seen in the given context, we can do better because we can leverage statistics about the word from other contexts. In particular, we can *back-off* to the statistics of \\\\(n-1\\\\) grams. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0010548523206751054, 0.0010548523206751054)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_laplace_bigram.probability('skies','skies'), adjusted_laplace_bigram.probability('[/BAR]','skies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple technique to use the \\\\(n-1\\\\) gram statistics is interpolation. Here we  compose the probability of a word as the weighted sum of the probability of an \\\\(n\\\\)-gram model \\\\(p'\\\\) and a back-off \\\\(n-1\\\\) model \\\\(p''\\\\): \n",
    "\n",
    "$$\n",
    "\\prob_{\\alpha}(w_i|w_{i-n},\\ldots,w_{i-1}) = \\alpha \\cdot \\prob'(w_i|w_{i-n},\\ldots,w_{i-1}) + (1 - \\alpha) \\cdot \\prob''(w_i|w_{i-n+1},\\ldots,w_{i-1})\n",
    "$$\n",
    "\n",
    "A Python implementation of this model can be seen below. We also show how a more likely unigram now has a higher probability in a context it hasn't seen in before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0001662701118815446, 0.09764798462230231)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class InterpolatedLM(LanguageModel):\n",
    "    def __init__(self, main, backoff, alpha):\n",
    "        super().__init__(main.vocab, main.order)\n",
    "        self.main = main\n",
    "        self.backoff = backoff\n",
    "        self.alpha = alpha\n",
    "    def probability(self, word, *history):\n",
    "        return self.alpha * self.main.probability(word,*history) + \\\n",
    "               (1.0 - self.alpha) * self.backoff.probability(word,*history)\n",
    "\n",
    "interpolated = InterpolatedLM(adjusted_laplace_bigram,unigram,0.01)\n",
    "interpolated.probability('skies','skies'), interpolated.probability('[/BAR]','skies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now find a good $\\alpha$ parameter to optimise for perplexity. Notice that in practice this should be done using a development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "<style>\n",
       "\n",
       "</style>\n",
       "\n",
       "<div id=\"fig_el241398420815908005074980690\"></div>\n",
       "<script>\n",
       "function mpld3_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(mpld3) !== \"undefined\" && mpld3._mpld3IsLoaded){\n",
       "   // already loaded: just create the figure\n",
       "   !function(mpld3){\n",
       "       \n",
       "       mpld3.draw_figure(\"fig_el241398420815908005074980690\", {\"width\": 480.0, \"axes\": [{\"xdomain\": [0.0, 1.0], \"sharey\": [], \"paths\": [], \"axes\": [{\"position\": \"bottom\", \"tickformat\": null, \"nticks\": 6, \"fontsize\": 10.0, \"tickvalues\": null, \"scale\": \"linear\", \"grid\": {\"gridOn\": false}}, {\"position\": \"left\", \"tickformat\": null, \"nticks\": 8, \"fontsize\": 10.0, \"tickvalues\": null, \"scale\": \"linear\", \"grid\": {\"gridOn\": false}}], \"axesbgalpha\": null, \"markers\": [], \"sharex\": [], \"collections\": [], \"yscale\": \"linear\", \"id\": \"el24139842081587384\", \"texts\": [], \"bbox\": [0.125, 0.125, 0.775, 0.775], \"ylim\": [45.0, 80.0], \"axesbg\": \"#FFFFFF\", \"images\": [], \"ydomain\": [45.0, 80.0], \"xlim\": [0.0, 1.0], \"lines\": [{\"linewidth\": 1.0, \"alpha\": 1, \"color\": \"#0000FF\", \"zorder\": 2, \"coordinates\": \"data\", \"xindex\": 0, \"yindex\": 1, \"dasharray\": \"10,0\", \"id\": \"el24139842022034792\", \"data\": \"data01\"}], \"xscale\": \"linear\", \"zoomable\": true}], \"height\": 320.0, \"id\": \"el24139842081590800\", \"plugins\": [{\"type\": \"reset\"}, {\"enabled\": false, \"type\": \"zoom\", \"button\": true}, {\"enabled\": false, \"type\": \"boxzoom\", \"button\": true}], \"data\": {\"data01\": [[0.0, 78.12529362403502], [0.1, 64.18965602468867], [0.2, 57.547909002234285], [0.30000000000000004, 53.482031938971595], [0.4, 50.818123773770786], [0.5, 49.095724655180675], [0.6000000000000001, 48.124642575720316], [0.7000000000000001, 47.875434794372126], [0.8, 48.500856130800784], [0.9, 50.60716743812544], [1.0, 59.51634219903197]]}});\n",
       "   }(mpld3);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/mpld3\n",
       "   require.config({paths: {d3: \"https://mpld3.github.io/js/d3.v3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      mpld3_load_lib(\"https://mpld3.github.io/js/mpld3.v0.2.js\", function(){\n",
       "         \n",
       "         mpld3.draw_figure(\"fig_el241398420815908005074980690\", {\"width\": 480.0, \"axes\": [{\"xdomain\": [0.0, 1.0], \"sharey\": [], \"paths\": [], \"axes\": [{\"position\": \"bottom\", \"tickformat\": null, \"nticks\": 6, \"fontsize\": 10.0, \"tickvalues\": null, \"scale\": \"linear\", \"grid\": {\"gridOn\": false}}, {\"position\": \"left\", \"tickformat\": null, \"nticks\": 8, \"fontsize\": 10.0, \"tickvalues\": null, \"scale\": \"linear\", \"grid\": {\"gridOn\": false}}], \"axesbgalpha\": null, \"markers\": [], \"sharex\": [], \"collections\": [], \"yscale\": \"linear\", \"id\": \"el24139842081587384\", \"texts\": [], \"bbox\": [0.125, 0.125, 0.775, 0.775], \"ylim\": [45.0, 80.0], \"axesbg\": \"#FFFFFF\", \"images\": [], \"ydomain\": [45.0, 80.0], \"xlim\": [0.0, 1.0], \"lines\": [{\"linewidth\": 1.0, \"alpha\": 1, \"color\": \"#0000FF\", \"zorder\": 2, \"coordinates\": \"data\", \"xindex\": 0, \"yindex\": 1, \"dasharray\": \"10,0\", \"id\": \"el24139842022034792\", \"data\": \"data01\"}], \"xscale\": \"linear\", \"zoomable\": true}], \"height\": 320.0, \"id\": \"el24139842081590800\", \"plugins\": [{\"type\": \"reset\"}, {\"enabled\": false, \"type\": \"zoom\", \"button\": true}, {\"enabled\": false, \"type\": \"boxzoom\", \"button\": true}], \"data\": {\"data01\": [[0.0, 78.12529362403502], [0.1, 64.18965602468867], [0.2, 57.547909002234285], [0.30000000000000004, 53.482031938971595], [0.4, 50.818123773770786], [0.5, 49.095724655180675], [0.6000000000000001, 48.124642575720316], [0.7000000000000001, 47.875434794372126], [0.8, 48.500856130800784], [0.9, 50.60716743812544], [1.0, 59.51634219903197]]}});\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & mpld3\n",
       "    mpld3_load_lib(\"https://mpld3.github.io/js/d3.v3.min.js\", function(){\n",
       "         mpld3_load_lib(\"https://mpld3.github.io/js/mpld3.v0.2.js\", function(){\n",
       "                 \n",
       "                 mpld3.draw_figure(\"fig_el241398420815908005074980690\", {\"width\": 480.0, \"axes\": [{\"xdomain\": [0.0, 1.0], \"sharey\": [], \"paths\": [], \"axes\": [{\"position\": \"bottom\", \"tickformat\": null, \"nticks\": 6, \"fontsize\": 10.0, \"tickvalues\": null, \"scale\": \"linear\", \"grid\": {\"gridOn\": false}}, {\"position\": \"left\", \"tickformat\": null, \"nticks\": 8, \"fontsize\": 10.0, \"tickvalues\": null, \"scale\": \"linear\", \"grid\": {\"gridOn\": false}}], \"axesbgalpha\": null, \"markers\": [], \"sharex\": [], \"collections\": [], \"yscale\": \"linear\", \"id\": \"el24139842081587384\", \"texts\": [], \"bbox\": [0.125, 0.125, 0.775, 0.775], \"ylim\": [45.0, 80.0], \"axesbg\": \"#FFFFFF\", \"images\": [], \"ydomain\": [45.0, 80.0], \"xlim\": [0.0, 1.0], \"lines\": [{\"linewidth\": 1.0, \"alpha\": 1, \"color\": \"#0000FF\", \"zorder\": 2, \"coordinates\": \"data\", \"xindex\": 0, \"yindex\": 1, \"dasharray\": \"10,0\", \"id\": \"el24139842022034792\", \"data\": \"data01\"}], \"xscale\": \"linear\", \"zoomable\": true}], \"height\": 320.0, \"id\": \"el24139842081590800\", \"plugins\": [{\"type\": \"reset\"}, {\"enabled\": false, \"type\": \"zoom\", \"button\": true}, {\"enabled\": false, \"type\": \"boxzoom\", \"button\": true}], \"data\": {\"data01\": [[0.0, 78.12529362403502], [0.1, 64.18965602468867], [0.2, 57.547909002234285], [0.30000000000000004, 53.482031938971595], [0.4, 50.818123773770786], [0.5, 49.095724655180675], [0.6000000000000001, 48.124642575720316], [0.7000000000000001, 47.875434794372126], [0.8, 48.500856130800784], [0.9, 50.60716743812544], [1.0, 59.51634219903197]]}});\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = np.arange(0,1.1,0.1)\n",
    "perplexities = [perplexity(InterpolatedLM(adjusted_laplace_bigram,unigram,alpha),oov_test) for alpha in alphas]\n",
    "fig = plt.figure()\n",
    "plt.plot(alphas,perplexities)\n",
    "mpld3.display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backoff \n",
    "Instead of combining probabilities for all words given a context, it makes sense to back-off only when no counts for a given event are available and rely on available counts where possible. \n",
    "\n",
    "A particularly simple, if not to say stupid, backoff method is [Stupid Backoff](http://www.aclweb.org/anthology/D07-1090.pdf). Let \\\\(w\\\\) be a word and \\\\(h_{n}\\\\) be an n-gram of length \\\\(n\\\\):  \n",
    "\n",
    "$$\n",
    "\\prob_{\\mbox{Stupid}}(w|h_n) = \n",
    "\\begin{cases}\n",
    "\\frac{\\counts{\\train}{h_n,w}}{\\counts{\\train}{h_n}}  &= \\mbox{if }\\counts{\\train}{h_n,w} > 0 \\\\\\\\\n",
    "\\prob_{\\mbox{Stupid}}(w|h_{n-1}) & \\mbox{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class StupidBackoff(LanguageModel):\n",
    "    def __init__(self, main, backoff, alpha):\n",
    "        super().__init__(main.vocab, main.order)\n",
    "        self.main = main\n",
    "        self.backoff = backoff\n",
    "        self.alpha = alpha\n",
    "    def probability(self, word, *history):\n",
    "        return self.main.probability(word,*history) \\\n",
    "          if self.main.counts((word,)+tuple(history)) > 0 \\\n",
    "          else self.alpha * self.backoff.probability(word,*history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the Stupid LM is very effective when it comes to *extrinsic* evaluations, but it doesn't represent a valid probability distribution: when you sum over the probabilities of all words given a history, the result may be larger than 1. This is the case because the main n-gram model probabilities for all non-zero count words already sum to 1. The fact that the probabilities sum to more than 1 makes perplexity values meaningless. The code below illustrates the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.071144317734964"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stupid = StupidBackoff(bigram, unigram, 0.1)\n",
    "sum([stupid.probability(word, 'the') for word in stupid.vocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are several \"proper backoff models\" that do not have this problem, e.g. the Katz-Backoff method. We refer to other material below for a deeper discussion of these.\n",
    "\n",
    "### Background Reading\n",
    "\n",
    "* Jurafsky & Martin, Speech and Language Processing: Chapter 4, N-Grams.\n",
    "* Bill MacCartney, Stanford NLP Lunch Tutorial: [Smoothing](http://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
