{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\Xs}{\\mathcal{X}}\n",
    "\\newcommand{\\Ys}{\\mathcal{Y}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\repr}{\\mathbf{f}}\n",
    "\\newcommand{\\repry}{\\mathbf{g}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\DeclareMathOperator{\\argmin}{argmin}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "\\newcommand{\\indi}{\\mathbb{I}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Prediction\n",
    "\n",
    "In general there seems to be no emerging unified _theory of NLP_ to emerge, and most textbooks and courses explain NLP as \n",
    "\n",
    "> a collection of problems, techniques, ideas, frameworks, etc. that really are not tied together in any reasonable way other than the fact that they have to do with NLP.\n",
    ">\n",
    ">  -- <cite>[Hal Daume](http://nlpers.blogspot.co.uk/2012/12/teaching-intro-grad-nlp.html)</cite>\n",
    "\n",
    "That's not to say though that they aren't cross-cutting patterns, general best practices and recipes that reoccur frequently. One such reoccurring pattern I found in many NLP papers and systems is what I like to refer to as the *[structured prediction](http://en.wikipedia.org/wiki/Structured_prediction) recipe*.\n",
    "\n",
    "The general goal we address with this recipe is the following. We like to, given some input structure $\\x \\in \\Xs$, predict a suitable output structure $\\y \\in \\Ys$. In the context of NLP $\\Xs$ may be the set of documents, and $\\Ys$ a set of document classes (e.g. sports and business). $\\Xs$ may also be the set of French sentences, and $\\Ys$ the set of English sentences. In this case each $\\y \\in \\Ys$ is a _structured_ object (hence the use of bold face). This structured **output** aspect of the problem has profound consequences on the methods used to address it (as opposed to structure in the input, which one can deal with relatively straight-forwardly). Generally we are also given some _training set_ $\\train$ which may contain input-output pairs $(\\x,\\y)_i$, but possibly also just input data (in unsupervised learning), annotated data but for a different task (multi-task, distant, weak supervision, etc.), or some mixture of it. \n",
    "\n",
    "\n",
    "With the above ingredients the recipe goes as follows: \n",
    "\n",
    " 1. Define a parametrized _model_ or _scoring function_ \\\\(s_\\params(\\x,\\y)\\\\) that measures the _match_ of a given \\\\(\\x\\\\) and \\\\(\\y\\\\). This model incorporates the background knowledge we have about the task, and usually involves _representations_  $\\repr_\\params(\\x)$ and $\\repry_\\params(\\y)$ that capture the character of the input and output relevant to the task---in the past representations were mostly handcrafted but with the advent of deep learning they tend to be learned from data now. The model is also controlled by a set of real-valued parameters \\\\(\\params\\\\).\n",
    " 1. _Learn_ the parameters \\\\(\\params\\\\) from the training data \\\\(\\train\\\\), ideally such that performance on the task of choice is optimized. This learning step usually involves some _continuous optimization problem_ that serves as a surrogate for the task performance we like to maximize. This problem defines a _loss function_ we have to minimise to optimise task performance.  \n",
    " 1. Given an input \\\\(\\x\\\\) _predict_ the highest-scoring (and hence best-matching) output structure $$ \\y^* = \\argmax_{\\y\\in\\Ys} s(\\x,\\y) $$ to serve as the prediction of the model. Given that most of the structures we care about in NLP are discrete, this task usually involves some _discrete optimization problem_ and is important not just at _test time_ when the model is applied, but often also during training (as, intuitively, we like to train the model such that it _predicts well_). \n",
    " \n",
    "You will see examples of this recipe throughout the book, as well as frameworks and methods that make this recipe possible. It's worthwhile noting that good NLPers usually combine three skills in accordance with this recipe: 1. modelling, 2. continuous optimization and 3. discrete optimization. For the second and third some basic mathematical background is generally useful, for the first some understanding of the language phenomena you seek to model can be helpful. It's probably fair to say that modelling is the most important bit, and in practice this often shows through the fact that clever features (part of the model) beat clever optimization quite often. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example\n",
    "\n",
    "It is difficult to show this recipe in action without going into too much depth. We will do so by introducing a very simple structured prediction example that illustrates all aspects of the recipe. We consider the task of machine translation from English to Japanese, and make the following (extremely unrealistic) assumptions:\n",
    "* There are only 4 target Japanese sentences we care about. All English sentences we encounter can be translated to one of these 4 sentences. \n",
    "* The lengths of the source English and target Japanese sentence are sufficient representations of the problem. That is, one can decide which Japanese sentence is the correct translation be comparing its lengths to the length of the English sentence. \n",
    "\n",
    "It is important to remember that while this is a toy example, the main steps and ingredients are found in every statistical NLP application. \n",
    "\n",
    "### Input and Output Spaces\n",
    "First we define our input and output spaces $\\Xs$ and $\\Ys$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I want coffe', 'コヒがほし'),\n",
       " ('where is the restroom?', 'This is not Japanese at all')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_space = ['I want coffe', 'where is the restroom?', 'As he crossed toward the pharmacy at the corner he involuntarily turned his head because of a burst of light that had ricocheted from his temple']\n",
    "y_space = ['コヒがほし','This is not Japanese at all','What the hell, Sebastian! Fix this example']\n",
    "data = list(zip(x_space,y_space))\n",
    "train = data[:2]\n",
    "test = data[2:]\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Modelling consists of two steps: finding suitable representations of $\\x$ and $\\y$, and measuring their compatibility using a scoring function. \n",
    "#### Representation\n",
    "We incorporate the assumptions that only sentence lengths matter when translating by defining $\\repr(\\x)=|\\x|$ and $\\repry(\\y)=|\\y|$ where $|\\cdot|$ counts the number of characters in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"Calculate a representation of the input `x`.\"\"\"\n",
    "    return len(x)\n",
    "def g(y):\n",
    "    \"\"\"Calculate a representation of the output `y`.\"\"\"\n",
    "    return len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring Function\n",
    "The scoring function only cares about the difference in length between the sentences when assessing the compatibility between the English and Japanese sentence:\n",
    "\n",
    "$$\n",
    "s_\\param(\\x,\\y) = \\param |\\repr(\\x) - \\repry(\\y)|\n",
    "$$\n",
    "\n",
    "The single parameter $\\param\\in\\{-1,1\\}$ determines whether sentences should have similar ($1$) or dissimilar ($-1$) lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def s(theta,x,y):\n",
    "    \"\"\"Measure the compatibility of sentences `x` and `y` using parameter `theta`\"\"\"\n",
    "    return theta * abs(f(x) - g(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function\n",
    "We need to find out what the optimal $\\param$ is. While this is obvious for us in this case ($\\param=1$), we want to find the optimal value _programmatically_. We do so by providing a simple loss function: for every training instance we receive a penalty of $1$ if the highest scoring translation is not the correct one. We can formalise this as follows:\n",
    "\n",
    "$$\n",
    "l(\\param)=\\sum_{(\\x,\\y) \\in \\train} \\indi(\\y=\\y'_{\\param})\n",
    "$$\n",
    "where $\\y'_{\\param} \\in \\Ys$ is the highest scoring Japanese sentence \n",
    "$$\\y'_{\\param}=\\argmax_\\y s_\\param(\\x,\\y).$$\n",
    "\n",
    "In python this can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(theta, data):\n",
    "    \"\"\"Measure the total number of errors made when predicting with parameter `theta` on training set `data`\"\"\"\n",
    "    total = 0.0\n",
    "    for x,y in data:\n",
    "        max_score = -math.inf\n",
    "        result = None\n",
    "        for y_guess in y_space:\n",
    "            score = s(theta,x,y_guess)\n",
    "            if score > max_score:\n",
    "                result = y_guess\n",
    "                max_score = score\n",
    "        if y_guess != y:\n",
    "            total += 1.0\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "With the model and loss defined, we can now _learn_ the model parameters. In our case we have restricted $\\param \\in \\{-1,1\\}$ and hence learning simply amounts to calculating the loss for each value, and then picking the value with lowest loss.\n",
    "\n",
    "$$\n",
    "\\param^* = \\argmin_{\\param \\in \\{-1,1\\}} l(\\param) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_star = 1.0 if loss(1.0, train) < loss(-1.0, train) else -1.0\n",
    "theta_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Now that we have a trained model we can use it to predict translations on a test set. This amounts to finding \n",
    "\n",
    "$$\\y'_{\\param}=\\argmax_\\y s_\\param(\\x,\\y).$$ \n",
    "\n",
    "Notice that we solved this problem already within the calculation of the loss. This is a common pattern in structured prediction: when minimising a training loss we often have to solve the prediction problem in the inner loop. In many cases this is the core bottleneck of computation. \n",
    "\n",
    "For completeness, we show how one can find the highest scoring Japanese sentence $\\y$ for a given English sentence $\\x$ but note that this code can already be found the in loss code above. In practice one would try to reuse the prediction code below in the loss function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'コヒがほし'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(theta, x):\n",
    "    \"\"\"Find the most compatible output sentence given the input sentence `x` and parameter `theta`\"\"\"\n",
    "    max_score = -math.inf\n",
    "    result = None\n",
    "    for y_guess in y_space:\n",
    "        score = s(theta,x,y_guess)\n",
    "        if score > max_score:\n",
    "            result = y_guess\n",
    "            max_score = score\n",
    "    return result\n",
    "\n",
    "# We can now predict the test set translation:\n",
    "predict(theta_star, test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Practice\n",
    "In practice one needs to extend the above approach along several axes to be useful:\n",
    "\n",
    "1. Usually the feature representations and scoring functions are more elaborate. They may involve several non-linear transformations of both input and output. \n",
    "\n",
    "1. The space of parameters is usually multi-dimensional (with possibly millions of dimensions). It is hence impossible to exhaustively search through this space as we have done above. Instead you will find numeric optimisation algorithms, usually variants of Stochastic Gradient Descent. \n",
    "\n",
    "1. The size of the output space is often exponentional with respect to the input problem size. For example, in machine translation we have to search through *all* Japanese sentences, not just 4. This means that we cannot exhaustively search this space either. Dynamic Programming, Greedy algorithms and integer linear programming are common approaches to solve this issue.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Reading\n",
    "\n",
    "* Noah Smith, [Linguistic Structure Prediction](http://www.cs.cmu.edu/~nasmith/LSP/). Notice that his book can be downloaded for free when logging in through UCL. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
