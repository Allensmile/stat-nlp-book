{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# %cd .. \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import statnlpbook.util as util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\Xs}{\\mathcal{X}}\n",
    "\\newcommand{\\Ys}{\\mathcal{Y}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\source}{\\mathbf{s}}\n",
    "\\newcommand{\\target}{\\mathbf{t}}\n",
    "\\newcommand{\\ssource}{s}\n",
    "\\newcommand{\\starget}{t}\n",
    "\\newcommand{\\repr}{\\mathbf{f}}\n",
    "\\newcommand{\\repry}{\\mathbf{g}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\DeclareMathOperator{\\argmin}{argmin}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "\\newcommand{\\length}[1]{\\text{length}(#1) }\n",
    "\\newcommand{\\indi}{\\mathbb{I}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-based Machine Translation\n",
    "\n",
    "Machine Translation (MT) is one of the canonical NLP applications, and one that nowadays most people are familiar with, primarily through online translation services of the major search engine providers. While there is still some way to go before machines can provide fluent and flawless translations, in particular for more distant language pairs like English and Japanese, progress in this field has been remarkable. \n",
    "\n",
    "In this chapter we will illustrate the foundations of this progress, and focus on word-based machine translation models. In such models words are the basic unit of translation. Nowadays the field has mostly moved to phrase and syntax-based approaches, but the word-based approach is still important, both from a foundational point of view, and as sub-component in more complex approaches.\n",
    "\n",
    "## MT as Structured Prediction\n",
    "\n",
    "Formally we will see MT as the task of translating a _source_ sentence \\\\(\\source\\\\) to a _target_ sentence \\\\(\\target\\\\). We can tackle the problem using the [structured prediction recipe](structured_prediction.ipynb): We define a parametrised model \\\\(s_\\params(\\target,\\source)\\\\) that measures how well a target  \\\\(\\target\\\\) sentence matches a source sentence \\\\(\\source\\\\), learn the parameters \\\\(\\params\\\\) from training data, and then find\n",
    "\n",
    "\\begin{equation}\\label{decode-mt}\n",
    "\\argmax_\\target s_\\params(\\target,\\source)\n",
    "\\end{equation}\n",
    "\n",
    "as translation of \\\\(\\source\\\\). Different _statistical_ MT approaches, in this view, differ primarily in how \\\\(s\\\\) is defined, \\\\(\\params\\\\) are learned, and how the \\\\(\\argmax\\\\) is found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Channel Model for MT\n",
    "\n",
    "Many Word-based MT systems, as well as those based on more advanced representations, rely on a [Noisy Channel](https://www.dropbox.com/s/gfucv538m6anmgd/NoisyChannel.pdf?dl=0) model as choice for the scoring function \\\\(s_\\params\\\\). In this approach to MT we effectively model the translation process *in reverse*. That is, we assume that a probabilistic process (the speaker's brain) first generates the target sentence \\\\(\\target\\\\) according to the distribution \\\\(\\prob(\\target)\\\\). Then the target sentence \\\\(\\target\\\\) is transmitted through a _noisy channel_ \\\\(\\prob(\\source|\\target)\\\\) that translates \\\\(\\target\\\\) into \\\\(\\source\\\\). \n",
    "\n",
    "Hence translation is seen as adding noise to a clean \\\\(\\target\\\\). This _generative story_ defines a _joint distribution_ over target and source sentences \\\\(\\prob(\\source,\\target) = \\prob(\\target) \\prob(\\source|\\target) \\\\). We can in turn operate this distribution in the direction we actually care about: to infer a target sentence \\\\(\\target\\\\) given a source sentence \\\\(\\source\\\\) we find the _maximum a posteriori_ sentence\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{decode-nc}\n",
    "\\target^* = \\argmax_\\target \\prob(\\target | \\source) = \\argmax_\\target \\prob(\\target) \\, \\prob(\\source | \\target). \n",
    "\\end{equation}\n",
    "\n",
    "For the structured prediction recipe this means setting \n",
    "\n",
    "$$\n",
    "s_\\params(\\target,\\source) = \\prob(\\target) \\, \\prob(\\source | \\target). \n",
    "$$\n",
    "\n",
    "In the noisy channel approach for MT the distribution \\\\(\\prob(\\target)\\\\) that generates the target sentence is usually referred to as [language model](/template/statnlpbook/01_tasks/01_languagemodels), and the noisy channel is called the _translation model_. As we have discussed language models earlier, in this chapter we focus on the translation model \\\\(\\prob(\\source|\\target)\\\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Naive Baseline Translation Model\n",
    "The most straightforward translation model translates words one-by-one, in the order of appearance:\n",
    "$$\n",
    "\\prob_\\params^\\text{Naive}(\\ssource|\\starget) = \\prod_i^{\\length{\\source}} \\param_{\\ssource_i,\\starget_i}\n",
    "$$\n",
    "where \\\\(\\param_{\\ssource,\\starget} \\\\) is the probability of translating \\\\(\\starget\\\\) as \\\\(\\ssource\\\\). \\\\(\\params\\\\) is often referred to as *translation table*.\n",
    "\n",
    "For many language pairs one can acquire training sets \\\\(\\train=\\left( \\left(\\source_i,\\target_i\\right) \\right)_{i=1}^n \\\\) of paired source and target sentences. For example, for French and English the [Aligned Hansards](http://www.isi.edu/natural-language/download/hansard/) of the Parliament of Canada can be used. Given such a training set \\\\(\\train\\\\) we can learn the parameters \\\\(\\params\\\\) using the [Maximum Likelhood estimator](/template/statnlpbook/02_methods/0x_mle). In the case of our Naive model this amounts to setting\n",
    "$$\n",
    "\\param_{\\ssource,\\starget} = \\frac{\\counts{\\train}{s,t}}{\\counts{\\train}{t}} \n",
    "$$\n",
    "Here \\\\(\\counts{\\train}{s,t}\\\\) is the number of times we see target word \\\\(t\\\\) translated as source word \\\\(s\\\\), and \\\\(\\counts{\\train}{t}\\\\) the number of times we the target word \\\\(t\\\\) in total.\n",
    "\n",
    "### Training the Naive Model\n",
    "Let us prepare some toy data to show how train this naive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['the', 'house', 'is', 'small'], ['das', 'Haus', 'ist', 'klein']),\n",
       " (['the', 'house', 'is', 'small'], ['klein', 'ist', 'das', 'Haus']),\n",
       " (['a', 'man', 'is', 'tall'], ['ein', 'Mann', 'ist', 'groß']),\n",
       " (['my', 'house', 'is', 'small'], ['klein', 'ist', 'mein', 'Haus'])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw = [\n",
    "    (\"the house is small\", \"das Haus ist klein\"),\n",
    "    (\"the house is small\", \"klein ist das Haus\"),\n",
    "    (\"a man is tall\", \"ein Mann ist groß\"),\n",
    "    (\"my house is small\", \"klein ist mein Haus\")\n",
    "]\n",
    "train = [(t.split(\" \"), s.split(\" \")) for t,s in train_raw]\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we transformed raw strings into tokenised sentences via `split`. This dataset can be used to train the naive model as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "def learn_naive_model(data):\n",
    "    norm = defaultdict(float)\n",
    "    counts = defaultdict(float)\n",
    "    for target, source in data:\n",
    "        for i in range(0, len(target)):\n",
    "            norm[target[i]] += 1.0\n",
    "            counts[(source[i],target[i])] += 1.0\n",
    "    result = {}\n",
    "    for (source,target),score in counts.items():\n",
    "        result[(source,target)] = score / norm[target]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train on the toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEACAYAAABF+UbAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDHFJapghLkkN+3/NpxckWt7QZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1034f6eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = learn_naive_model(train)\n",
    "source_for_is, scores = zip(*[item for item in table.items() if item[0][1] == 'is'])\n",
    "list(source_for_is)\n",
    "util.plot_bar_graph(scores, source_for_is)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
