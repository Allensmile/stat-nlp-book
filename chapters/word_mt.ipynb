{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# %cd .. \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import statnlpbook.util as util\n",
    "util.execute_notebook('language_models.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\Xs}{\\mathcal{X}}\n",
    "\\newcommand{\\Ys}{\\mathcal{Y}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\aligns}{\\mathbf{a}}\n",
    "\\newcommand{\\align}{a}\n",
    "\\newcommand{\\source}{\\mathbf{s}}\n",
    "\\newcommand{\\target}{\\mathbf{t}}\n",
    "\\newcommand{\\ssource}{s}\n",
    "\\newcommand{\\starget}{t}\n",
    "\\newcommand{\\repr}{\\mathbf{f}}\n",
    "\\newcommand{\\repry}{\\mathbf{g}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\DeclareMathOperator{\\argmin}{argmin}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "\\newcommand{\\length}[1]{\\text{length}(#1) }\n",
    "\\newcommand{\\indi}{\\mathbb{I}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-based Machine Translation\n",
    "\n",
    "Machine Translation (MT) is one of the canonical NLP applications, and one that nowadays most people are familiar with, primarily through online translation services of the major search engine providers. While there is still some way to go before machines can provide fluent and flawless translations, in particular for more distant language pairs like English and Japanese, progress in this field has been remarkable. \n",
    "\n",
    "In this chapter we will illustrate the foundations of this progress, and focus on word-based machine translation models. In such models words are the basic unit of translation. Nowadays the field has mostly moved to phrase and syntax-based approaches, but the word-based approach is still important, both from a foundational point of view, and as sub-component in more complex approaches.\n",
    "\n",
    "## MT as Structured Prediction\n",
    "\n",
    "Formally we will see MT as the task of translating a _source_ sentence \\\\(\\source\\\\) to a _target_ sentence \\\\(\\target\\\\). We can tackle the problem using the [structured prediction recipe](structured_prediction.ipynb): We define a parametrised model \\\\(s_\\params(\\target,\\source)\\\\) that measures how well a target  \\\\(\\target\\\\) sentence matches a source sentence \\\\(\\source\\\\), learn the parameters \\\\(\\params\\\\) from training data, and then find\n",
    "\n",
    "\\begin{equation}\\label{decode-mt}\n",
    "\\argmax_\\target s_\\params(\\target,\\source)\n",
    "\\end{equation}\n",
    "\n",
    "as translation of \\\\(\\source\\\\). Different _statistical_ MT approaches, in this view, differ primarily in how \\\\(s\\\\) is defined, \\\\(\\params\\\\) are learned, and how the \\\\(\\argmax\\\\) is found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Channel Model for MT\n",
    "\n",
    "Many Word-based MT systems, as well as those based on more advanced representations, rely on a [Noisy Channel](https://www.dropbox.com/s/gfucv538m6anmgd/NoisyChannel.pdf?dl=0) model as choice for the scoring function \\\\(s_\\params\\\\). In this approach to MT we effectively model the translation process *in reverse*. That is, we assume that a probabilistic process (the speaker's brain) first generates the target sentence \\\\(\\target\\\\) according to the distribution \\\\(\\prob(\\target)\\\\). Then the target sentence \\\\(\\target\\\\) is transmitted through a _noisy channel_ \\\\(\\prob(\\source|\\target)\\\\) that translates \\\\(\\target\\\\) into \\\\(\\source\\\\). \n",
    "\n",
    "Hence translation is seen as adding noise to a clean \\\\(\\target\\\\). This _generative story_ defines a _joint distribution_ over target and source sentences \\\\(\\prob(\\source,\\target) = \\prob(\\target) \\prob(\\source|\\target) \\\\). We can in turn operate this distribution in the direction we actually care about: to infer a target sentence \\\\(\\target\\\\) given a source sentence \\\\(\\source\\\\) we find the _maximum a posteriori_ sentence\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{decode-nc}\n",
    "\\target^* = \\argmax_\\target \\prob(\\target | \\source) = \\argmax_\\target \\prob(\\target) \\, \\prob(\\source | \\target). \n",
    "\\end{equation}\n",
    "\n",
    "For the structured prediction recipe this means setting \n",
    "\n",
    "$$\n",
    "s_\\params(\\target,\\source) = \\prob(\\target) \\, \\prob(\\source | \\target). \n",
    "$$\n",
    "\n",
    "In the noisy channel approach for MT the distribution \\\\(\\prob(\\target)\\\\) that generates the target sentence is usually referred to as [language model](/template/statnlpbook/01_tasks/01_languagemodels), and the noisy channel is called the _translation model_. As we have discussed language models earlier, in this chapter we focus on the translation model \\\\(\\prob(\\source|\\target)\\\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Naive Baseline Translation Model\n",
    "The most straightforward translation model translates words one-by-one, in the order of appearance:\n",
    "$$\n",
    "\\prob_\\params^\\text{Naive}(\\ssource|\\starget) = \\prod_i^{\\length{\\source}} \\param_{\\ssource_i,\\starget_i}\n",
    "$$\n",
    "where \\\\(\\param_{\\ssource,\\starget} \\\\) is the probability of translating \\\\(\\starget\\\\) as \\\\(\\ssource\\\\). \\\\(\\params\\\\) is often referred to as *translation table*.\n",
    "\n",
    "For many language pairs one can acquire training sets \\\\(\\train=\\left( \\left(\\source_i,\\target_i\\right) \\right)_{i=1}^n \\\\) of paired source and target sentences. For example, for French and English the [Aligned Hansards](http://www.isi.edu/natural-language/download/hansard/) of the Parliament of Canada can be used. Given such a training set \\\\(\\train\\\\) we can learn the parameters \\\\(\\params\\\\) using the [Maximum Likelhood estimator](/template/statnlpbook/02_methods/0x_mle). In the case of our Naive model this amounts to setting\n",
    "$$\n",
    "\\param_{\\ssource,\\starget} = \\frac{\\counts{\\train}{s,t}}{\\counts{\\train}{t}} \n",
    "$$\n",
    "Here \\\\(\\counts{\\train}{s,t}\\\\) is the number of times we see target word \\\\(t\\\\) translated as source word \\\\(s\\\\), and \\\\(\\counts{\\train}{t}\\\\) the number of times we the target word \\\\(t\\\\) in total.\n",
    "\n",
    "### Training the Naive Model\n",
    "Let us prepare some toy data to show how train this naive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['the', 'house', 'is', 'small'], ['das', 'Haus', 'ist', 'klein']),\n",
       " (['the', 'house', 'is', 'small'], ['klein', 'ist', 'das', 'Haus']),\n",
       " (['a', 'man', 'is', 'tall'], ['ein', 'Mann', 'ist', 'groß']),\n",
       " (['my', 'house', 'is', 'small'], ['klein', 'ist', 'mein', 'Haus'])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw = [\n",
    "    (\"the house is small\", \"das Haus ist klein\"),\n",
    "    (\"the house is small\", \"klein ist das Haus\"),\n",
    "    (\"a man is tall\", \"ein Mann ist groß\"),\n",
    "    (\"my house is small\", \"klein ist mein Haus\")\n",
    "]\n",
    "train = [(t.split(\" \"), s.split(\" \")) for t,s in train_raw]\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we transformed raw strings into tokenised sentences via `split`. This dataset can be used to train the naive model as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "def learn_naive_model(data):\n",
    "    norm = defaultdict(float)\n",
    "    counts = defaultdict(float)\n",
    "    for target, source in data:\n",
    "        for i in range(0, len(target)):\n",
    "            norm[target[i]] += 1.0\n",
    "            counts[(source[i],target[i])] += 1.0\n",
    "    result = {}\n",
    "    for (source,target),score in counts.items():\n",
    "        result[(source,target)] = score / norm[target]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train on the toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEACAYAAABF+UbAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAD1dJREFUeJzt3WuwXWddx/HvL40dRKiIOEVTkwoVarmodcgUZSTYF6Re\nCCJKw63DgEYlFCkvygs14ZWAM4pOuWXIMAoDcWC4hNrWINMDAgMNoS2USUgUzbRpAculAq0Swt8X\ne6XdHPbZeyVnH/Z5dr6fmTVdl2c/63/ydP3ynLWy905VIUlq05pZFyBJOn2GuCQ1zBCXpIYZ4pLU\nMENckhpmiEtSw3qFeJLNSQ4lOZzk6hHHn5rkG0k+0y1/Pv1SJUmLrZ3UIMka4BrgUuBOYH+SD1TV\noUVNP1pVz1iBGiVJS+gzE98IHKmqo1V1HNgDbBnRLlOtTJI0UZ8QXwfcPrR9R7dvsScnuSXJPye5\naCrVSZLGmng7pacDwPqqujfJZcD7gcdMqW9J0hL6hPgxYP3Q9nndvvtV1beG1q9P8sYkD6+qrw23\nS+IHtUjSaaiqkbes+9xO2Q9ckGRDkrOBy4G9ww2SnDu0vhHI4gAfKqXhZccqqGE5C1SVyxLLjh07\nZl6Di2M7ahln4ky8qk4k2Q7sYxD6u6vqYJJtg8O1C3h2kj8BjgP3Ac+Z1K8kafl63ROvqhuAxy7a\n95ah9TcAb5huaZKkSXzH5inZNOsCtII2bdo06xK0QuZ5bDPpfstUT5bUyXuzmoVMvL8mafVJQi3j\nwaYkaZUyxCWpYYa4JDXMEJekhhniktQwQ1ySGmaIS1LDDHFJapghLkkNM8QlqWGGuCQ1zBCXpIYZ\n4pLUMENckhpmiEtSwwxxSWqYIS5JDTPEJalhhrgkNcwQl6SGGeKS1DBDXJIaZohLUsMMcUlqmCEu\nSQ0zxCWpYYa4JDXMEJekhhniktQwQ1ySGmaIS1LDDHFJapghLkkNM8QlqWG9QjzJ5iSHkhxOcvWY\ndk9KcjzJs6ZXoiRpKRNDPMka4Brg6cDjgK1JLlyi3WuAf5l2kZKk0frMxDcCR6rqaFUdB/YAW0a0\nexnwHuArU6xPkjRGnxBfB9w+tH1Ht+9+SX4GeGZVvQnI9MqTJI2zdkr9vB4Yvlc+Jsh3Dq1v6hZJ\n0kkLCwssLCz0apuqGt8guQTYWVWbu+1XAVVVrx1q88WTq8AjgG8Df1RVexf1VTD+fFpJYdJ4S1p9\nklBVIyfHfUL8LOALwKXAXcBNwNaqOrhE+7cBH6yq9444ZojPlCEutWhciE+8nVJVJ5JsB/YxuIe+\nu6oOJtk2OFy7Fr9k2RVLknqZOBOf6smcic+YM3GpReNm4r5jU5IaZohLUsMMcUlqmCEuSQ0zxCWp\nYYa4JDXMEJekhhniktQwQ1ySGmaIS1LDDHFJapghLkkNM8QlqWGGuCQ1zBCXpIYZ4pLUMENckhpm\niEtSwwxxSWqYIS5JDTPEJalhhrgkNcwQl6SGGeKS1DBDXJIaZohLUsMMcUlqmCEuSQ0zxCWpYYa4\nJDXMEJekhhniktQwQ1ySGmaIS1LDDHFJalivEE+yOcmhJIeTXD3i+DOS3Jrk5iSfTvIb0y9VkrRY\nqmp8g2QNcBi4FLgT2A9cXlWHhto8uKru7dafALyvqi4Y0VfB+PNpJYVJ4y1p9UlCVWXUsT4z8Y3A\nkao6WlXHgT3AluEGJwO88xDg7tMtVpLUX58QXwfcPrR9R7fv+yR5ZpKDwHXAldMpT5I0ztQebFbV\n+6vqF4DfAd4+rX4lSUtb26PNMWD90PZ53b6RqupjSdYm+cmq+uoPttg5tL6pWyRJJy0sLLCwsNCr\nbZ8Hm2cBX2DwYPMu4CZga1UdHGrz6Kr6j279YuDdVfXoEX35YHOmfLAptWjcg82JM/GqOpFkO7CP\nwe2X3VV1MMm2weHaBfxekhcC3wG+DTxneuVLkpYycSY+1ZM5E58xZ+JSi5b7TwwlSauUIS5JDTPE\nJalhhrgkNcwQl6SGGeKS1DBDXJIaZohLUsMMcUlqmCEuSQ0zxCWpYYa4JDXMEJekhhniktQwQ1yS\nGmaIS1LDDHFJapghLkkN6/Nt91Ivj3zk+Xz5y0dnXcYZ6dxzN/ClL/3XivXv2K5efsfmGWVlv2Mz\nCY7vrDi2883v2JSkuWSIS1LDDHFJapghLkkNM8QlqWGGuCQ1zBCXpIYZ4pLUMENckhpmiEtSwwxx\nSWqYIS5JDTPEJalhhrgkNcwQl6SGGeKS1LBeIZ5kc5JDSQ4nuXrE8ecmubVbPpbkCdMvVZK02MRv\n9kmyBjgMXArcCewHLq+qQ0NtLgEOVtU9STYDO6vqkhF9+c0+M+W3v8wvx3a+Le+bfTYCR6rqaFUd\nB/YAW4YbVNUnq+qebvOTwLrllCtJ6qdPiK8Dbh/avoPxIf0S4PrlFCVJ6meq33af5GnAi4CnTLNf\nSdJofUL8GLB+aPu8bt/3SfJEYBewuaq+vnR3O4fWN3WLJOkBC90yWZ8Hm2cBX2DwYPMu4CZga1Ud\nHGqzHvgw8IKq+uSYvnywOVM+/Jpfju18W/rB5sSZeFWdSLId2MfgHvruqjqYZNvgcO0C/gJ4OPDG\nDEb7eFVtnN4PIEkaZeJMfKoncyY+Y87W5pdjO9+W908MJUmrlCEuSQ0zxCWpYYa4JDXMEJekhhni\nktQwQ1ySGmaIS1LDDHFJapghLkkNM8QlqWGGuCQ1zBCXpIYZ4pLUMENckhpmiEtSwwxxSWqYIS5J\nDTPEJalhhrgkNcwQl6SGGeKS1DBDXJIaZohLUsMMcUlqmCEuSQ0zxCWpYYa4JDXMEJekhhniktQw\nQ1ySGmaIS1LDDHFJapghLkkNM8QlqWGGuCQ1rFeIJ9mc5FCSw0muHnH8sUk+keR/k1w1/TIlSaOs\nndQgyRrgGuBS4E5gf5IPVNWhoWZfBV4GPHNFqpQkjdRnJr4ROFJVR6vqOLAH2DLcoKrurqoDwHdX\noEZJ0hL6hPg64Pah7Tu6fZKkGZt4O2X6dg6tb+oWSdIDFrplsj4hfgxYP7R9XrfvNO08/ZdK0hlh\nE98/wX31ki373E7ZD1yQZEOSs4HLgb1j2qdHn5KkKZg4E6+qE0m2A/sYhP7uqjqYZNvgcO1Kci7w\naeChwPeSvBy4qKq+tZLFS9KZLlX1wztZUvDDO58WCys53klwfGfFsZ1voapG3uXwHZuS1DBDXJIa\nZohLUsMMcUlqmCEuSQ0zxCWpYYa4JDXMEJekhhniktQwQ1ySGmaIS1LDDHFJapghLkkNM8QlqWGG\nuCQ1zBCXpIYZ4pLUMENckhpmiEtSwwxxSWqYIS5JDTPEJalhhrgkNcwQl6SGGeKS1DBDXJIaZohL\nUsMMcUlqmCEuSQ0zxCWpYYa4JDXMEJekhhniktQwQ1ySGmaIS1LDeoV4ks1JDiU5nOTqJdr8fZIj\nSW5J8kvTLVOSNMrEEE+yBrgGeDrwOGBrkgsXtbkMeHRV/TywDXjzCtS6CizMugCtqIVZF6AVszDr\nAlZMn5n4RuBIVR2tquPAHmDLojZbgH8EqKpPAT+e5NypVroqLMy6AK2ohVkXoBWzMOsCVkyfEF8H\n3D60fUe3b1ybYyPaSJKmzAebktSwtT3aHAPWD22f1+1b3OZnJ7TppH91q9KrZ13AsiQr/efv+M6K\nYztJu2M7Tp8Q3w9ckGQDcBdwObB1UZu9wEuBf0pyCfCNqvry4o6qqvX/CyRpVZkY4lV1Isl2YB+D\n2y+7q+pgkm2Dw7Wrqq5L8ptJ/h34NvCilS1bkgSQqpp1DZKk0zSXDzaTnJ3kIxnYkOTGbv+vJHn9\nmNdtSLJ1aPupSd424Vyn2v/Jtk9MsvtUf7Yz1VJjOqLdjUkuPo3+dyR54YQ2VyT5y259W5Ln92x7\nZZIXnGpN86zveJ5Gv2PHpWszV9d1n3viLXoecG1VVfewpwCq6gBwYMzrfg54LvCuoX19flXp2/9w\n288meVSSn6qq/+5xjjPdyDGdlap6yyk0fxvwYeDtK1ROi1ZkPE9hXObmup7LmTiDIP5At34C+Brc\n/zfwB4fWb07ymSQHkvwY8FfAU7p9Lwe+A9wz4Vyn0v/9bTs3AL8/lZ94/i01pg9K8q4kn0/yXuBB\nJ1+Q5I1JbkryuSQ7hva/Jslt3UdEvK7b/S3gvgk13Ne1Ozlzv6pbv7I7/y1J3rm4bVV9E7g7yUXL\n+PnnzVLjeUWS9yXZl+SLSbYneWV3HX0iycO6do9Kcn2S/d2M/jHd/uFxubEb609l8LEhv9adb76u\n66qaq4XBX0x3LnHsqcDebn0v8ORu/cHd6+4/fprnHtv/iPZPA/bM+s9stS8TxvQVwFu79ScAx4GL\nu+2HDb3+RuDxwMOBQ0OvP+c0a9oBXNWtHwN+ZFx/DP592x/P+s9yNSwTxvMK4HB3zTyCQdj+YXfs\nb4Aru/V/ZfBRHzB4V/mHR4zLjcBfd+uXAR86zXpX9XU9jzPxRwDf7NHu48DfJnkZ8BNV9b0p19Gn\n/zuB86d83nk0bkx/HXgHQFV9Drh16NjlSQ4ANwMXdcs9wH1J3prkd5k8++7jVuCdSZ7HYFY2imP9\ngEnX6I1VdW9V3Q18Hbi22/854Pxu9vurwLuT3Ay8BVjqYz7e2/33ALBh2ZWvwut6HkMcerwroape\nC7wY+FHg4yd/HZuWnv2HGd/bbUjf9xgEIMn5wCuBp1XVLwLXAQ+qqhMMZm7vAX6bwa++y/VbDD4k\n7mJgfwYfGjeqLsf6AePG8/+G1mto+3sMnuOtAb5eVRdX1S93y+Mn9HWCKTwDXI3X9TyG+N3AQyY1\nSvKoqvp8Vb2OwRuaLmQwOzhnifZPSvIPfYtYov/Ffho42rfPM9i4Mf0og4dkJHk88MRu/zkM7kl/\nM4MPY7usa/NgBrdZbgCuGmp/vyQvTfKnp1Df+qr6CPCq7ryjanWsH9DrGl1KDZ4x/GeSZ5/cl+QH\nxnGEH/iLYx6u67kL8e7Xm9t6zKz/rHvgdQuDBx3XA58FTnQPLl6+qP164N5TKGVU/4ttBP7tFPo8\nI00Y0zcBD0nyeWAn8OnuNZ8FbgEOMrjd8rGu/TnAtUluZfAXwCtG9Hkh8NU+tSVZC7yj6+8A8HdV\n9T8jmjrWnVO4RmHpGe3zgRd3D5NvA57R47Wj+mr+up7LN/skuQJ4ZPerz7T6fC3w9qq6bYp9LgB/\nUFVfmVaf82olxnTMufYCz6qq706pv4cyePC2cRr9zYMf5nhOqKP563peQ/xs4EPAplqlP2D369+V\nVfWSWdfSghbGdClJrgS+VlXvmHUtq0XL4znOLK7ruQxxSTpTzN09cUk6kxjiktQwQ1ySGmaIS1LD\nDHFJapghLkkN+3/NpxckWt7QZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10350ee48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = learn_naive_model(train)\n",
    "def plot_table_for_target(table, target):\n",
    "    source_for_is, scores = zip(*[item for item in table.items() if item[0][1] == target])\n",
    "    util.plot_bar_graph(scores, source_for_is)\n",
    "plot_table_for_target(table, \"is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding with the Naive Model\n",
    "\n",
    "*Decoding* in MT is the task of finding the solution to equation \\\\(\\ref{decode-mt}\\\\). That is, we need to find that target sentence with maximum a posteriori probability, which is equivalent to finding the target sentence with maximum likelihood as per equation \\\\(\\ref{decode-nc}\\\\). The phrase \"decoding\" relates to the noisy channel analogy. Somebody generated a message, the channel encodes (translates) this message and the receiver needs to find out what the original message was.   \n",
    "\n",
    "In the naive model decoding is trivial if we assume a unigram language model. We need to choose, for each source word, the target word with maximal product of translation and language model probability. For more complex models this is not sufficient, and we discuss a more powerful decoding method later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'house', 'the', 'small']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(source_sent, model, lm):\n",
    "    \"\"\"\n",
    "    Decodes using the naive model. Translates each source token in isolation and appends the results.\n",
    "    Args:\n",
    "        source_sent: the source sentence as a list of tokens.\n",
    "        model: the naive model, a dictionary from (source,target) to probabilities.\n",
    "        lm: a language model as defined in the language_models chapter.\n",
    "    Returns:\n",
    "        a list of target tokens. \n",
    "    \"\"\"\n",
    "    source_to_targets = defaultdict(list)\n",
    "    for (source,target),prob in model.items():\n",
    "        source_to_targets[source] += [(target,prob)]\n",
    "    result = []\n",
    "    for tok in source_sent:\n",
    "        candidates = source_to_targets[tok]\n",
    "        multiplied_with_lm = [(target,prob * lm.probability(target)) for target, prob in candidates]\n",
    "        target = max(multiplied_with_lm, key=lambda t: t[1])\n",
    "        result.append(target[0])\n",
    "    return result\n",
    "\n",
    "source = train[1][1]\n",
    "lm = UniformLM(set([target for _, target in table.keys()]))\n",
    "target = decode(source, table, lm)\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive model is broken in several ways. Most severely, it ignores the fact that word order can differ and still yield (roughly) the same meaning.   \n",
    "\n",
    "## IBM Model 2\n",
    "The IBM Model 2 is one of the most influential translation models, even though these days it is only indirectly used in actual MT systems, for example to initialize translation and alignment models. As IBM Model 2 can be understood as generalization of IBM Model 1, we omit the latter for now and briefly illustrate it afterward our introduction of Model 2. Notice that parts of these exposition are based on the excellent [lecture notes on IBM Model 1 and 2](http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/ibm12.pdf) of Mike Collins.\n",
    "\n",
    "### Alignment\n",
    "The core difference of Model 2 to our naive baseline model is the introduction of _latent_ auxiliary variables: the word to word _alignment_ \\\\(\\aligns\\\\) between words. In particular, we introduce a variable \\\\(a_i \\in [0 \\ldots \\length{\\target}]\\\\) for each source sentence index \\\\(i \\in [1 \\ldots \\length{\\source}]\\\\). The word alignment \\\\(a_i = j \\\\) means that the source word at token \\\\(i\\\\) is _aligned_ with the target word at index \\\\(j\\\\). \n",
    "\n",
    "Notice that \\\\(\\align_i\\\\) can be \\\\(0\\\\). This corresponds to a imaginary _NULL_ token \\\\(\\starget_0\\\\) in the target sentence and allows source words to be omitted in an alignment. \n",
    "\n",
    "Below you see a simple example of an alignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <svg id='40692e7a-2e40-11e6-9277-a0999b02cfbb' xmlns=\"http://www.w3.org/2000/svg\"\n",
       "             xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "\n",
       "            <text x=\"0\" y=\"15\" class=\"source\">\n",
       "                <tspan id='t0'>NULL</tspan> <tspan id='t1'>the</tspan> <tspan id='t2'>house</tspan> <tspan id='t3'>is</tspan> <tspan id='t4'>small</tspan>\n",
       "            </text>\n",
       "            <text x=\"0\" y=\"100\" class=\"target\">\n",
       "                <tspan id='t0'>klein</tspan> <tspan id='t1'>ist</tspan> <tspan id='t2'>das</tspan> <tspan id='t3'>Haus</tspan>\n",
       "            </text>\n",
       "            <g class='connections'></g>\n",
       "            <script>\n",
       "              $(function() {\n",
       "                  root = $(document.getElementById('40692e7a-2e40-11e6-9277-a0999b02cfbb'));\n",
       "                  root.find('.connections').empty();\n",
       "                  alignments = [['.source #t1','.target #t2',1.0],['.source #t2','.target #t3',1.0],['.source #t3','.target #t1',1.0],['.source #t4','.target #t0',1.0]];\n",
       "                  function appendLine(alignment) {\n",
       "                      s1 = root.find(alignment[0])[0];\n",
       "                      x1 = s1.getExtentOfChar(0).x + s1.getComputedTextLength() / 2.0;\n",
       "                      y1 = s1.getExtentOfChar(0).y + s1.getExtentOfChar(0).height;\n",
       "                      s2 = root.find(alignment[1])[0];\n",
       "                      x2 = s2.getExtentOfChar(0).x + s2.getComputedTextLength() / 2.0;\n",
       "                      y2 = s2.getExtentOfChar(0).y;\n",
       "                      var newLine = document.createElementNS('http://www.w3.org/2000/svg','line');\n",
       "                      var score = alignment[2];\n",
       "                      newLine.setAttribute('x1',x1.toString());\n",
       "                      newLine.setAttribute('y1',y1.toString());\n",
       "                      newLine.setAttribute('x2',x2.toString());\n",
       "                      newLine.setAttribute('y2',y2.toString());\n",
       "                      newLine.setAttribute('style',\"stroke:black;stroke-width:2;stroke-opacity:\" + score + \";\");\n",
       "                      root.find('.connections').append(newLine)\n",
       "                  };\n",
       "                  for (var i = 0; i < alignments.length; i++) {\n",
       "                    appendLine(alignments[i]);\n",
       "                  }\n",
       "                  //console.log($(root).find('.connections'));\n",
       "              });\n",
       "            </script>\n",
       "        </svg>\n",
       "        "
      ],
      "text/plain": [
       "<statnlpbook.word_mt.Alignment at 0x1035152e8>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statnlpbook.word_mt as word_mt\n",
    "alignments=word_mt.Alignment(\"NULL the house is small\".split(\" \"),\n",
    "                             \"klein ist das Haus\".split(\" \"),\n",
    "                             [(1,2),(2,3),(3,1),(4,0)])\n",
    "alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an example where source words have been dropped, as indicated via the `NULL` alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <svg id='6efcb648-2e40-11e6-b89e-a0999b02cfbb' xmlns=\"http://www.w3.org/2000/svg\"\n",
       "             xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "\n",
       "            <text x=\"0\" y=\"15\" class=\"source\">\n",
       "                <tspan id='t0'>NULL</tspan> <tspan id='t1'>家</tspan> <tspan id='t2'>わ</tspan> <tspan id='t3'>小さいい</tspan> <tspan id='t4'>です</tspan>\n",
       "            </text>\n",
       "            <text x=\"0\" y=\"100\" class=\"target\">\n",
       "                <tspan id='t0'>The</tspan> <tspan id='t1'>house</tspan> <tspan id='t2'>is</tspan> <tspan id='t3'>small</tspan>\n",
       "            </text>\n",
       "            <g class='connections'></g>\n",
       "            <script>\n",
       "              $(function() {\n",
       "                  root = $(document.getElementById('6efcb648-2e40-11e6-b89e-a0999b02cfbb'));\n",
       "                  root.find('.connections').empty();\n",
       "                  alignments = [['.source #t0','.target #t0',1.0],['.source #t1','.target #t1',1.0],['.source #t3','.target #t3',1.0],['.source #t4','.target #t2',1.0]];\n",
       "                  function appendLine(alignment) {\n",
       "                      s1 = root.find(alignment[0])[0];\n",
       "                      x1 = s1.getExtentOfChar(0).x + s1.getComputedTextLength() / 2.0;\n",
       "                      y1 = s1.getExtentOfChar(0).y + s1.getExtentOfChar(0).height;\n",
       "                      s2 = root.find(alignment[1])[0];\n",
       "                      x2 = s2.getExtentOfChar(0).x + s2.getComputedTextLength() / 2.0;\n",
       "                      y2 = s2.getExtentOfChar(0).y;\n",
       "                      var newLine = document.createElementNS('http://www.w3.org/2000/svg','line');\n",
       "                      var score = alignment[2];\n",
       "                      newLine.setAttribute('x1',x1.toString());\n",
       "                      newLine.setAttribute('y1',y1.toString());\n",
       "                      newLine.setAttribute('x2',x2.toString());\n",
       "                      newLine.setAttribute('y2',y2.toString());\n",
       "                      newLine.setAttribute('style',\"stroke:black;stroke-width:2;stroke-opacity:\" + score + \";\");\n",
       "                      root.find('.connections').append(newLine)\n",
       "                  };\n",
       "                  for (var i = 0; i < alignments.length; i++) {\n",
       "                    appendLine(alignments[i]);\n",
       "                  }\n",
       "                  //console.log($(root).find('.connections'));\n",
       "              });\n",
       "            </script>\n",
       "        </svg>\n",
       "        "
      ],
      "text/plain": [
       "<statnlpbook.word_mt.Alignment at 0x1035154a8>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_mt.Alignment(\"NULL 家 わ 小さいい です\".split(\" \"),\n",
    "                  \"The house is small\".split(\" \"),\n",
    "                  [(0,0),(1,1),(3,3),(4,2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IBM Model 2 defines a conditional distribution \\\\(\\prob(\\source,\\aligns|\\target)\\\\) over both the source sentence \\\\(\\source\\\\) and its alignment \\\\(\\aligns\\\\) to the target sentence \\\\(\\target\\\\). Such a model can be used as translation model \\\\(\\prob(\\source|\\target)\\\\), as defined above, by marginalizing out the alignment \n",
    "\n",
    "$$\n",
    "\\prob(\\source|\\target) = \\sum_{\\aligns} \\prob(\\source,\\aligns|\\target).\n",
    "$$\n",
    "\n",
    "### Model Parametrization\n",
    "\n",
    "\n",
    "IBM Model 2 defines its conditional distribution over source and alignments using two sets of parameters \\\\(\\params=(\\balpha,\\bbeta)\\\\). Here \\\\(\\alpha(\\ssource|\\starget)\\\\) is a parameter defining the probability of translation target word \\\\(\\starget\\\\) into source word \\\\(\\ssource\\\\), and \\\\(\\beta(j|i,l_\\starget,l_\\ssource)\\\\) a parameter that defines the probability of aligning the source word at token \\\\(i\\\\) with the target word at token \\\\(j\\\\), conditioned on the length \\\\(l_\\starget\\\\) of the target sentence, and the length \\\\(l_\\ssource\\\\) of the source sentence. \n",
    "\n",
    "With the above parameters, IBM Model 2 defines a conditional distribution over source sentences and alignments, conditioned on a target sentence _and a desired source sentence length_ \\\\(l_\\ssource\\\\): <span class=\"summary\">Model 2 defines a conditional distribution over source sentences and alignments, conditioned on a target sentence _and a desired source sentence length_</span>\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{ibm2}\n",
    "  p_\\params^\\text{IBM2}(\\ssource_1 \\ldots \\ssource_{l_\\ssource},\\align_1 \\ldots \\align_{l_\\ssource}|\\starget_1 \\ldots \\starget_{l_\\starget}, l_\\ssource) = \\prod_i^{l_\\ssource} \\alpha(\\ssource_i|\\starget_{a_i}) \\beta(a_i|i,l_\\starget,l_\\ssource)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training IBM Model 2 with the EM Algorithm\n",
    "\n",
    "Training IBM Model 2 is less straightforward than training our naive baseline. The main reason is the lack of _gold alignments_ in the training data. That is, while we can quite easily find, or heuristically construct, _sentence-aligned_ corpora like our toy dataset, we generally do not have _word aligned_ sentences.\n",
    "\n",
    "To overcome this problem, IBM Model can be trained using the Expectation Maximization (EM) Algorithm, a general recipe when learning with partially observed data&mdash;in our case the data is partially observed because we observe the source and target sentences, but not their alignments. The EM algorithm maximizes a lower bound of the log-likelihood of the data. The log-likelihood of the data is:\n",
    "\n",
    "$$\n",
    "  \\sum_{(\\target_i,\\source_i) \\in \\train} \\log p_\\params^\\text{IBM2}(\\source_i|\\target_i) =  \\sum_{(\\target_i,\\source_i) \\in \\train} \\log \\sum_{\\aligns}  p_\\params^\\text{IBM2}(\\source_i,\\aligns|\\target_i) \n",
    "$$\n",
    "\n",
    "EM can be be seen as [block coordinate descent](https://www.dropbox.com/s/vrsefe3m57bxpgv/EMforTM.pdf?dl=0) on this bound.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EM algorithm is an iterative method that iterates between two steps, the E-step (Expectation) and the M-Step (Maximization), until convergence. For the case of IBM Model 2 the E and M steps are instantiated as follows: \n",
    "\n",
    "  * **E-Step**: given a current set of parameters \\\\(\\params\\\\), calculate the **expectations** \\\\(\\pi\\\\) of the latent alignment variables under the model \\\\(p_\\params^\\text{IBM2}\\\\) &mdash; this amounts to estimating a _soft alignment_ for each sentence.    \n",
    "  * **M-Step**: Given training set of soft alignments \\\\(\\pi\\\\), find new parameters \\\\(\\params\\\\) that **maximize** the log likelihood of this (weighted) training set. This amounts to soft counting. \n",
    "\n",
    "### E-Step\n",
    "<div class=\"book-start\"></div>\n",
    "\n",
    "The E-Step calculates the distribution\n",
    "\n",
    "$$\n",
    "\\pi(\\aligns|\\source,\\target) = p_\\params^\\text{IBM2}(\\aligns|\\source,\\target)\n",
    "$$\n",
    "\n",
    "for the current parameters \\\\(\\params\\\\). For Model 2 this distribution has a very simple form:\n",
    "\n",
    "$$\n",
    "\\pi(\\aligns|\\source,\\target) = \\prod_i^{l_{\\ssource}} \\pi(a_i|\\source,\\target,i) = \\prod_i^{l_{\\ssource}} \n",
    "  \\frac\n",
    "    {\\alpha(\\ssource_i|\\starget_{a_i}) \\beta(a_i|i,l_\\starget,l_\\ssource)}\n",
    "    {\\sum_j^{l_{\\starget}} \\alpha(\\ssource_i|\\starget_j) \\beta(j|i,l_\\starget,l_\\ssource) }\n",
    "$$\n",
    "\n",
    "Importantly, the distribution over alignments *factorizes* in a per-source-token fashion, and hence we only need to calculate, for each source token \\\\(i\\\\) and each possible alignment \\\\(a_i\\\\), the probability (or expectation) \\\\(\\pi(a_i|\\source,\\target,i)\\\\).\n",
    "\n",
    "Before we look at the implementation of this algorithm we will set up the training data to be compatible with our formulation. This involves introducing a 'NULL' token to each target sentence to allow source tokens to remain unaligned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['NULL', 'the', 'house', 'is', 'small'], ['klein', 'ist', 'das', 'Haus']),\n",
       " (['NULL', 'a', 'man', 'is', 'tall'], ['groß', 'ist', 'ein', 'Mann']),\n",
       " (['NULL', 'my', 'house', 'is', 'small'], ['klein', 'ist', 'mein', 'Haus']),\n",
       " (['NULL', 'the', 'building', 'is', 'big'], ['groß', 'ist', 'das', 'Gebäude']),\n",
       " (['NULL', 'the', 'building', 'is', 'long'],\n",
       "  ['lang', 'ist', 'das', 'Gebäude'])]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_2_raw = [\n",
    "  (\"NULL the house is small\" , \"klein ist das Haus\"),\n",
    "  (\"NULL a man is tall\" , \"groß ist ein Mann\"),\n",
    "  (\"NULL my house is small\" , \"klein ist mein Haus\"),\n",
    "  (\"NULL the building is big\" , \"groß ist das Gebäude\"),\n",
    "  (\"NULL the building is long\" , \"lang ist das Gebäude\")\n",
    "]\n",
    "train_model_2 =  [(t.split(\" \"), s.split(\" \")) for t,s in train_model_2_raw]\n",
    "train_model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement the E-Step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IBMModel2:\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "def norm_scores(scores):\n",
    "    norm = sum(scores)\n",
    "    return [s/norm for s in scores]\n",
    "\n",
    "def e_step(model, data):\n",
    "    all_alignments = []\n",
    "    for target,source in data:\n",
    "        def score(si, ti):\n",
    "            return model.alpha[source[si],target[ti]] * model.beta[ti,si, len(target),len(source)]\n",
    "        result = []\n",
    "        for si in range(0, len(source)):\n",
    "            scores = norm_scores([score(si,ti) for ti in range(0, len(target))])\n",
    "            result.append(scores)\n",
    "        all_alignments.append(result)\n",
    "    return all_alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run this code using a simple initial model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <svg id='8df19a36-2e49-11e6-9ff6-a0999b02cfbb' xmlns=\"http://www.w3.org/2000/svg\"\n",
       "             xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "\n",
       "            <text x=\"0\" y=\"15\" class=\"source\">\n",
       "                <tspan id='t0'>klein</tspan> <tspan id='t1'>ist</tspan> <tspan id='t2'>das</tspan> <tspan id='t3'>Haus</tspan>\n",
       "            </text>\n",
       "            <text x=\"0\" y=\"100\" class=\"target\">\n",
       "                <tspan id='t0'>NULL</tspan> <tspan id='t1'>the</tspan> <tspan id='t2'>house</tspan> <tspan id='t3'>is</tspan> <tspan id='t4'>small</tspan>\n",
       "            </text>\n",
       "            <g class='connections'></g>\n",
       "            <script>\n",
       "              $(function() {\n",
       "                  root = $(document.getElementById('8df19a36-2e49-11e6-9ff6-a0999b02cfbb'));\n",
       "                  root.find('.connections').empty();\n",
       "                  alignments = [['.source #t0','.target #t0',0.2],['.source #t0','.target #t1',0.2],['.source #t0','.target #t2',0.2],['.source #t0','.target #t3',0.2],['.source #t0','.target #t4',0.2],['.source #t1','.target #t0',0.2],['.source #t1','.target #t1',0.2],['.source #t1','.target #t2',0.2],['.source #t1','.target #t3',0.2],['.source #t1','.target #t4',0.2],['.source #t2','.target #t0',0.2],['.source #t2','.target #t1',0.2],['.source #t2','.target #t2',0.2],['.source #t2','.target #t3',0.2],['.source #t2','.target #t4',0.2],['.source #t3','.target #t0',0.2],['.source #t3','.target #t1',0.2],['.source #t3','.target #t2',0.2],['.source #t3','.target #t3',0.2],['.source #t3','.target #t4',0.2]];\n",
       "                  function appendLine(alignment) {\n",
       "                      s1 = root.find(alignment[0])[0];\n",
       "                      x1 = s1.getExtentOfChar(0).x + s1.getComputedTextLength() / 2.0;\n",
       "                      y1 = s1.getExtentOfChar(0).y + s1.getExtentOfChar(0).height;\n",
       "                      s2 = root.find(alignment[1])[0];\n",
       "                      x2 = s2.getExtentOfChar(0).x + s2.getComputedTextLength() / 2.0;\n",
       "                      y2 = s2.getExtentOfChar(0).y;\n",
       "                      var newLine = document.createElementNS('http://www.w3.org/2000/svg','line');\n",
       "                      var score = alignment[2];\n",
       "                      newLine.setAttribute('x1',x1.toString());\n",
       "                      newLine.setAttribute('y1',y1.toString());\n",
       "                      newLine.setAttribute('x2',x2.toString());\n",
       "                      newLine.setAttribute('y2',y2.toString());\n",
       "                      newLine.setAttribute('style',\"stroke:black;stroke-width:2;stroke-opacity:\" + score + \";\");\n",
       "                      root.find('.connections').append(newLine)\n",
       "                  };\n",
       "                  for (var i = 0; i < alignments.length; i++) {\n",
       "                    appendLine(alignments[i]);\n",
       "                  }\n",
       "                  //console.log($(root).find('.connections'));\n",
       "              });\n",
       "            </script>\n",
       "        </svg>\n",
       "        "
      ],
      "text/plain": [
       "<statnlpbook.word_mt.Alignment at 0x1035bd7b8>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_vocab = set([tok for _,s in train_model_2 for tok in s])\n",
    "target_vocab = set([tok for t,_ in train_model_2 for tok in t])\n",
    "\n",
    "max_length = 5\n",
    "alpha, beta = {}, {}\n",
    "for s in source_vocab:\n",
    "    for t in target_vocab:\n",
    "        alpha[s,t] = 1.0 / len(source_vocab)\n",
    "for ti in range(0, max_length):\n",
    "    for si in range(0, max_length):\n",
    "        for lt in range(1, max_length+1):\n",
    "            for ls in range(1, max_length+1):\n",
    "                beta[ti,si,lt,ls] = 1.0 / lt\n",
    "                \n",
    "init_model = IBMModel2(alpha,beta)\n",
    "align_matrices = e_step(init_model, train_model_2)\n",
    "word_mt.Alignment.from_matrix(align_matrices[0], train_model_2[0][1], train_model_2[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play around with the initialization of \\\\(\\balpha\\\\) to see how the alignments react to changes of the word-to-word translation probabilities.\n",
    "\n",
    "### M-Step\n",
    "\n",
    "The M-Step optimizes a *weighted* or *expected* version of the log-likelihood of the data, using the distribution \\\\(\\pi\\\\) from the last E-Step:\n",
    "\n",
    "$$\n",
    "  \\params^* = \\argmax_\\params \\sum_{(\\target,\\source) \\in \\train} \\sum_\\aligns \\pi(\\aligns|\\target,\\source) \\log \\prob _\\params^\\text{IBM2}(\\source,\\aligns|\\target)\n",
    "$$\n",
    "\n",
    "The summing over hidden alignments seems daunting, but because \\\\(\\pi\\\\) factorizes as we discussed above, we again have a simple closed-form solution:\n",
    "\n",
    "$$\n",
    "  \\alpha(\\ssource|\\starget) = \\frac\n",
    "    {\\sum_{(\\target,\\source)}\\sum_i^{l_\\source} \\sum_j^{l_\\target} \\pi(j|i) \\delta(\\ssource,\\ssource_i) \\delta(\\starget,\\starget_j) }\n",
    "    {\\sum_{(\\target,\\source)} \\sum_j^{l_\\target} \\delta(\\starget,\\starget_j) }\n",
    "$$\n",
    "\n",
    "where \\\\(\\delta(x,y)\\\\) is 1 if \\\\(x=y\\\\) and 0 otherwise. The updates for \\\\(\\beta\\\\) are similar. \n",
    "\n",
    "Let us implement the M-Step now. In this step we estimate parameters \\\\(\\params\\\\) from a given set of (soft) alignments \\\\(\\aligns\\\\). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEACAYAAAC3adEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFRwMZmZW+C97TwYhqzlOLQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1035ae400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def m_step(aligns, data):\n",
    "    alpha = defaultdict(float)\n",
    "    alpha_norm = defaultdict(float)\n",
    "    beta = defaultdict(float)\n",
    "    beta_norm = defaultdict(float)\n",
    "    for pi, (t,s) in zip(aligns, data):\n",
    "        for ti in range(0, len(t)):\n",
    "            for si in range(0, len(s)):\n",
    "                prob = pi[si][ti]\n",
    "                alpha[s[si], t[ti]] += prob\n",
    "                alpha_norm[t[ti]] += prob\n",
    "                beta[ti,si,len(t),len(s)] += prob\n",
    "                beta_norm[si,len(t),len(s)] += prob\n",
    "    for key in alpha.keys():\n",
    "        alpha[key] /= alpha_norm[key[1]]\n",
    "    for key in beta.keys():\n",
    "        beta[key] /= beta_norm[key[1:]]\n",
    "    return IBMModel2(alpha,beta)\n",
    "# val theta1 = mStep(alignments,trainM2)\n",
    "# barChart(theta1.alpha.filter(_._1._2 == \"is\"))\n",
    "theta1 = m_step(align_matrices, train_model_2)    \n",
    "# util.plot_bar_graph(scores, source_for_is)\n",
    "# theta1.alpha\n",
    "plot_table_for_target(theta1.alpha, \"is\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
