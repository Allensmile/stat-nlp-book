{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# %cd .. \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import statnlpbook.util as util\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\Xs}{\\mathcal{X}}\n",
    "\\newcommand{\\Ys}{\\mathcal{Y}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\aligns}{\\mathbf{a}}\n",
    "\\newcommand{\\align}{a}\n",
    "\\newcommand{\\source}{\\mathbf{s}}\n",
    "\\newcommand{\\target}{\\mathbf{t}}\n",
    "\\newcommand{\\ssource}{s}\n",
    "\\newcommand{\\starget}{t}\n",
    "\\newcommand{\\repr}{\\mathbf{f}}\n",
    "\\newcommand{\\repry}{\\mathbf{g}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\DeclareMathOperator{\\argmin}{argmin}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "\\newcommand{\\length}[1]{\\text{length}(#1) }\n",
    "\\newcommand{\\indi}{\\mathbb{I}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Labelling \n",
    "\n",
    "Many real-world applications can be cast as *sequence labelling* problems that involve assigning labels to each element in a sequence. For example, in *Part-Of-Speech tagging* each token in a sentence as assigned a part-of-speech such as verb or determiner that indicates the syntactic type of the token. In *Named Entity Tagging* we assign each token with the type of entity the token refers to, such as \"Person\" or \"Organisation\", or \"None\" if the token does not refer to an entity.  \n",
    "\n",
    "## Sequence Labelling as Structured Prediction\n",
    "\n",
    "The problem of sequence labelling is an obvious (and somewhat canonical) instance of structured prediction. Here the input space \\\\(\\Xs\\\\) are sequences of words and the output space $\\Ys$ are sequences of output labels. Our goal is again to define a model a model \\\\(s_{\\params}(\\x,\\y)\\\\) that assigns high *scores* to the sequence of label \\\\(\\y=y_1 \\ldots y_n\\\\) that fits the input text \\\\(\\x=x_1 \\ldots x_n\\\\), and lower scores otherwise. The model will be parametrized by \\\\(\\params\\\\), and these parameters we will learn from some training set \\\\(\\train\\\\) of \\\\((\\x,\\y)\\\\) pairs. In contrast to the classification scenario the prediction problem $\\argmax_\\y s_{\\params}(\\x,\\y)$ is now non-trivial in general, as we have to search through an exponentional number of label sequences. In practice this issue is overcome by making assumptions about the factorization structure of $s_{\\params}(\\x,\\y)$ and/or search approximations that sacrifice the ability to find the true optimum of the search problem for more expressiveness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging as Sequence Labelling\n",
    "Part-of-Speech (PoS) tagging is an important task within NLP. It is a standard pre-processing step in many tasks. For example, most dependency parsers assume as input PoS tagged sentences. Likewise, [Reverb](reverb), one of the most effective relation extraction methods, defines relations in terms of PoS sequences.\n",
    "\n",
    "Traditionally, and based on the existence of corresponding annotated training sets, PoS tagging has been applied to quite restricted domains such newswire or biomedical texts. Recently there has been increasing interest in NLP in general, and PoS tagging in particular, for social media data. He we will focus on PoS tagging for tweets and use the [Tweebank dataset](http://www.cs.cmu.edu/~ark/TweetNLP/#pos) and the [\"october 27\" splits](https://github.com/brendano/ark-tweet-nlp/tree/master/data/twpos-data-v0.3/oct27.splits).\n",
    "\n",
    "Let us load the data and look at an example tagged sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I/O predict/V I/O won't/V win/V a/D single/A game/N I/O bet/V on/P ./, Got/V Cliff/^ Lee/^ today/N ,/, so/P if/P he/O loses/V its/L on/P me/O RT/~ @e_one/@ :/~ Texas/^ (/, cont/~ )/, http://tl.gd/6meogh/U\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_tweebank(filename):\n",
    "    result = []\n",
    "    tweet = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                result.append(tweet)\n",
    "                tweet = []\n",
    "            else:\n",
    "                tweet.append(tuple(line.split()))\n",
    "    return result\n",
    "train = load_tweebank(\"../data/oct27.splits/oct27.train\")\n",
    "dev = load_tweebank(\"../data/oct27.splits/oct27.dev\")\n",
    "test = load_tweebank(\"../data/oct27.splits/oct27.test\")\n",
    "\" \".join([w + \"/\" + t for w,t in train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have printed the tokens of tweet paired with their PoS tag. The tags (such as \"O\", \"V\" and \"^\") are described in the [Tweebank annotation guideline](http://www.cs.cmu.edu/~ark/TweetNLP/annot_guidelines.pdf). For example, \"O\" denotes pronouns, \"V\" verbs and \"^\" proper nouns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count tags here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Models / Classifiers\n",
    "We will tackle sequence labelling as a (discriminative) structured prediction problem. This means we will build a model $p_\\params(\\y|\\x)$ that computes the conditional probability of output label sequence $\\y$ given input sequence $\\x$. We will first consider the simplest type of model: a *fully factorised* or *local* model. In this model the probability of $\\x$ and $\\y$ is a product of *local* probabilities for the label $y_i$ of each token:\n",
    "\n",
    "$$\n",
    "p_\\params(\\y|\\x) = \\prod_{i=1}^n p_\\params(y_i|\\x,i)\n",
    "$$\n",
    "\n",
    "In this model all labels are independent of each other. This assumption has a crucial benefit: inference (and hence training) in this model is trivial. To find the most likely assignment of labels under this model you can find the most likely tag for each token independently. Notice that each local term conditions on the complete input $\\x$, not just on $x_i$. This is important, as the sentential context at position $i$ is often important to determine the tag at $i$.     \n",
    "\n",
    "It is common to indicate this independence structure in a factor graph, a graphical representation of the model. In this representation each variable of the model (our per-token tag labels and the input sequence $\\x$) is drawn using a circle. Each factor in the model (terms in the product) is drawn as a box that connects the variables that appear in the corresponding term. For example, the term $p_\\params(y_3|\\x,3)$ would connect the variables $y_3$ and $\\x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"206pt\" height=\"188pt\"\n",
       " viewBox=\"0.00 0.00 206.00 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-184 202,-184 202,4 -4,4\"/>\n",
       "<!-- x -->\n",
       "<g id=\"node1\" class=\"node\"><title>x</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-157.8\" font-family=\"Times,serif\" font-size=\"14.00\">x</text>\n",
       "</g>\n",
       "<!-- f_1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>f_1</title>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-108 0,-108 0,-72 54,-72 54,-108\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-85.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"white\">f_1</text>\n",
       "</g>\n",
       "<!-- x&#45;&#45;f_1 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>x&#45;&#45;f_1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M84.4297,-146.834C72.9242,-135.649 56.8272,-119.999 44.6017,-108.113\"/>\n",
       "</g>\n",
       "<!-- f_2 -->\n",
       "<g id=\"node3\" class=\"node\"><title>f_2</title>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"126,-108 72,-108 72,-72 126,-72 126,-108\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-85.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"white\">f_2</text>\n",
       "</g>\n",
       "<!-- x&#45;&#45;f_2 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>x&#45;&#45;f_2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M99,-143.697C99,-132.846 99,-118.917 99,-108.104\"/>\n",
       "</g>\n",
       "<!-- f_3 -->\n",
       "<g id=\"node4\" class=\"node\"><title>f_3</title>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"198,-108 144,-108 144,-72 198,-72 198,-108\"/>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-85.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"white\">f_3</text>\n",
       "</g>\n",
       "<!-- x&#45;&#45;f_3 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>x&#45;&#45;f_3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M113.57,-146.834C125.076,-135.649 141.173,-119.999 153.398,-108.113\"/>\n",
       "</g>\n",
       "<!-- y_1 -->\n",
       "<g id=\"node5\" class=\"node\"><title>y_1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\">y_1</text>\n",
       "</g>\n",
       "<!-- f_1&#45;&#45;y_1 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>f_1&#45;&#45;y_1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M27,-71.6966C27,-60.8463 27,-46.9167 27,-36.1043\"/>\n",
       "</g>\n",
       "<!-- y_2 -->\n",
       "<g id=\"node6\" class=\"node\"><title>y_2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\">y_2</text>\n",
       "</g>\n",
       "<!-- f_2&#45;&#45;y_2 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>f_2&#45;&#45;y_2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M99,-71.6966C99,-60.8463 99,-46.9167 99,-36.1043\"/>\n",
       "</g>\n",
       "<!-- y_3 -->\n",
       "<g id=\"node7\" class=\"node\"><title>y_3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\">y_3</text>\n",
       "</g>\n",
       "<!-- f_3&#45;&#45;y_3 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>f_3&#45;&#45;y_3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M171,-71.6966C171,-60.8463 171,-46.9167 171,-36.1043\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Graph at 0x1123e3f60>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw factor graph using dot?\n",
    "# need to create two sub-graphs: one with y labels and one with x label\n",
    "# http://matthiaseisen.com/articles/graphviz/\n",
    "import graphviz as gv\n",
    "def add_nodes(graph, nodes):\n",
    "    for n in nodes:\n",
    "        if isinstance(n, tuple):\n",
    "            graph.node(n[0], **n[1])\n",
    "        else:\n",
    "            graph.node(n)\n",
    "    return graph\n",
    "\n",
    "def add_edges(graph, edges):\n",
    "    for e in edges:\n",
    "        if isinstance(e[0], tuple):\n",
    "            graph.edge(*e[0], **e[1])\n",
    "        else:\n",
    "            graph.edge(*e)\n",
    "    return graph\n",
    "\n",
    "import functools\n",
    "graph = functools.partial(gv.Graph, format='svg')\n",
    "digraph = functools.partial(gv.Digraph, format='svg')\n",
    "g1 = add_edges(\n",
    "    add_nodes(graph(), ['y_1', 'y_2', 'y_3']),\n",
    "    []\n",
    ")\n",
    "factor_style = {'shape':'box', 'fontcolor':'white', 'style':'filled', 'fillcolor':'black'}\n",
    "g2 = add_nodes(graph(), ['x',('f_1',factor_style),('f_2',factor_style),('f_3',factor_style)])\n",
    "g2.subgraph(g1)\n",
    "g2.edge('x','f_1')\n",
    "g2.edge('x','f_2')\n",
    "g2.edge('x','f_3')\n",
    "g2.edge('f_1','y_1')\n",
    "g2.edge('f_2','y_2')\n",
    "g2.edge('f_3','y_3')\n",
    "\n",
    "g2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The independence assumptions of this model allow us to think of it not as a single sequence model, but a sequence of classification models. We have a classifier $p_\\params(y|\\x,i)$ that predicts the best class/tag for a token based on its sentence $\\x$ and position $i$. \n",
    "\n",
    "\n",
    "## MEMM \n",
    "## CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Material\n",
    "* [Tackling the Poor Assumptions of Naive Bayes Text Classifiers](https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf), Rennie et al, ICML 2003 \n",
    "* [Simple Sentiment Classification](http://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf) ???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
