{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# %cd .. \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import statnlpbook.util as util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\Xs}{\\mathcal{X}}\n",
    "\\newcommand{\\Ys}{\\mathcal{Y}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\aligns}{\\mathbf{a}}\n",
    "\\newcommand{\\align}{a}\n",
    "\\newcommand{\\source}{\\mathbf{s}}\n",
    "\\newcommand{\\target}{\\mathbf{t}}\n",
    "\\newcommand{\\ssource}{s}\n",
    "\\newcommand{\\starget}{t}\n",
    "\\newcommand{\\repr}{\\mathbf{f}}\n",
    "\\newcommand{\\repry}{\\mathbf{g}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\DeclareMathOperator{\\argmin}{argmin}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "\\newcommand{\\length}[1]{\\text{length}(#1) }\n",
    "\\newcommand{\\indi}{\\mathbb{I}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification \n",
    "\n",
    "In many applications we need to automatically classify some input text with respect to a set of classes or labels. For example,\n",
    "\n",
    "* for information retrieval it is useful to classify documents into a set of topics, such as \"sport\" or \"business\",\n",
    "* for sentiment analysis we classify tweets into being \"positive\" or \"negative\" and \n",
    "* for Spam Filters we need to distinguish between Ham and Spam.\n",
    "\n",
    "<!-- TODO: Load Web Corpus, 4 Universities, something were Maxent works -->\n",
    "\n",
    "## Text Classification as Structured Prediction\n",
    "We can formalize text classification as the simplest instance of [structured prediction](/template/statnlpbook/02_methods/00_structuredprediction) where the input space \\\\(\\Xs\\\\) are sequences of words, and the output space \\\\(\\Ys\\\\) is a set of labels such as \\\\(\\Ys=\\\\{ \\text{sports},\\text{business}\\\\} \\\\) in document classification or \\\\(\\Ys=\\\\{ \\text{positive},\\text{negative}, \\text{neutral}\\\\} \\\\) in sentiment prediction. On a high level, our goal is to define a model a model \\\\(s_{\\params}(\\x,y)\\\\) that assigns high *scores* to the label \\\\(y\\\\) that fits the text \\\\(\\x\\\\), and lower scores otherwise. The model will be parametrized by \\\\(\\params\\\\), and these parameters we will learn from some training set \\\\(\\train\\\\) of \\\\((\\x,y)\\\\) pairs. When we need to classify a text \\\\(\\x\\\\) we have to solve the trivial (if the number of classes is low) maximization problem $\\argmax_y s_{\\params}(\\x,y)$. \n",
    "\n",
    "<!-- TODO: Show a silly classifier example? --> \n",
    "\n",
    "In the following we will present two typical approaches to text classifiers: Naive Bayes and discriminative linear classifiers. We will also see that both in fact can use the same model structure, and differ only in how model parameters are trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "One of the most widely used approaches to text classification relies on the so-called Naive Bayes (NB) Model. In NB we use a distribution $p^{\\mbox{NB}}_{\\params}$ for $s_\\params$. In particular, we use the *a posteriori* probability of a label \\\\(y\\\\) given the input text \\\\(\\x\\\\) as a score for that label given the text.   \n",
    "\n",
    "\\begin{equation}\n",
    "  s_{\\params}(\\x,\\y)\\ = p^{\\text{NB}}_{\\params}(y|\\x)\n",
    "\\end{equation}\n",
    "\n",
    "By Bayes Law we get\n",
    "\n",
    "\\begin{equation}\n",
    "    p^{\\text{NB}}_{\\params}(y|\\x) =\n",
    "  \\frac{p^{\\text{NB}}_{\\params}(\\x|y) p^\\text{NB}_{\\params}(y)}{p^{\\text{NB}}_{\\params}(x)}  \n",
    "\\end{equation}\n",
    "\n",
    "and when an input \\\\(\\x\\\\) is fixed we can focus on \n",
    "\n",
    "\\begin{equation}\\label{eq:NB}\n",
    "\\prob^{\\text{NB}}_{\\params}(\\x,y)= p^{\\text{NB}}_{\\params}(\\x|y) p^\\text{NB}_{\\params}(y)\n",
    "\\end{equation}\n",
    "\n",
    "because in this case  \\\\(p^{\\text{NB}}_{\\params}(x)\\\\) is a constant factor. In the above \\\\(p^{\\text{NB}}_{\\params}(\\x|y)\\\\) is the *likelihood*, and \\\\(p^\\text{NB}_{\\params}(y) \\\\) is the *prior*.\n",
    "\n",
    "<!--Let us assume that we have a number \\\\(K(\\x)\\\\) of feature functions \\\\(f_k(\\x)\\\\) that represent the input \\\\(\\x\\\\). For example, in document classification this set could be used to represent the text \\\\(\\x = (x_1,\\ldots,x_n)\\\\) as a bag of words by setting \\\\(f_k(\\x) = x_k\\\\) and \\\\(K(\\x) = n\\\\). We could also use bigrams instead, setting \\\\(f_k(\\x) = (x_k,x_{k+1})\\\\) and \\\\(K(\\x) = n-1\\\\), or any other representation that is effective for distinguishing between classes of text.-->\n",
    "\n",
    "The \"naivity\" of NB stems from a certain conditional independence assumption we make for the likelihood \\\\(p^{\\mbox{NB}}_{\\params}(\\x|y)\\\\). Note that conditional independence of two events \\\\(a\\\\) and \\\\(b\\\\) given a third event \\\\(c\\\\) requires that \\\\(p(a,b|c) = p(a|c) p(b|c)\\\\). In particular, for the likelihood in NB we have:\n",
    "\n",
    "\\begin{equation}\n",
    "  p^{\\text{NB}}_{\\params}(\\x|y) = \n",
    "  \\prod_i^{\\text{length}(\\x)} p^{\\text{NB}}_{\\params}(x_i|y)\n",
    "\\end{equation}\n",
    "\n",
    "That is, NB makes the assumption that the observed wors are independent of each other when *conditioned on the label* \\\\(y\\\\). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
